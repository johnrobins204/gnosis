2025-08-19 18:14:08,556 INFO inference run_from_config start for inference, keys=['default_model', 'input_csv', 'output_csv']
2025-08-19 18:14:08,558 INFO inference generated for model=google:test prompt_len=11
2025-08-19 18:14:08,558 INFO inference generated for model=ollama:demo prompt_len=12
2025-08-19 18:14:08,559 INFO inference run_from_config inference finished, wrote /home/johnro/sop-research/artifacts/inference_raw_447541.csv (rows=2) duration=0.00s
2025-08-19 18:14:08,561 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-19 18:14:08,563 INFO judge run_from_config judge finished, wrote /home/johnro/sop-research/artifacts/inference_judged_447541.csv (rows=2) duration=0.00s
2025-08-19 18:14:08,567 INFO analyst run_from_config start for analyst with cfg keys=['group_col', 'input_csv', 'output_csv', 'rating_col']
2025-08-19 18:14:08,568 INFO analyst analyze_df start group_col=model rating_col=judge_rating
2025-08-19 18:14:08,569 INFO analyst analyze_df done (groups=2)
2025-08-19 18:14:08,569 INFO analyst run_from_config finished successfully, wrote /home/johnro/sop-research/artifacts/analysis_summary_447541.csv
2025-08-20 15:08:31,594 INFO inference run_from_config start for inference, keys=['input_file', 'output_file', 'model', 'prompt_column']
2025-08-20 15:08:31,594 ERROR inference missing config key: input_csv
2025-08-20 15:12:11,115 INFO inference run_from_config start for inference, keys=['input_file', 'output_file', 'model', 'temperature', 'max_tokens', 'prompt_column', 'batch_size']
2025-08-20 15:12:11,115 ERROR inference missing config key: input_csv
2025-08-20 15:12:11,168 INFO inference run_from_config start for inference, keys=['input_file', 'output_file', 'model', 'temperature', 'max_tokens', 'prompt_column', 'batch_size']
2025-08-20 15:12:11,168 ERROR inference missing config key: input_csv
2025-08-20 15:12:11,172 INFO inference run_from_config start for inference, keys=['input_file', 'output_file', 'model', 'temperature', 'max_tokens', 'prompt_column', 'batch_size']
2025-08-20 15:12:11,172 ERROR inference missing config key: input_csv
2025-08-20 15:15:37,054 INFO inference run_from_config start for inference, keys=['input_csv', 'output_file', 'model', 'provider', 'temperature', 'max_tokens', 'prompt_column', 'batch_size', 'server']
2025-08-20 15:15:37,054 ERROR inference missing config key: output_csv
2025-08-20 15:15:37,105 INFO inference run_from_config start for inference, keys=['input_csv', 'output_file', 'model', 'provider', 'temperature', 'max_tokens', 'prompt_column', 'batch_size', 'server']
2025-08-20 15:15:37,105 ERROR inference missing config key: output_csv
2025-08-20 15:15:37,108 INFO inference run_from_config start for inference, keys=['input_csv', 'output_file', 'model', 'provider', 'temperature', 'max_tokens', 'prompt_column', 'batch_size', 'server']
2025-08-20 15:15:37,108 ERROR inference missing config key: output_csv
2025-08-20 15:17:09,271 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'model', 'provider', 'temperature', 'max_tokens', 'prompt_column', 'batch_size', 'server']
2025-08-20 15:17:09,272 ERROR inference generation failed for model=Llama:8B: 'list' object has no attribute 'model'
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 60, in run_from_config
    "model": resp.model,
AttributeError: 'list' object has no attribute 'model'
2025-08-20 15:17:09,272 ERROR inference generation failed for model=google:default: 'list' object has no attribute 'model'
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 60, in run_from_config
    "model": resp.model,
AttributeError: 'list' object has no attribute 'model'
2025-08-20 15:17:09,273 ERROR inference generation failed for model=Llama:8B: 'list' object has no attribute 'model'
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 60, in run_from_config
    "model": resp.model,
AttributeError: 'list' object has no attribute 'model'
2025-08-20 15:17:09,274 INFO inference run_from_config inference finished, wrote test_output.csv (rows=3) duration=0.00s
2025-08-20 15:17:09,324 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'model', 'provider', 'temperature', 'max_tokens', 'prompt_column', 'batch_size', 'server']
2025-08-20 15:17:09,325 ERROR inference generation failed for model=Llama:8B: 'list' object has no attribute 'model'
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 60, in run_from_config
    "model": resp.model,
AttributeError: 'list' object has no attribute 'model'
2025-08-20 15:17:09,325 ERROR inference generation failed for model=Llama:8B: 'list' object has no attribute 'model'
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 60, in run_from_config
    "model": resp.model,
AttributeError: 'list' object has no attribute 'model'
2025-08-20 15:17:09,325 ERROR inference generation failed for model=Llama:8B: 'list' object has no attribute 'model'
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 60, in run_from_config
    "model": resp.model,
AttributeError: 'list' object has no attribute 'model'
2025-08-20 15:17:09,326 ERROR inference generation failed for model=Llama:8B: 
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 58, in run_from_config
    resp: ModelResponse = model.generate(prompt)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1175, in _execute_mock_call
    result = next(effect)
StopIteration
2025-08-20 15:17:09,326 ERROR inference generation failed for model=Llama:8B: 
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 58, in run_from_config
    resp: ModelResponse = model.generate(prompt)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1175, in _execute_mock_call
    result = next(effect)
StopIteration
2025-08-20 15:17:09,326 ERROR inference generation failed for model=Llama:8B: 
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 58, in run_from_config
    resp: ModelResponse = model.generate(prompt)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1175, in _execute_mock_call
    result = next(effect)
StopIteration
2025-08-20 15:17:09,327 ERROR inference generation failed for model=Llama:8B: 
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 58, in run_from_config
    resp: ModelResponse = model.generate(prompt)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1175, in _execute_mock_call
    result = next(effect)
StopIteration
2025-08-20 15:17:09,327 ERROR inference generation failed for model=Llama:8B: 
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 58, in run_from_config
    resp: ModelResponse = model.generate(prompt)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1175, in _execute_mock_call
    result = next(effect)
StopIteration
2025-08-20 15:17:09,327 ERROR inference generation failed for model=Llama:8B: 
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 58, in run_from_config
    resp: ModelResponse = model.generate(prompt)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1175, in _execute_mock_call
    result = next(effect)
StopIteration
2025-08-20 15:17:09,328 ERROR inference generation failed for model=Llama:8B: 
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 58, in run_from_config
    resp: ModelResponse = model.generate(prompt)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1175, in _execute_mock_call
    result = next(effect)
StopIteration
2025-08-20 15:17:09,328 ERROR inference generation failed for model=Llama:8B: 
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 58, in run_from_config
    resp: ModelResponse = model.generate(prompt)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1175, in _execute_mock_call
    result = next(effect)
StopIteration
2025-08-20 15:17:09,328 ERROR inference generation failed for model=Llama:8B: 
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 58, in run_from_config
    resp: ModelResponse = model.generate(prompt)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1175, in _execute_mock_call
    result = next(effect)
StopIteration
2025-08-20 15:17:09,340 INFO inference run_from_config inference finished, wrote test_output.csv (rows=12) duration=0.01s
2025-08-20 15:17:09,343 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'model', 'provider', 'temperature', 'max_tokens', 'prompt_column', 'batch_size', 'server']
2025-08-20 15:17:09,344 ERROR inference generation failed for model=google:default: 'list' object has no attribute 'model'
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 60, in run_from_config
    "model": resp.model,
AttributeError: 'list' object has no attribute 'model'
2025-08-20 15:17:09,344 ERROR inference generation failed for model=google:default: Ollama API error: context length exceeded
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 58, in run_from_config
    resp: ModelResponse = model.generate(prompt)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1177, in _execute_mock_call
    raise result
Exception: Ollama API error: context length exceeded
2025-08-20 15:17:09,344 INFO inference run_from_config inference finished, wrote test_output.csv (rows=2) duration=0.00s
2025-08-20 15:17:32,902 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'model', 'provider', 'temperature', 'max_tokens', 'prompt_column', 'batch_size', 'server']
2025-08-20 15:17:32,903 ERROR inference generation failed for model=Llama:8B: 'list' object has no attribute 'model'
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 60, in run_from_config
    "model": resp.model,
AttributeError: 'list' object has no attribute 'model'
2025-08-20 15:17:32,903 ERROR inference generation failed for model=google:default: 'list' object has no attribute 'model'
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 60, in run_from_config
    "model": resp.model,
AttributeError: 'list' object has no attribute 'model'
2025-08-20 15:17:32,904 ERROR inference generation failed for model=Llama:8B: 'list' object has no attribute 'model'
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 60, in run_from_config
    "model": resp.model,
AttributeError: 'list' object has no attribute 'model'
2025-08-20 15:17:32,904 INFO inference run_from_config inference finished, wrote test_output.csv (rows=3) duration=0.00s
2025-08-20 15:17:32,952 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'model', 'provider', 'temperature', 'max_tokens', 'prompt_column', 'batch_size', 'server']
2025-08-20 15:17:32,952 ERROR inference generation failed for model=Llama:8B: 'list' object has no attribute 'model'
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 60, in run_from_config
    "model": resp.model,
AttributeError: 'list' object has no attribute 'model'
2025-08-20 15:17:32,953 ERROR inference generation failed for model=Llama:8B: 'list' object has no attribute 'model'
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 60, in run_from_config
    "model": resp.model,
AttributeError: 'list' object has no attribute 'model'
2025-08-20 15:17:32,953 ERROR inference generation failed for model=Llama:8B: 'list' object has no attribute 'model'
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 60, in run_from_config
    "model": resp.model,
AttributeError: 'list' object has no attribute 'model'
2025-08-20 15:17:32,954 ERROR inference generation failed for model=Llama:8B: 
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 58, in run_from_config
    resp: ModelResponse = model.generate(prompt)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1175, in _execute_mock_call
    result = next(effect)
StopIteration
2025-08-20 15:17:32,954 ERROR inference generation failed for model=Llama:8B: 
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 58, in run_from_config
    resp: ModelResponse = model.generate(prompt)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1175, in _execute_mock_call
    result = next(effect)
StopIteration
2025-08-20 15:17:32,954 ERROR inference generation failed for model=Llama:8B: 
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 58, in run_from_config
    resp: ModelResponse = model.generate(prompt)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1175, in _execute_mock_call
    result = next(effect)
StopIteration
2025-08-20 15:17:32,954 ERROR inference generation failed for model=Llama:8B: 
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 58, in run_from_config
    resp: ModelResponse = model.generate(prompt)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1175, in _execute_mock_call
    result = next(effect)
StopIteration
2025-08-20 15:17:32,955 ERROR inference generation failed for model=Llama:8B: 
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 58, in run_from_config
    resp: ModelResponse = model.generate(prompt)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1175, in _execute_mock_call
    result = next(effect)
StopIteration
2025-08-20 15:17:32,955 ERROR inference generation failed for model=Llama:8B: 
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 58, in run_from_config
    resp: ModelResponse = model.generate(prompt)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1175, in _execute_mock_call
    result = next(effect)
StopIteration
2025-08-20 15:17:32,955 ERROR inference generation failed for model=Llama:8B: 
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 58, in run_from_config
    resp: ModelResponse = model.generate(prompt)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1175, in _execute_mock_call
    result = next(effect)
StopIteration
2025-08-20 15:17:32,956 ERROR inference generation failed for model=Llama:8B: 
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 58, in run_from_config
    resp: ModelResponse = model.generate(prompt)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1175, in _execute_mock_call
    result = next(effect)
StopIteration
2025-08-20 15:17:32,956 ERROR inference generation failed for model=Llama:8B: 
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 58, in run_from_config
    resp: ModelResponse = model.generate(prompt)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1175, in _execute_mock_call
    result = next(effect)
StopIteration
2025-08-20 15:17:32,956 INFO inference run_from_config inference finished, wrote test_output.csv (rows=12) duration=0.00s
2025-08-20 15:17:32,972 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'model', 'provider', 'temperature', 'max_tokens', 'prompt_column', 'batch_size', 'server']
2025-08-20 15:17:32,972 ERROR inference generation failed for model=google:default: 'list' object has no attribute 'model'
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 60, in run_from_config
    "model": resp.model,
AttributeError: 'list' object has no attribute 'model'
2025-08-20 15:17:32,972 ERROR inference generation failed for model=google:default: Ollama API error: context length exceeded
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 58, in run_from_config
    resp: ModelResponse = model.generate(prompt)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1177, in _execute_mock_call
    raise result
Exception: Ollama API error: context length exceeded
2025-08-20 15:17:32,973 INFO inference run_from_config inference finished, wrote test_output.csv (rows=2) duration=0.00s
2025-08-20 15:22:22,751 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 15:22:22,751 INFO inference generated for model=Llama:8B prompt_len=26
2025-08-20 15:22:22,751 INFO inference generated for model=Llama:8B prompt_len=25
2025-08-20 15:22:22,752 INFO inference generated for model=Llama:8B prompt_len=31
2025-08-20 15:22:22,752 INFO inference run_from_config inference finished, wrote test_output.csv (rows=3) duration=0.00s
2025-08-20 15:22:22,754 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 15:22:22,754 INFO inference generated for model=Llama:8B prompt_len=26
2025-08-20 15:22:22,754 ERROR inference generation failed for model=Llama:8B: Ollama API error: context length exceeded
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 58, in run_from_config
    resp: ModelResponse = model.generate(prompt)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1177, in _execute_mock_call
    raise result
Exception: Ollama API error: context length exceeded
2025-08-20 15:22:22,755 INFO inference generated for model=Llama:8B prompt_len=31
2025-08-20 15:22:22,755 INFO inference run_from_config inference finished, wrote test_output.csv (rows=3) duration=0.00s
2025-08-20 15:22:22,756 INFO inference run_from_config start for inference, keys=['output_csv']
2025-08-20 15:22:22,757 ERROR inference missing config key: input_csv
2025-08-20 15:22:22,757 INFO inference run_from_config start for inference, keys=['input_csv']
2025-08-20 15:22:22,757 ERROR inference missing config key: output_csv
2025-08-20 15:22:22,758 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv']
2025-08-20 15:22:22,758 ERROR inference input CSV missing required 'prompt' column
2025-08-20 15:22:22,759 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv']
2025-08-20 15:22:22,759 ERROR inference failed to load input_csv: File not found
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 42, in run_from_config
    df = load_csv(input_csv)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1173, in _execute_mock_call
    raise effect
Exception: File not found
2025-08-20 15:22:46,338 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 15:22:46,340 INFO inference generated for model=google:Llama:8B prompt_len=12
2025-08-20 15:22:46,340 INFO src.io Wrote dataframe to /tmp/tmp_ix6sna6.csv (rows=1)
2025-08-20 15:22:46,341 INFO inference run_from_config inference finished, wrote /tmp/tmp_ix6sna6.csv (rows=1) duration=0.00s
2025-08-20 15:24:52,153 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 15:24:52,154 INFO inference generated for model=Llama:8B prompt_len=12
2025-08-20 15:24:52,155 INFO src.io Wrote dataframe to /tmp/tmpxwgq8w4u.csv (rows=1)
2025-08-20 15:24:52,155 INFO inference run_from_config inference finished, wrote /tmp/tmpxwgq8w4u.csv (rows=1) duration=0.00s
2025-08-20 15:32:07,741 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 15:32:07,742 INFO inference generated for model=Llama:8B prompt_len=26
2025-08-20 15:32:07,742 INFO inference generated for model=Llama:8B prompt_len=25
2025-08-20 15:32:07,743 INFO inference generated for model=Llama:8B prompt_len=31
2025-08-20 15:32:07,743 INFO inference run_from_config inference finished, wrote test_output.csv (rows=3) duration=0.00s
2025-08-20 15:32:07,746 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 15:32:07,746 INFO inference generated for model=Llama:8B prompt_len=26
2025-08-20 15:32:07,746 ERROR inference generation failed for model=Llama:8B: Ollama API error: context length exceeded
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 58, in run_from_config
    resp: ModelResponse = model.generate(prompt)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1177, in _execute_mock_call
    raise result
Exception: Ollama API error: context length exceeded
2025-08-20 15:32:07,748 INFO inference generated for model=Llama:8B prompt_len=31
2025-08-20 15:32:07,749 INFO inference run_from_config inference finished, wrote test_output.csv (rows=3) duration=0.00s
2025-08-20 15:32:07,752 INFO inference run_from_config start for inference, keys=['output_csv']
2025-08-20 15:32:07,752 ERROR inference missing config key: input_csv
2025-08-20 15:32:07,752 INFO inference run_from_config start for inference, keys=['input_csv']
2025-08-20 15:32:07,752 ERROR inference missing config key: output_csv
2025-08-20 15:32:07,754 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv']
2025-08-20 15:32:07,754 ERROR inference input CSV missing required 'prompt' column
2025-08-20 15:32:07,755 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv']
2025-08-20 15:32:07,755 ERROR inference failed to load input_csv: File not found
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 42, in run_from_config
    df = load_csv(input_csv)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1173, in _execute_mock_call
    raise effect
Exception: File not found
2025-08-20 15:32:07,759 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 15:32:07,761 INFO inference generated for model=Llama:8B prompt_len=12
2025-08-20 15:32:07,762 INFO src.io Wrote dataframe to /tmp/tmpd8_2sgdh.csv (rows=1)
2025-08-20 15:32:07,762 INFO inference run_from_config inference finished, wrote /tmp/tmpd8_2sgdh.csv (rows=1) duration=0.00s
2025-08-20 15:36:59,616 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir', 'criteria', 'scale']
2025-08-20 15:36:59,617 INFO judge run_from_config judge finished, wrote ratings.csv (rows=3) duration=0.00s
2025-08-20 15:36:59,631 INFO judge run_from_config start for judge, keys=['output_csv']
2025-08-20 15:36:59,631 ERROR judge missing config key: input_csv
2025-08-20 15:36:59,631 INFO judge run_from_config start for judge, keys=['input_csv']
2025-08-20 15:36:59,631 ERROR judge missing config key: output_csv
2025-08-20 15:36:59,632 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 15:36:59,632 WARNING judge template_dir does not exist: templates/default
2025-08-20 15:36:59,634 INFO src.io Wrote dataframe to output.csv (rows=2)
2025-08-20 15:36:59,634 INFO judge run_from_config judge finished, wrote output.csv (rows=2) duration=0.00s
2025-08-20 15:36:59,636 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 15:36:59,636 ERROR judge failed to load input_csv: File not found
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/judge.py", line 83, in run_from_config
    df = load_csv(input_csv)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1173, in _execute_mock_call
    raise effect
Exception: File not found
2025-08-20 15:40:20,854 WARNING judge template_dir does not exist: nonexistent_dir
2025-08-20 15:40:20,856 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 15:40:20,857 INFO judge run_from_config judge finished, wrote ratings.csv (rows=3) duration=0.00s
2025-08-20 15:40:20,859 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 15:40:20,859 INFO judge run_from_config judge finished, wrote ratings.csv (rows=1) duration=0.00s
2025-08-20 15:40:20,859 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 15:40:20,860 INFO judge run_from_config judge finished, wrote ratings.csv (rows=1) duration=0.00s
2025-08-20 15:40:20,861 INFO judge run_from_config start for judge, keys=['output_csv', 'template_dir']
2025-08-20 15:40:20,861 ERROR judge missing config key: input_csv
2025-08-20 15:40:20,861 INFO judge run_from_config start for judge, keys=['input_csv', 'template_dir']
2025-08-20 15:40:20,861 ERROR judge missing config key: output_csv
2025-08-20 15:40:20,861 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv']
2025-08-20 15:40:20,861 ERROR judge missing config key: template_dir
2025-08-20 15:40:20,862 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 15:40:20,862 ERROR judge failed to load input_csv: File not found
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/judge.py", line 83, in run_from_config
    df = load_csv(input_csv)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1173, in _execute_mock_call
    raise effect
Exception: File not found
2025-08-20 15:40:41,904 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 15:40:41,906 INFO src.io Wrote dataframe to /tmp/tmprs4vhe9l.csv (rows=1)
2025-08-20 15:40:41,906 INFO judge run_from_config judge finished, wrote /tmp/tmprs4vhe9l.csv (rows=1) duration=0.00s
2025-08-20 15:40:54,923 WARNING judge template_dir does not exist: nonexistent_dir
2025-08-20 15:40:54,926 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 15:40:54,929 INFO judge run_from_config judge finished, wrote ratings.csv (rows=3) duration=0.00s
2025-08-20 15:40:54,932 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 15:40:54,934 INFO judge run_from_config judge finished, wrote ratings.csv (rows=1) duration=0.00s
2025-08-20 15:40:54,934 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 15:40:54,935 INFO judge run_from_config judge finished, wrote ratings.csv (rows=1) duration=0.00s
2025-08-20 15:40:54,937 INFO judge run_from_config start for judge, keys=['output_csv', 'template_dir']
2025-08-20 15:40:54,937 ERROR judge missing config key: input_csv
2025-08-20 15:40:54,937 INFO judge run_from_config start for judge, keys=['input_csv', 'template_dir']
2025-08-20 15:40:54,937 ERROR judge missing config key: output_csv
2025-08-20 15:40:54,937 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv']
2025-08-20 15:40:54,937 ERROR judge missing config key: template_dir
2025-08-20 15:40:54,940 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 15:40:54,940 ERROR judge failed to load input_csv: File not found
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/judge.py", line 83, in run_from_config
    df = load_csv(input_csv)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1173, in _execute_mock_call
    raise effect
Exception: File not found
2025-08-20 15:40:54,945 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 15:40:54,948 INFO src.io Wrote dataframe to /tmp/tmpb4ro7lyc.csv (rows=1)
2025-08-20 15:40:54,948 INFO judge run_from_config judge finished, wrote /tmp/tmpb4ro7lyc.csv (rows=1) duration=0.00s
2025-08-20 15:57:41,251 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 15:57:41,252 INFO inference generated for model=Llama:8B prompt_len=26
2025-08-20 15:57:41,252 INFO inference generated for model=Llama:8B prompt_len=25
2025-08-20 15:57:41,252 INFO inference generated for model=Llama:8B prompt_len=31
2025-08-20 15:57:41,253 INFO inference run_from_config inference finished, wrote test_output.csv (rows=3) duration=0.00s
2025-08-20 15:57:41,254 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 15:57:41,255 INFO inference generated for model=Llama:8B prompt_len=26
2025-08-20 15:57:41,255 ERROR inference generation failed for model=Llama:8B: Ollama API error: context length exceeded
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 58, in run_from_config
    resp: ModelResponse = model.generate(prompt)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1177, in _execute_mock_call
    raise result
Exception: Ollama API error: context length exceeded
2025-08-20 15:57:41,256 INFO inference generated for model=Llama:8B prompt_len=31
2025-08-20 15:57:41,256 INFO inference run_from_config inference finished, wrote test_output.csv (rows=3) duration=0.00s
2025-08-20 15:57:41,257 INFO inference run_from_config start for inference, keys=['output_csv']
2025-08-20 15:57:41,257 ERROR inference missing config key: input_csv
2025-08-20 15:57:41,258 INFO inference run_from_config start for inference, keys=['input_csv']
2025-08-20 15:57:41,258 ERROR inference missing config key: output_csv
2025-08-20 15:57:41,259 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv']
2025-08-20 15:57:41,259 ERROR inference input CSV missing required 'prompt' column
2025-08-20 15:57:41,260 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv']
2025-08-20 15:57:41,260 ERROR inference failed to load input_csv: File not found
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 42, in run_from_config
    df = load_csv(input_csv)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1173, in _execute_mock_call
    raise effect
Exception: File not found
2025-08-20 15:57:41,263 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 15:57:41,264 INFO inference generated for model=Llama:8B prompt_len=12
2025-08-20 15:57:41,265 INFO src.io Wrote dataframe to /tmp/tmpbwlsi98s.csv (rows=1)
2025-08-20 15:57:41,265 INFO inference run_from_config inference finished, wrote /tmp/tmpbwlsi98s.csv (rows=1) duration=0.00s
2025-08-20 15:57:41,680 WARNING judge template_dir does not exist: nonexistent_dir
2025-08-20 15:57:41,682 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 15:57:41,683 INFO judge run_from_config judge finished, wrote ratings.csv (rows=3) duration=0.00s
2025-08-20 15:57:41,685 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 15:57:41,686 INFO judge run_from_config judge finished, wrote ratings.csv (rows=1) duration=0.00s
2025-08-20 15:57:41,686 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 15:57:41,687 INFO judge run_from_config judge finished, wrote ratings.csv (rows=1) duration=0.00s
2025-08-20 15:57:41,688 INFO judge run_from_config start for judge, keys=['output_csv', 'template_dir']
2025-08-20 15:57:41,688 ERROR judge missing config key: input_csv
2025-08-20 15:57:41,688 INFO judge run_from_config start for judge, keys=['input_csv', 'template_dir']
2025-08-20 15:57:41,688 ERROR judge missing config key: output_csv
2025-08-20 15:57:41,688 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv']
2025-08-20 15:57:41,688 ERROR judge missing config key: template_dir
2025-08-20 15:57:41,689 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 15:57:41,689 ERROR judge failed to load input_csv: File not found
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/judge.py", line 83, in run_from_config
    df = load_csv(input_csv)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1173, in _execute_mock_call
    raise effect
Exception: File not found
2025-08-20 15:57:41,693 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 15:57:41,695 INFO src.io Wrote dataframe to /tmp/tmp0khyhao4.csv (rows=1)
2025-08-20 15:57:41,695 INFO judge run_from_config judge finished, wrote /tmp/tmp0khyhao4.csv (rows=1) duration=0.00s
2025-08-20 15:57:53,046 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 15:57:53,047 INFO inference generated for model=Llama:8B prompt_len=26
2025-08-20 15:57:53,047 INFO inference generated for model=Llama:8B prompt_len=25
2025-08-20 15:57:53,048 INFO inference generated for model=Llama:8B prompt_len=31
2025-08-20 15:57:53,048 INFO inference run_from_config inference finished, wrote test_output.csv (rows=3) duration=0.00s
2025-08-20 15:57:53,051 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 15:57:53,051 INFO inference generated for model=Llama:8B prompt_len=26
2025-08-20 15:57:53,052 ERROR inference generation failed for model=Llama:8B: Ollama API error: context length exceeded
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 58, in run_from_config
    resp: ModelResponse = model.generate(prompt)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1177, in _execute_mock_call
    raise result
Exception: Ollama API error: context length exceeded
2025-08-20 15:57:53,053 INFO inference generated for model=Llama:8B prompt_len=31
2025-08-20 15:57:53,054 INFO inference run_from_config inference finished, wrote test_output.csv (rows=3) duration=0.00s
2025-08-20 15:57:53,055 INFO inference run_from_config start for inference, keys=['output_csv']
2025-08-20 15:57:53,055 ERROR inference missing config key: input_csv
2025-08-20 15:57:53,055 INFO inference run_from_config start for inference, keys=['input_csv']
2025-08-20 15:57:53,056 ERROR inference missing config key: output_csv
2025-08-20 15:57:53,057 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv']
2025-08-20 15:57:53,057 ERROR inference input CSV missing required 'prompt' column
2025-08-20 15:57:53,058 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv']
2025-08-20 15:57:53,058 ERROR inference failed to load input_csv: File not found
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 42, in run_from_config
    df = load_csv(input_csv)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1173, in _execute_mock_call
    raise effect
Exception: File not found
2025-08-20 15:57:53,063 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 15:57:53,064 INFO inference generated for model=Llama:8B prompt_len=12
2025-08-20 15:57:53,065 INFO src.io Wrote dataframe to /tmp/tmp2t0ncxke.csv (rows=1)
2025-08-20 15:57:53,065 INFO inference run_from_config inference finished, wrote /tmp/tmp2t0ncxke.csv (rows=1) duration=0.00s
2025-08-20 15:57:53,635 WARNING judge template_dir does not exist: nonexistent_dir
2025-08-20 15:57:53,637 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 15:57:53,640 INFO judge run_from_config judge finished, wrote ratings.csv (rows=3) duration=0.00s
2025-08-20 15:57:53,642 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 15:57:53,643 INFO judge run_from_config judge finished, wrote ratings.csv (rows=1) duration=0.00s
2025-08-20 15:57:53,643 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 15:57:53,644 INFO judge run_from_config judge finished, wrote ratings.csv (rows=1) duration=0.00s
2025-08-20 15:57:53,645 INFO judge run_from_config start for judge, keys=['output_csv', 'template_dir']
2025-08-20 15:57:53,645 ERROR judge missing config key: input_csv
2025-08-20 15:57:53,645 INFO judge run_from_config start for judge, keys=['input_csv', 'template_dir']
2025-08-20 15:57:53,646 ERROR judge missing config key: output_csv
2025-08-20 15:57:53,646 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv']
2025-08-20 15:57:53,646 ERROR judge missing config key: template_dir
2025-08-20 15:57:53,648 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 15:57:53,648 ERROR judge failed to load input_csv: File not found
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/judge.py", line 83, in run_from_config
    df = load_csv(input_csv)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1173, in _execute_mock_call
    raise effect
Exception: File not found
2025-08-20 15:57:53,652 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 15:57:53,654 INFO src.io Wrote dataframe to /tmp/tmppt03p9j3.csv (rows=1)
2025-08-20 15:57:53,654 INFO judge run_from_config judge finished, wrote /tmp/tmppt03p9j3.csv (rows=1) duration=0.00s
2025-08-20 15:59:43,818 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 15:59:43,819 INFO inference generated for model=Llama:8B prompt_len=26
2025-08-20 15:59:43,819 INFO inference generated for model=Llama:8B prompt_len=25
2025-08-20 15:59:43,819 INFO inference generated for model=Llama:8B prompt_len=31
2025-08-20 15:59:43,820 INFO inference run_from_config inference finished, wrote test_output.csv (rows=3) duration=0.00s
2025-08-20 15:59:43,822 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 15:59:43,822 INFO inference generated for model=Llama:8B prompt_len=26
2025-08-20 15:59:43,822 ERROR inference generation failed for model=Llama:8B: Ollama API error: context length exceeded
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 58, in run_from_config
    resp: ModelResponse = model.generate(prompt)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1177, in _execute_mock_call
    raise result
Exception: Ollama API error: context length exceeded
2025-08-20 15:59:43,823 INFO inference generated for model=Llama:8B prompt_len=31
2025-08-20 15:59:43,824 INFO inference run_from_config inference finished, wrote test_output.csv (rows=3) duration=0.00s
2025-08-20 15:59:43,826 INFO inference run_from_config start for inference, keys=['output_csv']
2025-08-20 15:59:43,826 ERROR inference missing config key: input_csv
2025-08-20 15:59:43,827 INFO inference run_from_config start for inference, keys=['input_csv']
2025-08-20 15:59:43,827 ERROR inference missing config key: output_csv
2025-08-20 15:59:43,829 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv']
2025-08-20 15:59:43,829 ERROR inference input CSV missing required 'prompt' column
2025-08-20 15:59:43,831 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv']
2025-08-20 15:59:43,831 ERROR inference failed to load input_csv: File not found
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 42, in run_from_config
    df = load_csv(input_csv)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1173, in _execute_mock_call
    raise effect
Exception: File not found
2025-08-20 15:59:43,835 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 15:59:43,836 INFO inference generated for model=Llama:8B prompt_len=12
2025-08-20 15:59:43,836 INFO src.io Wrote dataframe to /tmp/tmp8uqoim6p.csv (rows=1)
2025-08-20 15:59:43,836 INFO inference run_from_config inference finished, wrote /tmp/tmp8uqoim6p.csv (rows=1) duration=0.00s
2025-08-20 15:59:44,249 WARNING judge template_dir does not exist: nonexistent_dir
2025-08-20 15:59:44,252 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 15:59:44,253 INFO judge run_from_config judge finished, wrote ratings.csv (rows=3) duration=0.00s
2025-08-20 15:59:44,255 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 15:59:44,256 INFO judge run_from_config judge finished, wrote ratings.csv (rows=1) duration=0.00s
2025-08-20 15:59:44,256 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 15:59:44,257 INFO judge run_from_config judge finished, wrote ratings.csv (rows=1) duration=0.00s
2025-08-20 15:59:44,258 INFO judge run_from_config start for judge, keys=['output_csv', 'template_dir']
2025-08-20 15:59:44,258 ERROR judge missing config key: input_csv
2025-08-20 15:59:44,258 INFO judge run_from_config start for judge, keys=['input_csv', 'template_dir']
2025-08-20 15:59:44,258 ERROR judge missing config key: output_csv
2025-08-20 15:59:44,258 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv']
2025-08-20 15:59:44,258 ERROR judge missing config key: template_dir
2025-08-20 15:59:44,259 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 15:59:44,259 ERROR judge failed to load input_csv: File not found
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/judge.py", line 83, in run_from_config
    df = load_csv(input_csv)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1173, in _execute_mock_call
    raise effect
Exception: File not found
2025-08-20 15:59:44,262 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 15:59:44,264 INFO src.io Wrote dataframe to /tmp/tmpbr_uddcz.csv (rows=1)
2025-08-20 15:59:44,264 INFO judge run_from_config judge finished, wrote /tmp/tmpbr_uddcz.csv (rows=1) duration=0.00s
2025-08-20 15:59:52,617 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 15:59:52,618 INFO inference generated for model=Llama:8B prompt_len=26
2025-08-20 15:59:52,618 INFO inference generated for model=Llama:8B prompt_len=25
2025-08-20 15:59:52,619 INFO inference generated for model=Llama:8B prompt_len=31
2025-08-20 15:59:52,619 INFO inference run_from_config inference finished, wrote test_output.csv (rows=3) duration=0.00s
2025-08-20 15:59:52,622 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 15:59:52,622 INFO inference generated for model=Llama:8B prompt_len=26
2025-08-20 15:59:52,623 ERROR inference generation failed for model=Llama:8B: Ollama API error: context length exceeded
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 58, in run_from_config
    resp: ModelResponse = model.generate(prompt)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1177, in _execute_mock_call
    raise result
Exception: Ollama API error: context length exceeded
2025-08-20 15:59:52,625 INFO inference generated for model=Llama:8B prompt_len=31
2025-08-20 15:59:52,626 INFO inference run_from_config inference finished, wrote test_output.csv (rows=3) duration=0.00s
2025-08-20 15:59:52,628 INFO inference run_from_config start for inference, keys=['output_csv']
2025-08-20 15:59:52,628 ERROR inference missing config key: input_csv
2025-08-20 15:59:52,628 INFO inference run_from_config start for inference, keys=['input_csv']
2025-08-20 15:59:52,628 ERROR inference missing config key: output_csv
2025-08-20 15:59:52,629 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv']
2025-08-20 15:59:52,630 ERROR inference input CSV missing required 'prompt' column
2025-08-20 15:59:52,631 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv']
2025-08-20 15:59:52,631 ERROR inference failed to load input_csv: File not found
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 42, in run_from_config
    df = load_csv(input_csv)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1173, in _execute_mock_call
    raise effect
Exception: File not found
2025-08-20 15:59:52,636 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 15:59:52,638 INFO inference generated for model=Llama:8B prompt_len=12
2025-08-20 15:59:52,639 INFO src.io Wrote dataframe to /tmp/tmppm2h6zd8.csv (rows=1)
2025-08-20 15:59:52,639 INFO inference run_from_config inference finished, wrote /tmp/tmppm2h6zd8.csv (rows=1) duration=0.00s
2025-08-20 15:59:53,169 WARNING judge template_dir does not exist: nonexistent_dir
2025-08-20 15:59:53,172 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 15:59:53,174 INFO judge run_from_config judge finished, wrote ratings.csv (rows=3) duration=0.00s
2025-08-20 15:59:53,177 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 15:59:53,177 INFO judge run_from_config judge finished, wrote ratings.csv (rows=1) duration=0.00s
2025-08-20 15:59:53,178 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 15:59:53,178 INFO judge run_from_config judge finished, wrote ratings.csv (rows=1) duration=0.00s
2025-08-20 15:59:53,179 INFO judge run_from_config start for judge, keys=['output_csv', 'template_dir']
2025-08-20 15:59:53,180 ERROR judge missing config key: input_csv
2025-08-20 15:59:53,180 INFO judge run_from_config start for judge, keys=['input_csv', 'template_dir']
2025-08-20 15:59:53,180 ERROR judge missing config key: output_csv
2025-08-20 15:59:53,180 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv']
2025-08-20 15:59:53,180 ERROR judge missing config key: template_dir
2025-08-20 15:59:53,182 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 15:59:53,182 ERROR judge failed to load input_csv: File not found
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/judge.py", line 83, in run_from_config
    df = load_csv(input_csv)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1173, in _execute_mock_call
    raise effect
Exception: File not found
2025-08-20 15:59:53,186 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 15:59:53,189 INFO src.io Wrote dataframe to /tmp/tmp1w94su75.csv (rows=1)
2025-08-20 15:59:53,189 INFO judge run_from_config judge finished, wrote /tmp/tmp1w94su75.csv (rows=1) duration=0.00s
2025-08-20 16:02:38,119 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 16:02:38,119 INFO inference generated for model=Llama:8B prompt_len=26
2025-08-20 16:02:38,119 INFO inference generated for model=Llama:8B prompt_len=25
2025-08-20 16:02:38,120 INFO inference generated for model=Llama:8B prompt_len=31
2025-08-20 16:02:38,120 INFO inference run_from_config inference finished, wrote test_output.csv (rows=3) duration=0.00s
2025-08-20 16:02:38,122 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 16:02:38,122 INFO inference generated for model=Llama:8B prompt_len=26
2025-08-20 16:02:38,122 ERROR inference generation failed for model=Llama:8B: Ollama API error: context length exceeded
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 58, in run_from_config
    resp: ModelResponse = model.generate(prompt)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1177, in _execute_mock_call
    raise result
Exception: Ollama API error: context length exceeded
2025-08-20 16:02:38,123 INFO inference generated for model=Llama:8B prompt_len=31
2025-08-20 16:02:38,123 INFO inference run_from_config inference finished, wrote test_output.csv (rows=3) duration=0.00s
2025-08-20 16:02:38,125 INFO inference run_from_config start for inference, keys=['output_csv']
2025-08-20 16:02:38,125 ERROR inference missing config key: input_csv
2025-08-20 16:02:38,126 INFO inference run_from_config start for inference, keys=['input_csv']
2025-08-20 16:02:38,126 ERROR inference missing config key: output_csv
2025-08-20 16:02:38,127 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv']
2025-08-20 16:02:38,127 ERROR inference input CSV missing required 'prompt' column
2025-08-20 16:02:38,128 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv']
2025-08-20 16:02:38,128 ERROR inference failed to load input_csv: File not found
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 42, in run_from_config
    df = load_csv(input_csv)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1173, in _execute_mock_call
    raise effect
Exception: File not found
2025-08-20 16:02:38,130 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 16:02:38,131 INFO inference generated for model=Llama:8B prompt_len=12
2025-08-20 16:02:38,132 INFO src.io Wrote dataframe to /tmp/tmp_qpyxaai.csv (rows=1)
2025-08-20 16:02:38,132 INFO inference run_from_config inference finished, wrote /tmp/tmp_qpyxaai.csv (rows=1) duration=0.00s
2025-08-20 16:02:38,518 WARNING judge template_dir does not exist: nonexistent_dir
2025-08-20 16:02:38,520 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:02:38,522 INFO judge run_from_config judge finished, wrote ratings.csv (rows=3) duration=0.00s
2025-08-20 16:02:38,523 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:02:38,524 INFO judge run_from_config judge finished, wrote ratings.csv (rows=1) duration=0.00s
2025-08-20 16:02:38,524 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:02:38,524 INFO judge run_from_config judge finished, wrote ratings.csv (rows=1) duration=0.00s
2025-08-20 16:02:38,525 INFO judge run_from_config start for judge, keys=['output_csv', 'template_dir']
2025-08-20 16:02:38,525 ERROR judge missing config key: input_csv
2025-08-20 16:02:38,526 INFO judge run_from_config start for judge, keys=['input_csv', 'template_dir']
2025-08-20 16:02:38,526 ERROR judge missing config key: output_csv
2025-08-20 16:02:38,526 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv']
2025-08-20 16:02:38,526 ERROR judge missing config key: template_dir
2025-08-20 16:02:38,527 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:02:38,527 ERROR judge failed to load input_csv: File not found
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/judge.py", line 83, in run_from_config
    df = load_csv(input_csv)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1173, in _execute_mock_call
    raise effect
Exception: File not found
2025-08-20 16:02:38,529 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:02:38,531 INFO src.io Wrote dataframe to /tmp/tmpu7568ur4.csv (rows=1)
2025-08-20 16:02:38,531 INFO judge run_from_config judge finished, wrote /tmp/tmpu7568ur4.csv (rows=1) duration=0.00s
2025-08-20 16:02:40,483 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 16:02:40,484 INFO inference generated for model=Llama:8B prompt_len=26
2025-08-20 16:02:40,484 INFO inference generated for model=Llama:8B prompt_len=25
2025-08-20 16:02:40,484 INFO inference generated for model=Llama:8B prompt_len=31
2025-08-20 16:02:40,485 INFO inference run_from_config inference finished, wrote test_output.csv (rows=3) duration=0.00s
2025-08-20 16:02:40,487 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 16:02:40,487 INFO inference generated for model=Llama:8B prompt_len=26
2025-08-20 16:02:40,488 ERROR inference generation failed for model=Llama:8B: Ollama API error: context length exceeded
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 58, in run_from_config
    resp: ModelResponse = model.generate(prompt)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1177, in _execute_mock_call
    raise result
Exception: Ollama API error: context length exceeded
2025-08-20 16:02:40,489 INFO inference generated for model=Llama:8B prompt_len=31
2025-08-20 16:02:40,490 INFO inference run_from_config inference finished, wrote test_output.csv (rows=3) duration=0.00s
2025-08-20 16:02:40,491 INFO inference run_from_config start for inference, keys=['output_csv']
2025-08-20 16:02:40,491 ERROR inference missing config key: input_csv
2025-08-20 16:02:40,492 INFO inference run_from_config start for inference, keys=['input_csv']
2025-08-20 16:02:40,492 ERROR inference missing config key: output_csv
2025-08-20 16:02:40,493 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv']
2025-08-20 16:02:40,493 ERROR inference input CSV missing required 'prompt' column
2025-08-20 16:02:40,494 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv']
2025-08-20 16:02:40,494 ERROR inference failed to load input_csv: File not found
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 42, in run_from_config
    df = load_csv(input_csv)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1173, in _execute_mock_call
    raise effect
Exception: File not found
2025-08-20 16:02:40,498 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 16:02:40,500 INFO inference generated for model=Llama:8B prompt_len=12
2025-08-20 16:02:40,500 INFO src.io Wrote dataframe to /tmp/tmpleq_6csl.csv (rows=1)
2025-08-20 16:02:40,500 INFO inference run_from_config inference finished, wrote /tmp/tmpleq_6csl.csv (rows=1) duration=0.00s
2025-08-20 16:02:41,017 WARNING judge template_dir does not exist: nonexistent_dir
2025-08-20 16:02:41,020 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:02:41,021 INFO judge run_from_config judge finished, wrote ratings.csv (rows=3) duration=0.00s
2025-08-20 16:02:41,024 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:02:41,024 INFO judge run_from_config judge finished, wrote ratings.csv (rows=1) duration=0.00s
2025-08-20 16:02:41,025 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:02:41,025 INFO judge run_from_config judge finished, wrote ratings.csv (rows=1) duration=0.00s
2025-08-20 16:02:41,027 INFO judge run_from_config start for judge, keys=['output_csv', 'template_dir']
2025-08-20 16:02:41,027 ERROR judge missing config key: input_csv
2025-08-20 16:02:41,027 INFO judge run_from_config start for judge, keys=['input_csv', 'template_dir']
2025-08-20 16:02:41,027 ERROR judge missing config key: output_csv
2025-08-20 16:02:41,027 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv']
2025-08-20 16:02:41,027 ERROR judge missing config key: template_dir
2025-08-20 16:02:41,029 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:02:41,029 ERROR judge failed to load input_csv: File not found
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/judge.py", line 83, in run_from_config
    df = load_csv(input_csv)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1173, in _execute_mock_call
    raise effect
Exception: File not found
2025-08-20 16:02:41,032 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:02:41,034 INFO src.io Wrote dataframe to /tmp/tmp0j9vqlca.csv (rows=1)
2025-08-20 16:02:41,035 INFO judge run_from_config judge finished, wrote /tmp/tmp0j9vqlca.csv (rows=1) duration=0.00s
2025-08-20 16:02:56,076 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 16:02:56,077 INFO inference generated for model=Llama:8B prompt_len=26
2025-08-20 16:02:56,077 INFO inference generated for model=Llama:8B prompt_len=25
2025-08-20 16:02:56,077 INFO inference generated for model=Llama:8B prompt_len=31
2025-08-20 16:02:56,078 INFO inference run_from_config inference finished, wrote test_output.csv (rows=3) duration=0.00s
2025-08-20 16:02:56,080 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 16:02:56,080 INFO inference generated for model=Llama:8B prompt_len=26
2025-08-20 16:02:56,080 ERROR inference generation failed for model=Llama:8B: Ollama API error: context length exceeded
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 58, in run_from_config
    resp: ModelResponse = model.generate(prompt)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1177, in _execute_mock_call
    raise result
Exception: Ollama API error: context length exceeded
2025-08-20 16:02:56,081 INFO inference generated for model=Llama:8B prompt_len=31
2025-08-20 16:02:56,081 INFO inference run_from_config inference finished, wrote test_output.csv (rows=3) duration=0.00s
2025-08-20 16:02:56,082 INFO inference run_from_config start for inference, keys=['output_csv']
2025-08-20 16:02:56,082 ERROR inference missing config key: input_csv
2025-08-20 16:02:56,083 INFO inference run_from_config start for inference, keys=['input_csv']
2025-08-20 16:02:56,083 ERROR inference missing config key: output_csv
2025-08-20 16:02:56,084 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv']
2025-08-20 16:02:56,084 ERROR inference input CSV missing required 'prompt' column
2025-08-20 16:02:56,085 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv']
2025-08-20 16:02:56,085 ERROR inference failed to load input_csv: File not found
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 42, in run_from_config
    df = load_csv(input_csv)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1173, in _execute_mock_call
    raise effect
Exception: File not found
2025-08-20 16:02:56,088 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 16:02:56,089 INFO inference generated for model=Llama:8B prompt_len=12
2025-08-20 16:02:56,089 INFO src.io Wrote dataframe to /tmp/tmp_4pmgf4g.csv (rows=1)
2025-08-20 16:02:56,089 INFO inference run_from_config inference finished, wrote /tmp/tmp_4pmgf4g.csv (rows=1) duration=0.00s
2025-08-20 16:02:56,500 WARNING judge template_dir does not exist: nonexistent_dir
2025-08-20 16:02:56,502 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:02:56,504 INFO judge run_from_config judge finished, wrote ratings.csv (rows=3) duration=0.00s
2025-08-20 16:02:56,505 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:02:56,506 INFO judge run_from_config judge finished, wrote ratings.csv (rows=1) duration=0.00s
2025-08-20 16:02:56,506 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:02:56,507 INFO judge run_from_config judge finished, wrote ratings.csv (rows=1) duration=0.00s
2025-08-20 16:02:56,508 INFO judge run_from_config start for judge, keys=['output_csv', 'template_dir']
2025-08-20 16:02:56,508 ERROR judge missing config key: input_csv
2025-08-20 16:02:56,508 INFO judge run_from_config start for judge, keys=['input_csv', 'template_dir']
2025-08-20 16:02:56,509 ERROR judge missing config key: output_csv
2025-08-20 16:02:56,509 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv']
2025-08-20 16:02:56,509 ERROR judge missing config key: template_dir
2025-08-20 16:02:56,510 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:02:56,510 ERROR judge failed to load input_csv: File not found
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/judge.py", line 83, in run_from_config
    df = load_csv(input_csv)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1173, in _execute_mock_call
    raise effect
Exception: File not found
2025-08-20 16:02:56,513 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:02:56,516 INFO src.io Wrote dataframe to /tmp/tmpi5d7irje.csv (rows=1)
2025-08-20 16:02:56,516 INFO judge run_from_config judge finished, wrote /tmp/tmpi5d7irje.csv (rows=1) duration=0.00s
2025-08-20 16:02:58,544 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 16:02:58,545 INFO inference generated for model=Llama:8B prompt_len=26
2025-08-20 16:02:58,545 INFO inference generated for model=Llama:8B prompt_len=25
2025-08-20 16:02:58,546 INFO inference generated for model=Llama:8B prompt_len=31
2025-08-20 16:02:58,546 INFO inference run_from_config inference finished, wrote test_output.csv (rows=3) duration=0.00s
2025-08-20 16:02:58,549 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 16:02:58,549 INFO inference generated for model=Llama:8B prompt_len=26
2025-08-20 16:02:58,550 ERROR inference generation failed for model=Llama:8B: Ollama API error: context length exceeded
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 58, in run_from_config
    resp: ModelResponse = model.generate(prompt)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1177, in _execute_mock_call
    raise result
Exception: Ollama API error: context length exceeded
2025-08-20 16:02:58,551 INFO inference generated for model=Llama:8B prompt_len=31
2025-08-20 16:02:58,552 INFO inference run_from_config inference finished, wrote test_output.csv (rows=3) duration=0.00s
2025-08-20 16:02:58,553 INFO inference run_from_config start for inference, keys=['output_csv']
2025-08-20 16:02:58,554 ERROR inference missing config key: input_csv
2025-08-20 16:02:58,554 INFO inference run_from_config start for inference, keys=['input_csv']
2025-08-20 16:02:58,554 ERROR inference missing config key: output_csv
2025-08-20 16:02:58,555 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv']
2025-08-20 16:02:58,556 ERROR inference input CSV missing required 'prompt' column
2025-08-20 16:02:58,557 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv']
2025-08-20 16:02:58,557 ERROR inference failed to load input_csv: File not found
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 42, in run_from_config
    df = load_csv(input_csv)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1173, in _execute_mock_call
    raise effect
Exception: File not found
2025-08-20 16:02:58,561 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 16:02:58,562 INFO inference generated for model=Llama:8B prompt_len=12
2025-08-20 16:02:58,563 INFO src.io Wrote dataframe to /tmp/tmpry6jdpta.csv (rows=1)
2025-08-20 16:02:58,563 INFO inference run_from_config inference finished, wrote /tmp/tmpry6jdpta.csv (rows=1) duration=0.00s
2025-08-20 16:02:59,082 WARNING judge template_dir does not exist: nonexistent_dir
2025-08-20 16:02:59,085 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:02:59,087 INFO judge run_from_config judge finished, wrote ratings.csv (rows=3) duration=0.00s
2025-08-20 16:02:59,089 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:02:59,090 INFO judge run_from_config judge finished, wrote ratings.csv (rows=1) duration=0.00s
2025-08-20 16:02:59,090 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:02:59,090 INFO judge run_from_config judge finished, wrote ratings.csv (rows=1) duration=0.00s
2025-08-20 16:02:59,092 INFO judge run_from_config start for judge, keys=['output_csv', 'template_dir']
2025-08-20 16:02:59,092 ERROR judge missing config key: input_csv
2025-08-20 16:02:59,092 INFO judge run_from_config start for judge, keys=['input_csv', 'template_dir']
2025-08-20 16:02:59,092 ERROR judge missing config key: output_csv
2025-08-20 16:02:59,092 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv']
2025-08-20 16:02:59,093 ERROR judge missing config key: template_dir
2025-08-20 16:02:59,094 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:02:59,095 ERROR judge failed to load input_csv: File not found
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/judge.py", line 83, in run_from_config
    df = load_csv(input_csv)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1173, in _execute_mock_call
    raise effect
Exception: File not found
2025-08-20 16:02:59,100 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:02:59,102 INFO src.io Wrote dataframe to /tmp/tmpm6hri0ci.csv (rows=1)
2025-08-20 16:02:59,102 INFO judge run_from_config judge finished, wrote /tmp/tmpm6hri0ci.csv (rows=1) duration=0.00s
2025-08-20 16:05:15,665 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 16:05:15,665 INFO inference generated for model=Llama:8B prompt_len=26
2025-08-20 16:05:15,665 INFO inference generated for model=Llama:8B prompt_len=25
2025-08-20 16:05:15,666 INFO inference generated for model=Llama:8B prompt_len=31
2025-08-20 16:05:15,666 INFO inference run_from_config inference finished, wrote test_output.csv (rows=3) duration=0.00s
2025-08-20 16:05:15,668 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 16:05:15,668 INFO inference generated for model=Llama:8B prompt_len=26
2025-08-20 16:05:15,668 ERROR inference generation failed for model=Llama:8B: Ollama API error: context length exceeded
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 58, in run_from_config
    resp: ModelResponse = model.generate(prompt)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1177, in _execute_mock_call
    raise result
Exception: Ollama API error: context length exceeded
2025-08-20 16:05:15,669 INFO inference generated for model=Llama:8B prompt_len=31
2025-08-20 16:05:15,669 INFO inference run_from_config inference finished, wrote test_output.csv (rows=3) duration=0.00s
2025-08-20 16:05:15,670 INFO inference run_from_config start for inference, keys=['output_csv']
2025-08-20 16:05:15,670 ERROR inference missing config key: input_csv
2025-08-20 16:05:15,671 INFO inference run_from_config start for inference, keys=['input_csv']
2025-08-20 16:05:15,671 ERROR inference missing config key: output_csv
2025-08-20 16:05:15,672 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv']
2025-08-20 16:05:15,672 ERROR inference input CSV missing required 'prompt' column
2025-08-20 16:05:15,673 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv']
2025-08-20 16:05:15,673 ERROR inference failed to load input_csv: File not found
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 42, in run_from_config
    df = load_csv(input_csv)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1173, in _execute_mock_call
    raise effect
Exception: File not found
2025-08-20 16:05:15,676 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 16:05:15,678 INFO inference generated for model=Llama:8B prompt_len=12
2025-08-20 16:05:15,679 INFO src.io Wrote dataframe to /tmp/tmpaul8v_9m.csv (rows=1)
2025-08-20 16:05:15,679 INFO inference run_from_config inference finished, wrote /tmp/tmpaul8v_9m.csv (rows=1) duration=0.00s
2025-08-20 16:05:16,100 WARNING judge template_dir does not exist: nonexistent_dir
2025-08-20 16:05:16,103 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:05:16,104 INFO judge run_from_config judge finished, wrote ratings.csv (rows=3) duration=0.00s
2025-08-20 16:05:16,105 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:05:16,106 INFO judge run_from_config judge finished, wrote ratings.csv (rows=1) duration=0.00s
2025-08-20 16:05:16,106 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:05:16,106 INFO judge run_from_config judge finished, wrote ratings.csv (rows=1) duration=0.00s
2025-08-20 16:05:16,107 INFO judge run_from_config start for judge, keys=['output_csv', 'template_dir']
2025-08-20 16:05:16,108 ERROR judge missing config key: input_csv
2025-08-20 16:05:16,108 INFO judge run_from_config start for judge, keys=['input_csv', 'template_dir']
2025-08-20 16:05:16,108 ERROR judge missing config key: output_csv
2025-08-20 16:05:16,108 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv']
2025-08-20 16:05:16,108 ERROR judge missing config key: template_dir
2025-08-20 16:05:16,109 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:05:16,109 ERROR judge failed to load input_csv: File not found
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/judge.py", line 83, in run_from_config
    df = load_csv(input_csv)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1173, in _execute_mock_call
    raise effect
Exception: File not found
2025-08-20 16:05:16,112 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:05:16,115 INFO src.io Wrote dataframe to /tmp/tmp6kl6v4yi.csv (rows=1)
2025-08-20 16:05:16,116 INFO judge run_from_config judge finished, wrote /tmp/tmp6kl6v4yi.csv (rows=1) duration=0.00s
2025-08-20 16:05:22,316 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 16:05:22,317 INFO inference generated for model=Llama:8B prompt_len=26
2025-08-20 16:05:22,317 INFO inference generated for model=Llama:8B prompt_len=25
2025-08-20 16:05:22,317 INFO inference generated for model=Llama:8B prompt_len=31
2025-08-20 16:05:22,318 INFO inference run_from_config inference finished, wrote test_output.csv (rows=3) duration=0.00s
2025-08-20 16:05:22,320 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 16:05:22,320 INFO inference generated for model=Llama:8B prompt_len=26
2025-08-20 16:05:22,321 ERROR inference generation failed for model=Llama:8B: Ollama API error: context length exceeded
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 58, in run_from_config
    resp: ModelResponse = model.generate(prompt)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1177, in _execute_mock_call
    raise result
Exception: Ollama API error: context length exceeded
2025-08-20 16:05:22,322 INFO inference generated for model=Llama:8B prompt_len=31
2025-08-20 16:05:22,322 INFO inference run_from_config inference finished, wrote test_output.csv (rows=3) duration=0.00s
2025-08-20 16:05:22,324 INFO inference run_from_config start for inference, keys=['output_csv']
2025-08-20 16:05:22,324 ERROR inference missing config key: input_csv
2025-08-20 16:05:22,324 INFO inference run_from_config start for inference, keys=['input_csv']
2025-08-20 16:05:22,324 ERROR inference missing config key: output_csv
2025-08-20 16:05:22,325 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv']
2025-08-20 16:05:22,325 ERROR inference input CSV missing required 'prompt' column
2025-08-20 16:05:22,327 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv']
2025-08-20 16:05:22,327 ERROR inference failed to load input_csv: File not found
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 42, in run_from_config
    df = load_csv(input_csv)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1173, in _execute_mock_call
    raise effect
Exception: File not found
2025-08-20 16:05:22,330 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 16:05:22,331 INFO inference generated for model=Llama:8B prompt_len=12
2025-08-20 16:05:22,332 INFO src.io Wrote dataframe to /tmp/tmpnnaudbrg.csv (rows=1)
2025-08-20 16:05:22,332 INFO inference run_from_config inference finished, wrote /tmp/tmpnnaudbrg.csv (rows=1) duration=0.00s
2025-08-20 16:05:22,813 WARNING judge template_dir does not exist: nonexistent_dir
2025-08-20 16:05:22,815 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:05:22,817 INFO judge run_from_config judge finished, wrote ratings.csv (rows=3) duration=0.00s
2025-08-20 16:05:22,819 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:05:22,820 INFO judge run_from_config judge finished, wrote ratings.csv (rows=1) duration=0.00s
2025-08-20 16:05:22,820 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:05:22,820 INFO judge run_from_config judge finished, wrote ratings.csv (rows=1) duration=0.00s
2025-08-20 16:05:22,822 INFO judge run_from_config start for judge, keys=['output_csv', 'template_dir']
2025-08-20 16:05:22,822 ERROR judge missing config key: input_csv
2025-08-20 16:05:22,822 INFO judge run_from_config start for judge, keys=['input_csv', 'template_dir']
2025-08-20 16:05:22,822 ERROR judge missing config key: output_csv
2025-08-20 16:05:22,822 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv']
2025-08-20 16:05:22,822 ERROR judge missing config key: template_dir
2025-08-20 16:05:22,824 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:05:22,824 ERROR judge failed to load input_csv: File not found
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/judge.py", line 83, in run_from_config
    df = load_csv(input_csv)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1173, in _execute_mock_call
    raise effect
Exception: File not found
2025-08-20 16:05:22,829 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:05:22,831 INFO src.io Wrote dataframe to /tmp/tmpfupl7dst.csv (rows=1)
2025-08-20 16:05:22,831 INFO judge run_from_config judge finished, wrote /tmp/tmpfupl7dst.csv (rows=1) duration=0.00s
2025-08-20 16:09:02,515 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 16:09:02,516 INFO inference generated for model=Llama:8B prompt_len=26
2025-08-20 16:09:02,516 INFO inference generated for model=Llama:8B prompt_len=25
2025-08-20 16:09:02,516 INFO inference generated for model=Llama:8B prompt_len=31
2025-08-20 16:09:02,517 INFO inference run_from_config inference finished, wrote test_output.csv (rows=3) duration=0.00s
2025-08-20 16:09:02,518 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 16:09:02,519 INFO inference generated for model=Llama:8B prompt_len=26
2025-08-20 16:09:02,519 ERROR inference generation failed for model=Llama:8B: Ollama API error: context length exceeded
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 58, in run_from_config
    resp: ModelResponse = model.generate(prompt)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1177, in _execute_mock_call
    raise result
Exception: Ollama API error: context length exceeded
2025-08-20 16:09:02,520 INFO inference generated for model=Llama:8B prompt_len=31
2025-08-20 16:09:02,520 INFO inference run_from_config inference finished, wrote test_output.csv (rows=3) duration=0.00s
2025-08-20 16:09:02,521 INFO inference run_from_config start for inference, keys=['output_csv']
2025-08-20 16:09:02,521 ERROR inference missing config key: input_csv
2025-08-20 16:09:02,522 INFO inference run_from_config start for inference, keys=['input_csv']
2025-08-20 16:09:02,522 ERROR inference missing config key: output_csv
2025-08-20 16:09:02,523 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv']
2025-08-20 16:09:02,523 ERROR inference input CSV missing required 'prompt' column
2025-08-20 16:09:02,524 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv']
2025-08-20 16:09:02,524 ERROR inference failed to load input_csv: File not found
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 42, in run_from_config
    df = load_csv(input_csv)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1173, in _execute_mock_call
    raise effect
Exception: File not found
2025-08-20 16:09:02,527 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 16:09:02,528 INFO inference generated for model=Llama:8B prompt_len=12
2025-08-20 16:09:02,528 INFO src.io Wrote dataframe to /tmp/tmpk00lhgrn.csv (rows=1)
2025-08-20 16:09:02,528 INFO inference run_from_config inference finished, wrote /tmp/tmpk00lhgrn.csv (rows=1) duration=0.00s
2025-08-20 16:09:02,923 WARNING judge template_dir does not exist: nonexistent_dir
2025-08-20 16:09:02,926 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:09:02,927 INFO judge run_from_config judge finished, wrote ratings.csv (rows=3) duration=0.00s
2025-08-20 16:09:02,929 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:09:02,930 INFO judge run_from_config judge finished, wrote ratings.csv (rows=1) duration=0.00s
2025-08-20 16:09:02,930 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:09:02,931 INFO judge run_from_config judge finished, wrote ratings.csv (rows=1) duration=0.00s
2025-08-20 16:09:02,932 INFO judge run_from_config start for judge, keys=['output_csv', 'template_dir']
2025-08-20 16:09:02,932 ERROR judge missing config key: input_csv
2025-08-20 16:09:02,932 INFO judge run_from_config start for judge, keys=['input_csv', 'template_dir']
2025-08-20 16:09:02,932 ERROR judge missing config key: output_csv
2025-08-20 16:09:02,932 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv']
2025-08-20 16:09:02,932 ERROR judge missing config key: template_dir
2025-08-20 16:09:02,933 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:09:02,933 ERROR judge failed to load input_csv: File not found
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/judge.py", line 83, in run_from_config
    df = load_csv(input_csv)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1173, in _execute_mock_call
    raise effect
Exception: File not found
2025-08-20 16:09:02,936 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:09:02,938 INFO src.io Wrote dataframe to /tmp/tmpo76k7sqy.csv (rows=1)
2025-08-20 16:09:02,938 INFO judge run_from_config judge finished, wrote /tmp/tmpo76k7sqy.csv (rows=1) duration=0.00s
2025-08-20 16:09:08,536 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 16:09:08,537 INFO inference generated for model=Llama:8B prompt_len=26
2025-08-20 16:09:08,537 INFO inference generated for model=Llama:8B prompt_len=25
2025-08-20 16:09:08,537 INFO inference generated for model=Llama:8B prompt_len=31
2025-08-20 16:09:08,538 INFO inference run_from_config inference finished, wrote test_output.csv (rows=3) duration=0.00s
2025-08-20 16:09:08,540 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 16:09:08,541 INFO inference generated for model=Llama:8B prompt_len=26
2025-08-20 16:09:08,541 ERROR inference generation failed for model=Llama:8B: Ollama API error: context length exceeded
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 58, in run_from_config
    resp: ModelResponse = model.generate(prompt)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1177, in _execute_mock_call
    raise result
Exception: Ollama API error: context length exceeded
2025-08-20 16:09:08,543 INFO inference generated for model=Llama:8B prompt_len=31
2025-08-20 16:09:08,544 INFO inference run_from_config inference finished, wrote test_output.csv (rows=3) duration=0.00s
2025-08-20 16:09:08,545 INFO inference run_from_config start for inference, keys=['output_csv']
2025-08-20 16:09:08,545 ERROR inference missing config key: input_csv
2025-08-20 16:09:08,546 INFO inference run_from_config start for inference, keys=['input_csv']
2025-08-20 16:09:08,546 ERROR inference missing config key: output_csv
2025-08-20 16:09:08,547 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv']
2025-08-20 16:09:08,547 ERROR inference input CSV missing required 'prompt' column
2025-08-20 16:09:08,548 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv']
2025-08-20 16:09:08,548 ERROR inference failed to load input_csv: File not found
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 42, in run_from_config
    df = load_csv(input_csv)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1173, in _execute_mock_call
    raise effect
Exception: File not found
2025-08-20 16:09:08,552 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 16:09:08,553 INFO inference generated for model=Llama:8B prompt_len=12
2025-08-20 16:09:08,554 INFO src.io Wrote dataframe to /tmp/tmpd7npnihv.csv (rows=1)
2025-08-20 16:09:08,554 INFO inference run_from_config inference finished, wrote /tmp/tmpd7npnihv.csv (rows=1) duration=0.00s
2025-08-20 16:09:09,102 WARNING judge template_dir does not exist: nonexistent_dir
2025-08-20 16:09:09,105 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:09:09,107 INFO judge run_from_config judge finished, wrote ratings.csv (rows=3) duration=0.00s
2025-08-20 16:09:09,110 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:09:09,111 INFO judge run_from_config judge finished, wrote ratings.csv (rows=1) duration=0.00s
2025-08-20 16:09:09,111 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:09:09,112 INFO judge run_from_config judge finished, wrote ratings.csv (rows=1) duration=0.00s
2025-08-20 16:09:09,113 INFO judge run_from_config start for judge, keys=['output_csv', 'template_dir']
2025-08-20 16:09:09,113 ERROR judge missing config key: input_csv
2025-08-20 16:09:09,114 INFO judge run_from_config start for judge, keys=['input_csv', 'template_dir']
2025-08-20 16:09:09,114 ERROR judge missing config key: output_csv
2025-08-20 16:09:09,114 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv']
2025-08-20 16:09:09,114 ERROR judge missing config key: template_dir
2025-08-20 16:09:09,115 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:09:09,116 ERROR judge failed to load input_csv: File not found
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/judge.py", line 83, in run_from_config
    df = load_csv(input_csv)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1173, in _execute_mock_call
    raise effect
Exception: File not found
2025-08-20 16:09:09,119 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:09:09,122 INFO src.io Wrote dataframe to /tmp/tmps1hkvue6.csv (rows=1)
2025-08-20 16:09:09,122 INFO judge run_from_config judge finished, wrote /tmp/tmps1hkvue6.csv (rows=1) duration=0.00s
2025-08-20 16:10:24,607 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 16:10:24,608 INFO inference generated for model=Llama:8B prompt_len=26
2025-08-20 16:10:24,608 INFO inference generated for model=Llama:8B prompt_len=25
2025-08-20 16:10:24,608 INFO inference generated for model=Llama:8B prompt_len=31
2025-08-20 16:10:24,609 INFO inference run_from_config inference finished, wrote test_output.csv (rows=3) duration=0.00s
2025-08-20 16:10:24,611 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 16:10:24,611 INFO inference generated for model=Llama:8B prompt_len=26
2025-08-20 16:10:24,612 ERROR inference generation failed for model=Llama:8B: Ollama API error: context length exceeded
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 58, in run_from_config
    resp: ModelResponse = model.generate(prompt)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1177, in _execute_mock_call
    raise result
Exception: Ollama API error: context length exceeded
2025-08-20 16:10:24,612 INFO inference generated for model=Llama:8B prompt_len=31
2025-08-20 16:10:24,613 INFO inference run_from_config inference finished, wrote test_output.csv (rows=3) duration=0.00s
2025-08-20 16:10:24,614 INFO inference run_from_config start for inference, keys=['output_csv']
2025-08-20 16:10:24,614 ERROR inference missing config key: input_csv
2025-08-20 16:10:24,615 INFO inference run_from_config start for inference, keys=['input_csv']
2025-08-20 16:10:24,615 ERROR inference missing config key: output_csv
2025-08-20 16:10:24,616 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv']
2025-08-20 16:10:24,616 ERROR inference input CSV missing required 'prompt' column
2025-08-20 16:10:24,617 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv']
2025-08-20 16:10:24,617 ERROR inference failed to load input_csv: File not found
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 42, in run_from_config
    df = load_csv(input_csv)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1173, in _execute_mock_call
    raise effect
Exception: File not found
2025-08-20 16:10:24,620 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 16:10:24,621 INFO inference generated for model=Llama:8B prompt_len=12
2025-08-20 16:10:24,621 INFO src.io Wrote dataframe to /tmp/tmp83xg7b9b.csv (rows=1)
2025-08-20 16:10:24,622 INFO inference run_from_config inference finished, wrote /tmp/tmp83xg7b9b.csv (rows=1) duration=0.00s
2025-08-20 16:10:25,041 WARNING judge template_dir does not exist: nonexistent_dir
2025-08-20 16:10:25,045 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:10:25,048 INFO judge run_from_config judge finished, wrote ratings.csv (rows=3) duration=0.00s
2025-08-20 16:10:25,051 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:10:25,052 INFO judge run_from_config judge finished, wrote ratings.csv (rows=1) duration=0.00s
2025-08-20 16:10:25,052 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:10:25,052 INFO judge run_from_config judge finished, wrote ratings.csv (rows=1) duration=0.00s
2025-08-20 16:10:25,053 INFO judge run_from_config start for judge, keys=['output_csv', 'template_dir']
2025-08-20 16:10:25,054 ERROR judge missing config key: input_csv
2025-08-20 16:10:25,054 INFO judge run_from_config start for judge, keys=['input_csv', 'template_dir']
2025-08-20 16:10:25,054 ERROR judge missing config key: output_csv
2025-08-20 16:10:25,054 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv']
2025-08-20 16:10:25,054 ERROR judge missing config key: template_dir
2025-08-20 16:10:25,055 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:10:25,055 ERROR judge failed to load input_csv: File not found
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/judge.py", line 83, in run_from_config
    df = load_csv(input_csv)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1173, in _execute_mock_call
    raise effect
Exception: File not found
2025-08-20 16:10:25,059 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:10:25,061 INFO src.io Wrote dataframe to /tmp/tmpwbbwjkse.csv (rows=1)
2025-08-20 16:10:25,062 INFO judge run_from_config judge finished, wrote /tmp/tmpwbbwjkse.csv (rows=1) duration=0.00s
2025-08-20 16:10:32,826 INFO analytics loading input_csv: ratings.csv
2025-08-20 16:10:32,826 INFO analytics aggregating data by ['model']
2025-08-20 16:10:32,826 ERROR analytics error during analytics: DataAggregator.aggregate() got multiple values for argument 'group_by'
2025-08-20 16:10:32,836 INFO analytics loading input_csv: ratings.csv
2025-08-20 16:10:32,836 INFO analytics aggregating data by ['model']
2025-08-20 16:10:32,836 ERROR analytics error during analytics: DataAggregator.aggregate() got multiple values for argument 'group_by'
2025-08-20 16:10:32,841 ERROR analytics missing config key: input_csv
2025-08-20 16:10:32,841 ERROR analytics missing config key: output_csv
2025-08-20 16:10:32,841 ERROR analytics missing config key: group_by
2025-08-20 16:10:32,842 INFO analytics loading input_csv: input.csv
2025-08-20 16:10:32,843 ERROR analytics input data is empty
2025-08-20 16:10:32,844 INFO analytics loading input_csv: nonexistent.csv
2025-08-20 16:10:32,844 ERROR analytics failed to load input_csv: File not found
2025-08-20 16:10:32,846 INFO analytics loading input_csv: /tmp/tmpxrt6sa2g.csv
2025-08-20 16:10:32,848 INFO analytics aggregating data by ['model']
2025-08-20 16:10:32,848 ERROR analytics error during analytics: DataAggregator.aggregate() got multiple values for argument 'group_by'
2025-08-20 16:10:33,294 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model']
2025-08-20 16:10:33,296 INFO inference generated for model=Llama:8B prompt_len=12
2025-08-20 16:10:33,296 INFO inference generated for model=Llama:8B prompt_len=12
2025-08-20 16:10:33,296 INFO inference generated for model=gpt-3.5-turbo prompt_len=12
2025-08-20 16:10:33,296 INFO inference generated for model=gpt-3.5-turbo prompt_len=12
2025-08-20 16:10:33,297 INFO src.io Wrote dataframe to /tmp/tmph2skrafg/completions.csv (rows=4)
2025-08-20 16:10:33,298 INFO inference run_from_config inference finished, wrote /tmp/tmph2skrafg/completions.csv (rows=4) duration=0.00s
2025-08-20 16:10:33,298 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:10:33,302 INFO src.io Wrote dataframe to /tmp/tmph2skrafg/ratings.csv (rows=4)
2025-08-20 16:10:33,302 INFO judge run_from_config judge finished, wrote /tmp/tmph2skrafg/ratings.csv (rows=4) duration=0.00s
2025-08-20 16:10:33,304 INFO analytics loading input_csv: /tmp/tmph2skrafg/ratings.csv
2025-08-20 16:10:33,305 INFO analytics aggregating data by ['model']
2025-08-20 16:10:33,305 ERROR analytics error during analytics: DataAggregator.aggregate() got multiple values for argument 'group_by'
2025-08-20 16:10:33,874 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 16:10:33,875 INFO inference generated for model=Llama:8B prompt_len=26
2025-08-20 16:10:33,875 INFO inference generated for model=Llama:8B prompt_len=25
2025-08-20 16:10:33,876 INFO inference generated for model=Llama:8B prompt_len=31
2025-08-20 16:10:33,877 INFO inference run_from_config inference finished, wrote test_output.csv (rows=3) duration=0.00s
2025-08-20 16:10:33,879 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 16:10:33,880 INFO inference generated for model=Llama:8B prompt_len=26
2025-08-20 16:10:33,880 ERROR inference generation failed for model=Llama:8B: Ollama API error: context length exceeded
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 58, in run_from_config
    resp: ModelResponse = model.generate(prompt)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1177, in _execute_mock_call
    raise result
Exception: Ollama API error: context length exceeded
2025-08-20 16:10:33,883 INFO inference generated for model=Llama:8B prompt_len=31
2025-08-20 16:10:33,883 INFO inference run_from_config inference finished, wrote test_output.csv (rows=3) duration=0.00s
2025-08-20 16:10:33,885 INFO inference run_from_config start for inference, keys=['output_csv']
2025-08-20 16:10:33,885 ERROR inference missing config key: input_csv
2025-08-20 16:10:33,885 INFO inference run_from_config start for inference, keys=['input_csv']
2025-08-20 16:10:33,886 ERROR inference missing config key: output_csv
2025-08-20 16:10:33,887 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv']
2025-08-20 16:10:33,887 ERROR inference input CSV missing required 'prompt' column
2025-08-20 16:10:33,888 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv']
2025-08-20 16:10:33,888 ERROR inference failed to load input_csv: File not found
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 42, in run_from_config
    df = load_csv(input_csv)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1173, in _execute_mock_call
    raise effect
Exception: File not found
2025-08-20 16:10:33,894 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 16:10:33,895 INFO inference generated for model=Llama:8B prompt_len=12
2025-08-20 16:10:33,896 INFO src.io Wrote dataframe to /tmp/tmp89u7dvn6.csv (rows=1)
2025-08-20 16:10:33,896 INFO inference run_from_config inference finished, wrote /tmp/tmp89u7dvn6.csv (rows=1) duration=0.00s
2025-08-20 16:10:34,485 WARNING judge template_dir does not exist: nonexistent_dir
2025-08-20 16:10:34,488 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:10:34,489 INFO judge run_from_config judge finished, wrote ratings.csv (rows=3) duration=0.00s
2025-08-20 16:10:34,492 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:10:34,493 INFO judge run_from_config judge finished, wrote ratings.csv (rows=1) duration=0.00s
2025-08-20 16:10:34,493 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:10:34,494 INFO judge run_from_config judge finished, wrote ratings.csv (rows=1) duration=0.00s
2025-08-20 16:10:34,496 INFO judge run_from_config start for judge, keys=['output_csv', 'template_dir']
2025-08-20 16:10:34,496 ERROR judge missing config key: input_csv
2025-08-20 16:10:34,496 INFO judge run_from_config start for judge, keys=['input_csv', 'template_dir']
2025-08-20 16:10:34,497 ERROR judge missing config key: output_csv
2025-08-20 16:10:34,497 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv']
2025-08-20 16:10:34,497 ERROR judge missing config key: template_dir
2025-08-20 16:10:34,498 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:10:34,499 ERROR judge failed to load input_csv: File not found
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/judge.py", line 83, in run_from_config
    df = load_csv(input_csv)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1173, in _execute_mock_call
    raise effect
Exception: File not found
2025-08-20 16:10:34,503 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:10:34,505 INFO src.io Wrote dataframe to /tmp/tmpx6xd3t5w.csv (rows=1)
2025-08-20 16:10:34,505 INFO judge run_from_config judge finished, wrote /tmp/tmpx6xd3t5w.csv (rows=1) duration=0.00s
2025-08-20 16:10:43,231 INFO analytics loading input_csv: ratings.csv
2025-08-20 16:10:43,232 INFO analytics aggregating data by ['model']
2025-08-20 16:10:43,232 ERROR analytics error during analytics: DataAggregator.aggregate() got multiple values for argument 'group_by'
2025-08-20 16:10:43,242 INFO analytics loading input_csv: ratings.csv
2025-08-20 16:10:43,242 INFO analytics aggregating data by ['model']
2025-08-20 16:10:43,242 ERROR analytics error during analytics: DataAggregator.aggregate() got multiple values for argument 'group_by'
2025-08-20 16:10:43,250 ERROR analytics missing config key: input_csv
2025-08-20 16:10:43,250 ERROR analytics missing config key: output_csv
2025-08-20 16:10:43,250 ERROR analytics missing config key: group_by
2025-08-20 16:10:43,252 INFO analytics loading input_csv: input.csv
2025-08-20 16:10:43,252 ERROR analytics input data is empty
2025-08-20 16:10:43,253 INFO analytics loading input_csv: nonexistent.csv
2025-08-20 16:10:43,253 ERROR analytics failed to load input_csv: File not found
2025-08-20 16:11:32,545 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 16:11:32,546 INFO inference generated for model=Llama:8B prompt_len=26
2025-08-20 16:11:32,546 INFO inference generated for model=Llama:8B prompt_len=25
2025-08-20 16:11:32,546 INFO inference generated for model=Llama:8B prompt_len=31
2025-08-20 16:11:32,547 INFO inference run_from_config inference finished, wrote test_output.csv (rows=3) duration=0.00s
2025-08-20 16:11:32,548 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 16:11:32,548 INFO inference generated for model=Llama:8B prompt_len=26
2025-08-20 16:11:32,549 ERROR inference generation failed for model=Llama:8B: Ollama API error: context length exceeded
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 58, in run_from_config
    resp: ModelResponse = model.generate(prompt)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1177, in _execute_mock_call
    raise result
Exception: Ollama API error: context length exceeded
2025-08-20 16:11:32,550 INFO inference generated for model=Llama:8B prompt_len=31
2025-08-20 16:11:32,550 INFO inference run_from_config inference finished, wrote test_output.csv (rows=3) duration=0.00s
2025-08-20 16:11:32,551 INFO inference run_from_config start for inference, keys=['output_csv']
2025-08-20 16:11:32,552 ERROR inference missing config key: input_csv
2025-08-20 16:11:32,552 INFO inference run_from_config start for inference, keys=['input_csv']
2025-08-20 16:11:32,552 ERROR inference missing config key: output_csv
2025-08-20 16:11:32,553 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv']
2025-08-20 16:11:32,554 ERROR inference input CSV missing required 'prompt' column
2025-08-20 16:11:32,555 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv']
2025-08-20 16:11:32,555 ERROR inference failed to load input_csv: File not found
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 42, in run_from_config
    df = load_csv(input_csv)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1173, in _execute_mock_call
    raise effect
Exception: File not found
2025-08-20 16:11:32,559 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 16:11:32,561 INFO inference generated for model=Llama:8B prompt_len=12
2025-08-20 16:11:32,561 INFO src.io Wrote dataframe to /tmp/tmph9chhpg7.csv (rows=1)
2025-08-20 16:11:32,561 INFO inference run_from_config inference finished, wrote /tmp/tmph9chhpg7.csv (rows=1) duration=0.00s
2025-08-20 16:11:32,994 WARNING judge template_dir does not exist: nonexistent_dir
2025-08-20 16:11:32,996 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:11:32,997 INFO judge run_from_config judge finished, wrote ratings.csv (rows=3) duration=0.00s
2025-08-20 16:11:32,999 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:11:32,999 INFO judge run_from_config judge finished, wrote ratings.csv (rows=1) duration=0.00s
2025-08-20 16:11:32,999 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:11:33,000 INFO judge run_from_config judge finished, wrote ratings.csv (rows=1) duration=0.00s
2025-08-20 16:11:33,001 INFO judge run_from_config start for judge, keys=['output_csv', 'template_dir']
2025-08-20 16:11:33,001 ERROR judge missing config key: input_csv
2025-08-20 16:11:33,001 INFO judge run_from_config start for judge, keys=['input_csv', 'template_dir']
2025-08-20 16:11:33,001 ERROR judge missing config key: output_csv
2025-08-20 16:11:33,001 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv']
2025-08-20 16:11:33,001 ERROR judge missing config key: template_dir
2025-08-20 16:11:33,002 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:11:33,002 ERROR judge failed to load input_csv: File not found
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/judge.py", line 83, in run_from_config
    df = load_csv(input_csv)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1173, in _execute_mock_call
    raise effect
Exception: File not found
2025-08-20 16:11:33,005 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:11:33,007 INFO src.io Wrote dataframe to /tmp/tmpv8e3s6t1.csv (rows=1)
2025-08-20 16:11:33,007 INFO judge run_from_config judge finished, wrote /tmp/tmpv8e3s6t1.csv (rows=1) duration=0.00s
2025-08-20 16:11:42,249 INFO analytics loading input_csv: ratings.csv
2025-08-20 16:11:42,249 INFO analytics aggregating data by ['model']
2025-08-20 16:11:42,252 ERROR analytics error during analytics: 'Series' object has no attribute 'columns'
2025-08-20 16:11:42,259 INFO analytics loading input_csv: ratings.csv
2025-08-20 16:11:42,259 INFO analytics aggregating data by ['model']
2025-08-20 16:11:42,260 ERROR analytics error during analytics: 'Series' object has no attribute 'columns'
2025-08-20 16:11:42,266 ERROR analytics missing config key: input_csv
2025-08-20 16:11:42,266 ERROR analytics missing config key: output_csv
2025-08-20 16:11:42,266 ERROR analytics missing config key: group_by
2025-08-20 16:11:42,267 INFO analytics loading input_csv: input.csv
2025-08-20 16:11:42,267 ERROR analytics input data is empty
2025-08-20 16:11:42,268 INFO analytics loading input_csv: nonexistent.csv
2025-08-20 16:11:42,268 ERROR analytics failed to load input_csv: File not found
2025-08-20 16:11:42,273 INFO analytics loading input_csv: /tmp/tmph9noj8e5.csv
2025-08-20 16:11:42,274 INFO analytics aggregating data by ['model']
2025-08-20 16:11:42,276 ERROR analytics error during analytics: 'Series' object has no attribute 'columns'
2025-08-20 16:11:42,714 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model']
2025-08-20 16:11:42,716 INFO inference generated for model=Llama:8B prompt_len=12
2025-08-20 16:11:42,716 INFO inference generated for model=Llama:8B prompt_len=12
2025-08-20 16:11:42,716 INFO inference generated for model=gpt-3.5-turbo prompt_len=12
2025-08-20 16:11:42,716 INFO inference generated for model=gpt-3.5-turbo prompt_len=12
2025-08-20 16:11:42,717 INFO src.io Wrote dataframe to /tmp/tmp2_056d5t/completions.csv (rows=4)
2025-08-20 16:11:42,717 INFO inference run_from_config inference finished, wrote /tmp/tmp2_056d5t/completions.csv (rows=4) duration=0.00s
2025-08-20 16:11:42,717 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:11:42,721 INFO src.io Wrote dataframe to /tmp/tmp2_056d5t/ratings.csv (rows=4)
2025-08-20 16:11:42,721 INFO judge run_from_config judge finished, wrote /tmp/tmp2_056d5t/ratings.csv (rows=4) duration=0.00s
2025-08-20 16:11:42,722 INFO analytics loading input_csv: /tmp/tmp2_056d5t/ratings.csv
2025-08-20 16:11:42,722 INFO analytics aggregating data by ['model']
2025-08-20 16:11:42,724 ERROR analytics error during analytics: 'Series' object has no attribute 'columns'
2025-08-20 16:11:43,295 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 16:11:43,296 INFO inference generated for model=Llama:8B prompt_len=26
2025-08-20 16:11:43,296 INFO inference generated for model=Llama:8B prompt_len=25
2025-08-20 16:11:43,296 INFO inference generated for model=Llama:8B prompt_len=31
2025-08-20 16:11:43,297 INFO inference run_from_config inference finished, wrote test_output.csv (rows=3) duration=0.00s
2025-08-20 16:11:43,299 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 16:11:43,299 INFO inference generated for model=Llama:8B prompt_len=26
2025-08-20 16:11:43,300 ERROR inference generation failed for model=Llama:8B: Ollama API error: context length exceeded
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 58, in run_from_config
    resp: ModelResponse = model.generate(prompt)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1177, in _execute_mock_call
    raise result
Exception: Ollama API error: context length exceeded
2025-08-20 16:11:43,302 INFO inference generated for model=Llama:8B prompt_len=31
2025-08-20 16:11:43,302 INFO inference run_from_config inference finished, wrote test_output.csv (rows=3) duration=0.00s
2025-08-20 16:11:43,304 INFO inference run_from_config start for inference, keys=['output_csv']
2025-08-20 16:11:43,304 ERROR inference missing config key: input_csv
2025-08-20 16:11:43,304 INFO inference run_from_config start for inference, keys=['input_csv']
2025-08-20 16:11:43,304 ERROR inference missing config key: output_csv
2025-08-20 16:11:43,305 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv']
2025-08-20 16:11:43,306 ERROR inference input CSV missing required 'prompt' column
2025-08-20 16:11:43,307 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv']
2025-08-20 16:11:43,307 ERROR inference failed to load input_csv: File not found
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 42, in run_from_config
    df = load_csv(input_csv)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1173, in _execute_mock_call
    raise effect
Exception: File not found
2025-08-20 16:11:43,310 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 16:11:43,312 INFO inference generated for model=Llama:8B prompt_len=12
2025-08-20 16:11:43,313 INFO src.io Wrote dataframe to /tmp/tmpkk02po19.csv (rows=1)
2025-08-20 16:11:43,313 INFO inference run_from_config inference finished, wrote /tmp/tmpkk02po19.csv (rows=1) duration=0.00s
2025-08-20 16:11:43,879 WARNING judge template_dir does not exist: nonexistent_dir
2025-08-20 16:11:43,882 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:11:43,884 INFO judge run_from_config judge finished, wrote ratings.csv (rows=3) duration=0.00s
2025-08-20 16:11:43,886 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:11:43,887 INFO judge run_from_config judge finished, wrote ratings.csv (rows=1) duration=0.00s
2025-08-20 16:11:43,887 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:11:43,888 INFO judge run_from_config judge finished, wrote ratings.csv (rows=1) duration=0.00s
2025-08-20 16:11:43,890 INFO judge run_from_config start for judge, keys=['output_csv', 'template_dir']
2025-08-20 16:11:43,890 ERROR judge missing config key: input_csv
2025-08-20 16:11:43,890 INFO judge run_from_config start for judge, keys=['input_csv', 'template_dir']
2025-08-20 16:11:43,890 ERROR judge missing config key: output_csv
2025-08-20 16:11:43,890 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv']
2025-08-20 16:11:43,890 ERROR judge missing config key: template_dir
2025-08-20 16:11:43,892 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:11:43,892 ERROR judge failed to load input_csv: File not found
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/judge.py", line 83, in run_from_config
    df = load_csv(input_csv)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1173, in _execute_mock_call
    raise effect
Exception: File not found
2025-08-20 16:11:43,897 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:11:43,900 INFO src.io Wrote dataframe to /tmp/tmpw29v0vpe.csv (rows=1)
2025-08-20 16:11:43,900 INFO judge run_from_config judge finished, wrote /tmp/tmpw29v0vpe.csv (rows=1) duration=0.00s
2025-08-20 16:11:51,007 INFO analytics loading input_csv: ratings.csv
2025-08-20 16:11:51,008 INFO analytics aggregating data by ['model']
2025-08-20 16:11:51,010 ERROR analytics error during analytics: 'Series' object has no attribute 'columns'
2025-08-20 16:11:51,019 INFO analytics loading input_csv: ratings.csv
2025-08-20 16:11:51,019 INFO analytics aggregating data by ['model']
2025-08-20 16:11:51,021 ERROR analytics error during analytics: 'Series' object has no attribute 'columns'
2025-08-20 16:11:51,031 ERROR analytics missing config key: input_csv
2025-08-20 16:11:51,032 ERROR analytics missing config key: output_csv
2025-08-20 16:11:51,032 ERROR analytics missing config key: group_by
2025-08-20 16:11:51,033 INFO analytics loading input_csv: input.csv
2025-08-20 16:11:51,033 ERROR analytics input data is empty
2025-08-20 16:11:51,034 INFO analytics loading input_csv: nonexistent.csv
2025-08-20 16:11:51,034 ERROR analytics failed to load input_csv: File not found
2025-08-20 16:12:42,638 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 16:12:42,639 INFO inference generated for model=Llama:8B prompt_len=26
2025-08-20 16:12:42,639 INFO inference generated for model=Llama:8B prompt_len=25
2025-08-20 16:12:42,640 INFO inference generated for model=Llama:8B prompt_len=31
2025-08-20 16:12:42,640 INFO inference run_from_config inference finished, wrote test_output.csv (rows=3) duration=0.00s
2025-08-20 16:12:42,642 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 16:12:42,643 INFO inference generated for model=Llama:8B prompt_len=26
2025-08-20 16:12:42,643 ERROR inference generation failed for model=Llama:8B: Ollama API error: context length exceeded
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 58, in run_from_config
    resp: ModelResponse = model.generate(prompt)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1177, in _execute_mock_call
    raise result
Exception: Ollama API error: context length exceeded
2025-08-20 16:12:42,643 INFO inference generated for model=Llama:8B prompt_len=31
2025-08-20 16:12:42,644 INFO inference run_from_config inference finished, wrote test_output.csv (rows=3) duration=0.00s
2025-08-20 16:12:42,645 INFO inference run_from_config start for inference, keys=['output_csv']
2025-08-20 16:12:42,645 ERROR inference missing config key: input_csv
2025-08-20 16:12:42,646 INFO inference run_from_config start for inference, keys=['input_csv']
2025-08-20 16:12:42,646 ERROR inference missing config key: output_csv
2025-08-20 16:12:42,647 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv']
2025-08-20 16:12:42,647 ERROR inference input CSV missing required 'prompt' column
2025-08-20 16:12:42,648 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv']
2025-08-20 16:12:42,648 ERROR inference failed to load input_csv: File not found
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 42, in run_from_config
    df = load_csv(input_csv)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1173, in _execute_mock_call
    raise effect
Exception: File not found
2025-08-20 16:12:42,651 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 16:12:42,652 INFO inference generated for model=Llama:8B prompt_len=12
2025-08-20 16:12:42,653 INFO src.io Wrote dataframe to /tmp/tmp33x_rju5.csv (rows=1)
2025-08-20 16:12:42,653 INFO inference run_from_config inference finished, wrote /tmp/tmp33x_rju5.csv (rows=1) duration=0.00s
2025-08-20 16:12:43,112 WARNING judge template_dir does not exist: nonexistent_dir
2025-08-20 16:12:43,114 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:12:43,115 INFO judge run_from_config judge finished, wrote ratings.csv (rows=3) duration=0.00s
2025-08-20 16:12:43,117 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:12:43,118 INFO judge run_from_config judge finished, wrote ratings.csv (rows=1) duration=0.00s
2025-08-20 16:12:43,118 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:12:43,118 INFO judge run_from_config judge finished, wrote ratings.csv (rows=1) duration=0.00s
2025-08-20 16:12:43,119 INFO judge run_from_config start for judge, keys=['output_csv', 'template_dir']
2025-08-20 16:12:43,119 ERROR judge missing config key: input_csv
2025-08-20 16:12:43,120 INFO judge run_from_config start for judge, keys=['input_csv', 'template_dir']
2025-08-20 16:12:43,120 ERROR judge missing config key: output_csv
2025-08-20 16:12:43,120 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv']
2025-08-20 16:12:43,120 ERROR judge missing config key: template_dir
2025-08-20 16:12:43,121 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:12:43,121 ERROR judge failed to load input_csv: File not found
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/judge.py", line 83, in run_from_config
    df = load_csv(input_csv)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1173, in _execute_mock_call
    raise effect
Exception: File not found
2025-08-20 16:12:43,123 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:12:43,126 INFO src.io Wrote dataframe to /tmp/tmpr1drsde7.csv (rows=1)
2025-08-20 16:12:43,126 INFO judge run_from_config judge finished, wrote /tmp/tmpr1drsde7.csv (rows=1) duration=0.00s
2025-08-20 16:12:51,118 INFO analytics loading input_csv: ratings.csv
2025-08-20 16:12:51,118 INFO analytics aggregating data by ['model']
2025-08-20 16:12:51,121 WARNING analytics Using fallback aggregation: 'Series' object has no attribute 'columns'
2025-08-20 16:12:51,121 INFO analytics writing output to analysis.csv
2025-08-20 16:12:51,123 INFO analytics loading input_csv: ratings.csv
2025-08-20 16:12:51,123 INFO analytics aggregating data by ['model']
2025-08-20 16:12:51,124 WARNING analytics Using fallback aggregation: 'Series' object has no attribute 'columns'
2025-08-20 16:12:51,125 INFO analytics writing output to analysis.csv
2025-08-20 16:12:51,126 ERROR analytics missing config key: input_csv
2025-08-20 16:12:51,126 ERROR analytics missing config key: output_csv
2025-08-20 16:12:51,127 ERROR analytics missing config key: group_by
2025-08-20 16:12:51,128 INFO analytics loading input_csv: input.csv
2025-08-20 16:12:51,128 ERROR analytics input data is empty
2025-08-20 16:12:51,129 INFO analytics loading input_csv: nonexistent.csv
2025-08-20 16:12:51,129 ERROR analytics failed to load input_csv: File not found
2025-08-20 16:12:51,131 INFO analytics loading input_csv: /tmp/tmp8zt878b6.csv
2025-08-20 16:12:51,132 INFO analytics aggregating data by ['model']
2025-08-20 16:12:51,134 WARNING analytics Using fallback aggregation: 'Series' object has no attribute 'columns'
2025-08-20 16:12:51,134 INFO analytics writing output to /tmp/tmpa7fifnb8.csv
2025-08-20 16:12:51,135 INFO src.io Wrote dataframe to /tmp/tmpa7fifnb8.csv (rows=2)
2025-08-20 16:12:51,570 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model']
2025-08-20 16:12:51,571 INFO inference generated for model=Llama:8B prompt_len=12
2025-08-20 16:12:51,571 INFO inference generated for model=Llama:8B prompt_len=12
2025-08-20 16:12:51,571 INFO inference generated for model=gpt-3.5-turbo prompt_len=12
2025-08-20 16:12:51,571 INFO inference generated for model=gpt-3.5-turbo prompt_len=12
2025-08-20 16:12:51,572 INFO src.io Wrote dataframe to /tmp/tmp3tffhzo1/completions.csv (rows=4)
2025-08-20 16:12:51,572 INFO inference run_from_config inference finished, wrote /tmp/tmp3tffhzo1/completions.csv (rows=4) duration=0.00s
2025-08-20 16:12:51,572 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:12:51,574 INFO src.io Wrote dataframe to /tmp/tmp3tffhzo1/ratings.csv (rows=4)
2025-08-20 16:12:51,574 INFO judge run_from_config judge finished, wrote /tmp/tmp3tffhzo1/ratings.csv (rows=4) duration=0.00s
2025-08-20 16:12:51,575 INFO analytics loading input_csv: /tmp/tmp3tffhzo1/ratings.csv
2025-08-20 16:12:51,576 INFO analytics aggregating data by ['model']
2025-08-20 16:12:51,578 WARNING analytics Using fallback aggregation: 'Series' object has no attribute 'columns'
2025-08-20 16:12:51,579 INFO analytics writing output to /tmp/tmp3tffhzo1/analysis.csv
2025-08-20 16:12:51,579 INFO src.io Wrote dataframe to /tmp/tmp3tffhzo1/analysis.csv (rows=2)
2025-08-20 16:12:52,092 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 16:12:52,093 INFO inference generated for model=Llama:8B prompt_len=26
2025-08-20 16:12:52,093 INFO inference generated for model=Llama:8B prompt_len=25
2025-08-20 16:12:52,093 INFO inference generated for model=Llama:8B prompt_len=31
2025-08-20 16:12:52,094 INFO inference run_from_config inference finished, wrote test_output.csv (rows=3) duration=0.00s
2025-08-20 16:12:52,096 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 16:12:52,096 INFO inference generated for model=Llama:8B prompt_len=26
2025-08-20 16:12:52,097 ERROR inference generation failed for model=Llama:8B: Ollama API error: context length exceeded
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 58, in run_from_config
    resp: ModelResponse = model.generate(prompt)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1177, in _execute_mock_call
    raise result
Exception: Ollama API error: context length exceeded
2025-08-20 16:12:52,098 INFO inference generated for model=Llama:8B prompt_len=31
2025-08-20 16:12:52,099 INFO inference run_from_config inference finished, wrote test_output.csv (rows=3) duration=0.00s
2025-08-20 16:12:52,101 INFO inference run_from_config start for inference, keys=['output_csv']
2025-08-20 16:12:52,101 ERROR inference missing config key: input_csv
2025-08-20 16:12:52,101 INFO inference run_from_config start for inference, keys=['input_csv']
2025-08-20 16:12:52,101 ERROR inference missing config key: output_csv
2025-08-20 16:12:52,103 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv']
2025-08-20 16:12:52,103 ERROR inference input CSV missing required 'prompt' column
2025-08-20 16:12:52,105 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv']
2025-08-20 16:12:52,105 ERROR inference failed to load input_csv: File not found
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 42, in run_from_config
    df = load_csv(input_csv)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1173, in _execute_mock_call
    raise effect
Exception: File not found
2025-08-20 16:12:52,110 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 16:12:52,111 INFO inference generated for model=Llama:8B prompt_len=12
2025-08-20 16:12:52,112 INFO src.io Wrote dataframe to /tmp/tmpsmmkegyn.csv (rows=1)
2025-08-20 16:12:52,112 INFO inference run_from_config inference finished, wrote /tmp/tmpsmmkegyn.csv (rows=1) duration=0.00s
2025-08-20 16:12:52,676 WARNING judge template_dir does not exist: nonexistent_dir
2025-08-20 16:12:52,679 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:12:52,680 INFO judge run_from_config judge finished, wrote ratings.csv (rows=3) duration=0.00s
2025-08-20 16:12:52,683 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:12:52,684 INFO judge run_from_config judge finished, wrote ratings.csv (rows=1) duration=0.00s
2025-08-20 16:12:52,684 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:12:52,685 INFO judge run_from_config judge finished, wrote ratings.csv (rows=1) duration=0.00s
2025-08-20 16:12:52,687 INFO judge run_from_config start for judge, keys=['output_csv', 'template_dir']
2025-08-20 16:12:52,687 ERROR judge missing config key: input_csv
2025-08-20 16:12:52,687 INFO judge run_from_config start for judge, keys=['input_csv', 'template_dir']
2025-08-20 16:12:52,688 ERROR judge missing config key: output_csv
2025-08-20 16:12:52,688 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv']
2025-08-20 16:12:52,688 ERROR judge missing config key: template_dir
2025-08-20 16:12:52,689 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:12:52,689 ERROR judge failed to load input_csv: File not found
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/judge.py", line 83, in run_from_config
    df = load_csv(input_csv)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1173, in _execute_mock_call
    raise effect
Exception: File not found
2025-08-20 16:12:52,694 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:12:52,696 INFO src.io Wrote dataframe to /tmp/tmp0vdvo55l.csv (rows=1)
2025-08-20 16:12:52,696 INFO judge run_from_config judge finished, wrote /tmp/tmp0vdvo55l.csv (rows=1) duration=0.00s
2025-08-20 16:13:00,041 INFO analytics loading input_csv: ratings.csv
2025-08-20 16:13:00,041 INFO analytics aggregating data by ['model']
2025-08-20 16:13:00,043 WARNING analytics Using fallback aggregation: 'Series' object has no attribute 'columns'
2025-08-20 16:13:00,044 INFO analytics writing output to analysis.csv
2025-08-20 16:13:00,046 INFO analytics loading input_csv: ratings.csv
2025-08-20 16:13:00,046 INFO analytics aggregating data by ['model']
2025-08-20 16:13:00,049 WARNING analytics Using fallback aggregation: 'Series' object has no attribute 'columns'
2025-08-20 16:13:00,050 INFO analytics writing output to analysis.csv
2025-08-20 16:13:00,052 ERROR analytics missing config key: input_csv
2025-08-20 16:13:00,052 ERROR analytics missing config key: output_csv
2025-08-20 16:13:00,052 ERROR analytics missing config key: group_by
2025-08-20 16:13:00,053 INFO analytics loading input_csv: input.csv
2025-08-20 16:13:00,053 ERROR analytics input data is empty
2025-08-20 16:13:00,054 INFO analytics loading input_csv: nonexistent.csv
2025-08-20 16:13:00,054 ERROR analytics failed to load input_csv: File not found
2025-08-20 16:16:38,681 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 16:16:38,682 INFO inference generated for model=Llama:8B prompt_len=26
2025-08-20 16:16:38,682 INFO inference generated for model=Llama:8B prompt_len=25
2025-08-20 16:16:38,682 INFO inference generated for model=Llama:8B prompt_len=31
2025-08-20 16:16:38,683 INFO inference run_from_config inference finished, wrote test_output.csv (rows=3) duration=0.00s
2025-08-20 16:16:38,684 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 16:16:38,685 INFO inference generated for model=Llama:8B prompt_len=26
2025-08-20 16:16:38,685 ERROR inference generation failed for model=Llama:8B: Ollama API error: context length exceeded
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 58, in run_from_config
    resp: ModelResponse = model.generate(prompt)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1177, in _execute_mock_call
    raise result
Exception: Ollama API error: context length exceeded
2025-08-20 16:16:38,687 INFO inference generated for model=Llama:8B prompt_len=31
2025-08-20 16:16:38,687 INFO inference run_from_config inference finished, wrote test_output.csv (rows=3) duration=0.00s
2025-08-20 16:16:38,688 INFO inference run_from_config start for inference, keys=['output_csv']
2025-08-20 16:16:38,689 ERROR inference missing config key: input_csv
2025-08-20 16:16:38,689 INFO inference run_from_config start for inference, keys=['input_csv']
2025-08-20 16:16:38,689 ERROR inference missing config key: output_csv
2025-08-20 16:16:38,690 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv']
2025-08-20 16:16:38,690 ERROR inference input CSV missing required 'prompt' column
2025-08-20 16:16:38,691 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv']
2025-08-20 16:16:38,691 ERROR inference failed to load input_csv: File not found
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 42, in run_from_config
    df = load_csv(input_csv)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1173, in _execute_mock_call
    raise effect
Exception: File not found
2025-08-20 16:16:38,695 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 16:16:38,696 INFO inference generated for model=Llama:8B prompt_len=12
2025-08-20 16:16:38,697 INFO src.io Wrote dataframe to /tmp/tmp95y2ahy8.csv (rows=1)
2025-08-20 16:16:38,697 INFO inference run_from_config inference finished, wrote /tmp/tmp95y2ahy8.csv (rows=1) duration=0.00s
2025-08-20 16:16:39,128 WARNING judge template_dir does not exist: nonexistent_dir
2025-08-20 16:16:39,131 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:16:39,132 INFO judge run_from_config judge finished, wrote ratings.csv (rows=3) duration=0.00s
2025-08-20 16:16:39,134 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:16:39,135 INFO judge run_from_config judge finished, wrote ratings.csv (rows=1) duration=0.00s
2025-08-20 16:16:39,135 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:16:39,136 INFO judge run_from_config judge finished, wrote ratings.csv (rows=1) duration=0.00s
2025-08-20 16:16:39,137 INFO judge run_from_config start for judge, keys=['output_csv', 'template_dir']
2025-08-20 16:16:39,137 ERROR judge missing config key: input_csv
2025-08-20 16:16:39,137 INFO judge run_from_config start for judge, keys=['input_csv', 'template_dir']
2025-08-20 16:16:39,137 ERROR judge missing config key: output_csv
2025-08-20 16:16:39,137 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv']
2025-08-20 16:16:39,137 ERROR judge missing config key: template_dir
2025-08-20 16:16:39,138 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:16:39,138 ERROR judge failed to load input_csv: File not found
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/judge.py", line 83, in run_from_config
    df = load_csv(input_csv)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1173, in _execute_mock_call
    raise effect
Exception: File not found
2025-08-20 16:16:39,141 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:16:39,143 INFO src.io Wrote dataframe to /tmp/tmps5wa7ymq.csv (rows=1)
2025-08-20 16:16:39,143 INFO judge run_from_config judge finished, wrote /tmp/tmps5wa7ymq.csv (rows=1) duration=0.00s
2025-08-20 16:16:48,057 INFO analytics loading input_csv: ratings.csv
2025-08-20 16:16:48,057 INFO analytics aggregating data by ['model']
2025-08-20 16:16:48,059 WARNING analytics Using fallback aggregation: 'Series' object has no attribute 'columns'
2025-08-20 16:16:48,060 INFO analytics writing output to analysis.csv
2025-08-20 16:16:48,063 INFO analytics loading input_csv: ratings.csv
2025-08-20 16:16:48,063 INFO analytics aggregating data by ['model']
2025-08-20 16:16:48,065 WARNING analytics Using fallback aggregation: 'Series' object has no attribute 'columns'
2025-08-20 16:16:48,066 INFO analytics writing output to analysis.csv
2025-08-20 16:16:48,068 ERROR analytics missing config key: input_csv
2025-08-20 16:16:48,068 ERROR analytics missing config key: output_csv
2025-08-20 16:16:48,068 ERROR analytics missing config key: group_by
2025-08-20 16:16:48,070 INFO analytics loading input_csv: input.csv
2025-08-20 16:16:48,070 ERROR analytics input data is empty
2025-08-20 16:16:48,071 INFO analytics loading input_csv: nonexistent.csv
2025-08-20 16:16:48,072 ERROR analytics failed to load input_csv: File not found
2025-08-20 16:16:48,075 INFO analytics loading input_csv: /tmp/tmpq8czza96.csv
2025-08-20 16:16:48,076 INFO analytics aggregating data by ['model']
2025-08-20 16:16:48,078 WARNING analytics Using fallback aggregation: 'Series' object has no attribute 'columns'
2025-08-20 16:16:48,078 INFO analytics writing output to /tmp/tmpqztdnwm8.csv
2025-08-20 16:16:48,079 INFO src.io Wrote dataframe to /tmp/tmpqztdnwm8.csv (rows=2)
2025-08-20 16:16:48,543 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model']
2025-08-20 16:16:48,544 INFO inference generated for model=Llama:8B prompt_len=12
2025-08-20 16:16:48,545 INFO inference generated for model=Llama:8B prompt_len=12
2025-08-20 16:16:48,545 INFO inference generated for model=gpt-3.5-turbo prompt_len=12
2025-08-20 16:16:48,545 INFO inference generated for model=gpt-3.5-turbo prompt_len=12
2025-08-20 16:16:48,546 INFO src.io Wrote dataframe to /tmp/tmpt5jl322i/completions.csv (rows=4)
2025-08-20 16:16:48,546 INFO inference run_from_config inference finished, wrote /tmp/tmpt5jl322i/completions.csv (rows=4) duration=0.00s
2025-08-20 16:16:48,546 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:16:48,548 INFO src.io Wrote dataframe to /tmp/tmpt5jl322i/ratings.csv (rows=4)
2025-08-20 16:16:48,548 INFO judge run_from_config judge finished, wrote /tmp/tmpt5jl322i/ratings.csv (rows=4) duration=0.00s
2025-08-20 16:16:48,549 INFO analytics loading input_csv: /tmp/tmpt5jl322i/ratings.csv
2025-08-20 16:16:48,550 INFO analytics aggregating data by ['model']
2025-08-20 16:16:48,551 WARNING analytics Using fallback aggregation: 'Series' object has no attribute 'columns'
2025-08-20 16:16:48,552 INFO analytics writing output to /tmp/tmpt5jl322i/analysis.csv
2025-08-20 16:16:48,553 INFO src.io Wrote dataframe to /tmp/tmpt5jl322i/analysis.csv (rows=2)
2025-08-20 16:16:49,125 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 16:16:49,125 INFO inference generated for model=Llama:8B prompt_len=26
2025-08-20 16:16:49,126 INFO inference generated for model=Llama:8B prompt_len=25
2025-08-20 16:16:49,126 INFO inference generated for model=Llama:8B prompt_len=31
2025-08-20 16:16:49,127 INFO inference run_from_config inference finished, wrote test_output.csv (rows=3) duration=0.00s
2025-08-20 16:16:49,129 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 16:16:49,129 INFO inference generated for model=Llama:8B prompt_len=26
2025-08-20 16:16:49,130 ERROR inference generation failed for model=Llama:8B: Ollama API error: context length exceeded
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 58, in run_from_config
    resp: ModelResponse = model.generate(prompt)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1177, in _execute_mock_call
    raise result
Exception: Ollama API error: context length exceeded
2025-08-20 16:16:49,132 INFO inference generated for model=Llama:8B prompt_len=31
2025-08-20 16:16:49,132 INFO inference run_from_config inference finished, wrote test_output.csv (rows=3) duration=0.00s
2025-08-20 16:16:49,134 INFO inference run_from_config start for inference, keys=['output_csv']
2025-08-20 16:16:49,134 ERROR inference missing config key: input_csv
2025-08-20 16:16:49,134 INFO inference run_from_config start for inference, keys=['input_csv']
2025-08-20 16:16:49,135 ERROR inference missing config key: output_csv
2025-08-20 16:16:49,136 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv']
2025-08-20 16:16:49,136 ERROR inference input CSV missing required 'prompt' column
2025-08-20 16:16:49,138 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv']
2025-08-20 16:16:49,138 ERROR inference failed to load input_csv: File not found
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 42, in run_from_config
    df = load_csv(input_csv)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1173, in _execute_mock_call
    raise effect
Exception: File not found
2025-08-20 16:16:49,141 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 16:16:49,143 INFO inference generated for model=Llama:8B prompt_len=12
2025-08-20 16:16:49,144 INFO src.io Wrote dataframe to /tmp/tmp9j9y6umc.csv (rows=1)
2025-08-20 16:16:49,145 INFO inference run_from_config inference finished, wrote /tmp/tmp9j9y6umc.csv (rows=1) duration=0.00s
2025-08-20 16:16:49,729 WARNING judge template_dir does not exist: nonexistent_dir
2025-08-20 16:16:49,732 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:16:49,734 INFO judge run_from_config judge finished, wrote ratings.csv (rows=3) duration=0.00s
2025-08-20 16:16:49,736 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:16:49,737 INFO judge run_from_config judge finished, wrote ratings.csv (rows=1) duration=0.00s
2025-08-20 16:16:49,737 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:16:49,738 INFO judge run_from_config judge finished, wrote ratings.csv (rows=1) duration=0.00s
2025-08-20 16:16:49,739 INFO judge run_from_config start for judge, keys=['output_csv', 'template_dir']
2025-08-20 16:16:49,740 ERROR judge missing config key: input_csv
2025-08-20 16:16:49,740 INFO judge run_from_config start for judge, keys=['input_csv', 'template_dir']
2025-08-20 16:16:49,740 ERROR judge missing config key: output_csv
2025-08-20 16:16:49,740 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv']
2025-08-20 16:16:49,740 ERROR judge missing config key: template_dir
2025-08-20 16:16:49,742 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:16:49,742 ERROR judge failed to load input_csv: File not found
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/judge.py", line 83, in run_from_config
    df = load_csv(input_csv)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1173, in _execute_mock_call
    raise effect
Exception: File not found
2025-08-20 16:16:49,746 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:16:49,748 INFO src.io Wrote dataframe to /tmp/tmp0njziywu.csv (rows=1)
2025-08-20 16:16:49,749 INFO judge run_from_config judge finished, wrote /tmp/tmp0njziywu.csv (rows=1) duration=0.00s
2025-08-20 16:16:58,289 INFO analytics loading input_csv: ratings.csv
2025-08-20 16:16:58,290 INFO analytics aggregating data by ['model']
2025-08-20 16:16:58,291 WARNING analytics Using fallback aggregation: 'Series' object has no attribute 'columns'
2025-08-20 16:16:58,292 INFO analytics writing output to analysis.csv
2025-08-20 16:16:58,294 INFO analytics loading input_csv: ratings.csv
2025-08-20 16:16:58,294 INFO analytics aggregating data by ['model']
2025-08-20 16:16:58,297 WARNING analytics Using fallback aggregation: 'Series' object has no attribute 'columns'
2025-08-20 16:16:58,297 INFO analytics writing output to analysis.csv
2025-08-20 16:16:58,299 ERROR analytics missing config key: input_csv
2025-08-20 16:16:58,300 ERROR analytics missing config key: output_csv
2025-08-20 16:16:58,300 ERROR analytics missing config key: group_by
2025-08-20 16:16:58,301 INFO analytics loading input_csv: input.csv
2025-08-20 16:16:58,301 ERROR analytics input data is empty
2025-08-20 16:16:58,303 INFO analytics loading input_csv: nonexistent.csv
2025-08-20 16:16:58,303 ERROR analytics failed to load input_csv: File not found
2025-08-20 16:19:04,990 INFO analytics loading input_csv: test_input.csv
2025-08-20 16:19:04,990 ERROR analytics failed to load input_csv: CSV not found: test_input.csv
2025-08-20 16:19:05,023 INFO analytics loading input_csv: test_input.csv
2025-08-20 16:19:05,023 ERROR analytics failed to load input_csv: CSV not found: test_input.csv
2025-08-20 16:19:05,027 INFO analytics loading input_csv: nonexistent.csv
2025-08-20 16:19:05,028 ERROR analytics failed to load input_csv: CSV not found: nonexistent.csv
2025-08-20 16:42:37,220 ERROR visualization missing config key: input_csv
2025-08-20 16:42:37,221 ERROR visualization missing config key: output_dir
2025-08-20 16:42:37,221 ERROR visualization missing config key: visualization_type
2025-08-20 16:42:37,222 INFO visualization loading input_csv: nonexistent.csv
2025-08-20 16:42:37,222 ERROR visualization failed to load input_csv: File not found
2025-08-20 16:42:37,224 INFO visualization loading input_csv: input.csv
2025-08-20 16:42:37,224 ERROR visualization input data is empty
2025-08-20 16:42:37,226 INFO visualization loading input_csv: input.csv
2025-08-20 16:42:37,226 INFO visualization creating comparative visualization for score
2025-08-20 16:42:37,231 INFO visualization loading input_csv: input.csv
2025-08-20 16:42:37,231 INFO visualization creating statistical visualization for score
2025-08-20 16:42:37,233 INFO visualization loading input_csv: input.csv
2025-08-20 16:42:37,238 INFO visualization creating radar chart
2025-08-20 16:42:37,240 INFO visualization loading input_csv: input.csv
2025-08-20 16:42:37,241 ERROR visualization unknown visualization_type: unknown
2025-08-20 16:42:37,242 INFO visualization loading input_csv: input.csv
2025-08-20 16:42:37,242 ERROR visualization metric_name is required for comparative visualization
2025-08-20 16:42:37,242 INFO visualization loading input_csv: input.csv
2025-08-20 16:42:37,243 ERROR visualization metric_name is required for statistical visualization
2025-08-20 16:42:37,244 INFO visualization loading input_csv: input.csv
2025-08-20 16:42:37,244 ERROR visualization no numeric columns found for radar chart
2025-08-20 16:42:44,533 INFO visualization loading input_csv: /tmp/tmpnpm7tu8l.csv
2025-08-20 16:42:44,542 INFO visualization creating comparative visualization for score
2025-08-20 16:42:44,786 INFO visualization loading input_csv: /tmp/tmpnpm7tu8l.csv
2025-08-20 16:42:44,788 INFO visualization creating radar chart
2025-08-20 16:50:50,832 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 16:50:50,833 INFO inference generated for model=Llama:8B prompt_len=26
2025-08-20 16:50:50,833 INFO inference generated for model=Llama:8B prompt_len=25
2025-08-20 16:50:50,833 INFO inference generated for model=Llama:8B prompt_len=31
2025-08-20 16:50:50,834 INFO inference run_from_config inference finished, wrote test_output.csv (rows=3) duration=0.00s
2025-08-20 16:50:50,835 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 16:50:50,836 INFO inference generated for model=Llama:8B prompt_len=26
2025-08-20 16:50:50,836 ERROR inference generation failed for model=Llama:8B: Ollama API error: context length exceeded
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 58, in run_from_config
    resp: ModelResponse = model.generate(prompt)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1177, in _execute_mock_call
    raise result
Exception: Ollama API error: context length exceeded
2025-08-20 16:50:50,837 INFO inference generated for model=Llama:8B prompt_len=31
2025-08-20 16:50:50,837 INFO inference run_from_config inference finished, wrote test_output.csv (rows=3) duration=0.00s
2025-08-20 16:50:50,839 INFO inference run_from_config start for inference, keys=['output_csv']
2025-08-20 16:50:50,839 ERROR inference missing config key: input_csv
2025-08-20 16:50:50,839 INFO inference run_from_config start for inference, keys=['input_csv']
2025-08-20 16:50:50,839 ERROR inference missing config key: output_csv
2025-08-20 16:50:50,840 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv']
2025-08-20 16:50:50,841 ERROR inference input CSV missing required 'prompt' column
2025-08-20 16:50:50,842 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv']
2025-08-20 16:50:50,842 ERROR inference failed to load input_csv: File not found
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 42, in run_from_config
    df = load_csv(input_csv)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1173, in _execute_mock_call
    raise effect
Exception: File not found
2025-08-20 16:50:50,844 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 16:50:50,845 INFO inference generated for model=Llama:8B prompt_len=12
2025-08-20 16:50:50,846 INFO src.io Wrote dataframe to /tmp/tmpfqjljtpg.csv (rows=1)
2025-08-20 16:50:50,846 INFO inference run_from_config inference finished, wrote /tmp/tmpfqjljtpg.csv (rows=1) duration=0.00s
2025-08-20 16:50:51,264 WARNING judge template_dir does not exist: nonexistent_dir
2025-08-20 16:50:51,266 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:50:51,267 INFO judge run_from_config judge finished, wrote ratings.csv (rows=3) duration=0.00s
2025-08-20 16:50:51,269 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:50:51,269 INFO judge run_from_config judge finished, wrote ratings.csv (rows=1) duration=0.00s
2025-08-20 16:50:51,270 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:50:51,270 INFO judge run_from_config judge finished, wrote ratings.csv (rows=1) duration=0.00s
2025-08-20 16:50:51,271 INFO judge run_from_config start for judge, keys=['output_csv', 'template_dir']
2025-08-20 16:50:51,271 ERROR judge missing config key: input_csv
2025-08-20 16:50:51,271 INFO judge run_from_config start for judge, keys=['input_csv', 'template_dir']
2025-08-20 16:50:51,272 ERROR judge missing config key: output_csv
2025-08-20 16:50:51,272 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv']
2025-08-20 16:50:51,272 ERROR judge missing config key: template_dir
2025-08-20 16:50:51,273 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:50:51,273 ERROR judge failed to load input_csv: File not found
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/judge.py", line 83, in run_from_config
    df = load_csv(input_csv)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1173, in _execute_mock_call
    raise effect
Exception: File not found
2025-08-20 16:50:51,277 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:50:51,280 INFO src.io Wrote dataframe to /tmp/tmpppkclu2z.csv (rows=1)
2025-08-20 16:50:51,280 INFO judge run_from_config judge finished, wrote /tmp/tmpppkclu2z.csv (rows=1) duration=0.00s
2025-08-20 16:50:56,470 INFO analytics loading input_csv: ratings.csv
2025-08-20 16:50:56,470 INFO analytics aggregating data by ['model']
2025-08-20 16:50:56,475 WARNING analytics Using fallback aggregation: 'Series' object has no attribute 'columns'
2025-08-20 16:50:56,476 INFO analytics writing output to analysis.csv
2025-08-20 16:50:56,478 INFO analytics loading input_csv: ratings.csv
2025-08-20 16:50:56,478 INFO analytics aggregating data by ['model']
2025-08-20 16:50:56,479 WARNING analytics Using fallback aggregation: 'Series' object has no attribute 'columns'
2025-08-20 16:50:56,480 INFO analytics writing output to analysis.csv
2025-08-20 16:50:56,481 ERROR analytics missing config key: input_csv
2025-08-20 16:50:56,481 ERROR analytics missing config key: output_csv
2025-08-20 16:50:56,481 ERROR analytics missing config key: group_by
2025-08-20 16:50:56,483 INFO analytics loading input_csv: input.csv
2025-08-20 16:50:56,483 ERROR analytics input data is empty
2025-08-20 16:50:56,484 INFO analytics loading input_csv: nonexistent.csv
2025-08-20 16:50:56,484 ERROR analytics failed to load input_csv: File not found
2025-08-20 16:50:56,486 INFO analytics loading input_csv: /tmp/tmpz2ze01g3.csv
2025-08-20 16:50:56,487 INFO analytics aggregating data by ['model']
2025-08-20 16:50:56,488 WARNING analytics Using fallback aggregation: 'Series' object has no attribute 'columns'
2025-08-20 16:50:56,489 INFO analytics writing output to /tmp/tmp2nawjsz3.csv
2025-08-20 16:50:56,489 INFO src.io Wrote dataframe to /tmp/tmp2nawjsz3.csv (rows=2)
2025-08-20 16:50:56,885 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model']
2025-08-20 16:50:56,886 INFO inference generated for model=Llama:8B prompt_len=12
2025-08-20 16:50:56,886 INFO inference generated for model=Llama:8B prompt_len=12
2025-08-20 16:50:56,886 INFO inference generated for model=gpt-3.5-turbo prompt_len=12
2025-08-20 16:50:56,886 INFO inference generated for model=gpt-3.5-turbo prompt_len=12
2025-08-20 16:50:56,887 INFO src.io Wrote dataframe to /tmp/tmpm8wxb48z/completions.csv (rows=4)
2025-08-20 16:50:56,887 INFO inference run_from_config inference finished, wrote /tmp/tmpm8wxb48z/completions.csv (rows=4) duration=0.00s
2025-08-20 16:50:56,887 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:50:56,889 INFO src.io Wrote dataframe to /tmp/tmpm8wxb48z/ratings.csv (rows=4)
2025-08-20 16:50:56,889 INFO judge run_from_config judge finished, wrote /tmp/tmpm8wxb48z/ratings.csv (rows=4) duration=0.00s
2025-08-20 16:50:56,890 INFO analytics loading input_csv: /tmp/tmpm8wxb48z/ratings.csv
2025-08-20 16:50:56,891 INFO analytics aggregating data by ['model']
2025-08-20 16:50:56,893 WARNING analytics Using fallback aggregation: 'Series' object has no attribute 'columns'
2025-08-20 16:50:56,894 INFO analytics writing output to /tmp/tmpm8wxb48z/analysis.csv
2025-08-20 16:50:56,895 INFO src.io Wrote dataframe to /tmp/tmpm8wxb48z/analysis.csv (rows=2)
2025-08-20 16:50:57,378 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 16:50:57,379 INFO inference generated for model=Llama:8B prompt_len=26
2025-08-20 16:50:57,380 INFO inference generated for model=Llama:8B prompt_len=25
2025-08-20 16:50:57,380 INFO inference generated for model=Llama:8B prompt_len=31
2025-08-20 16:50:57,381 INFO inference run_from_config inference finished, wrote test_output.csv (rows=3) duration=0.00s
2025-08-20 16:50:57,384 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 16:50:57,385 INFO inference generated for model=Llama:8B prompt_len=26
2025-08-20 16:50:57,385 ERROR inference generation failed for model=Llama:8B: Ollama API error: context length exceeded
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 58, in run_from_config
    resp: ModelResponse = model.generate(prompt)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1177, in _execute_mock_call
    raise result
Exception: Ollama API error: context length exceeded
2025-08-20 16:50:57,386 INFO inference generated for model=Llama:8B prompt_len=31
2025-08-20 16:50:57,387 INFO inference run_from_config inference finished, wrote test_output.csv (rows=3) duration=0.00s
2025-08-20 16:50:57,389 INFO inference run_from_config start for inference, keys=['output_csv']
2025-08-20 16:50:57,389 ERROR inference missing config key: input_csv
2025-08-20 16:50:57,390 INFO inference run_from_config start for inference, keys=['input_csv']
2025-08-20 16:50:57,390 ERROR inference missing config key: output_csv
2025-08-20 16:50:57,391 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv']
2025-08-20 16:50:57,391 ERROR inference input CSV missing required 'prompt' column
2025-08-20 16:50:57,392 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv']
2025-08-20 16:50:57,393 ERROR inference failed to load input_csv: File not found
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 42, in run_from_config
    df = load_csv(input_csv)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1173, in _execute_mock_call
    raise effect
Exception: File not found
2025-08-20 16:50:57,397 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 16:50:57,398 INFO inference generated for model=Llama:8B prompt_len=12
2025-08-20 16:50:57,399 INFO src.io Wrote dataframe to /tmp/tmpjqvpdxof.csv (rows=1)
2025-08-20 16:50:57,399 INFO inference run_from_config inference finished, wrote /tmp/tmpjqvpdxof.csv (rows=1) duration=0.00s
2025-08-20 16:50:57,962 WARNING judge template_dir does not exist: nonexistent_dir
2025-08-20 16:50:57,966 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:50:57,968 INFO judge run_from_config judge finished, wrote ratings.csv (rows=3) duration=0.00s
2025-08-20 16:50:57,970 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:50:57,971 INFO judge run_from_config judge finished, wrote ratings.csv (rows=1) duration=0.00s
2025-08-20 16:50:57,971 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:50:57,972 INFO judge run_from_config judge finished, wrote ratings.csv (rows=1) duration=0.00s
2025-08-20 16:50:57,973 INFO judge run_from_config start for judge, keys=['output_csv', 'template_dir']
2025-08-20 16:50:57,973 ERROR judge missing config key: input_csv
2025-08-20 16:50:57,973 INFO judge run_from_config start for judge, keys=['input_csv', 'template_dir']
2025-08-20 16:50:57,973 ERROR judge missing config key: output_csv
2025-08-20 16:50:57,974 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv']
2025-08-20 16:50:57,974 ERROR judge missing config key: template_dir
2025-08-20 16:50:57,975 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:50:57,975 ERROR judge failed to load input_csv: File not found
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/judge.py", line 83, in run_from_config
    df = load_csv(input_csv)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1173, in _execute_mock_call
    raise effect
Exception: File not found
2025-08-20 16:50:57,980 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:50:57,982 INFO src.io Wrote dataframe to /tmp/tmpudnmxmfc.csv (rows=1)
2025-08-20 16:50:57,982 INFO judge run_from_config judge finished, wrote /tmp/tmpudnmxmfc.csv (rows=1) duration=0.00s
2025-08-20 16:55:28,705 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 16:55:28,705 INFO inference generated for model=Llama:8B prompt_len=26
2025-08-20 16:55:28,706 INFO inference generated for model=Llama:8B prompt_len=25
2025-08-20 16:55:28,706 INFO inference generated for model=Llama:8B prompt_len=31
2025-08-20 16:55:28,706 INFO inference run_from_config inference finished, wrote test_output.csv (rows=3) duration=0.00s
2025-08-20 16:55:28,708 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 16:55:28,708 INFO inference generated for model=Llama:8B prompt_len=26
2025-08-20 16:55:28,708 ERROR inference generation failed for model=Llama:8B: Ollama API error: context length exceeded
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 58, in run_from_config
    resp: ModelResponse = model.generate(prompt)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1177, in _execute_mock_call
    raise result
Exception: Ollama API error: context length exceeded
2025-08-20 16:55:28,709 INFO inference generated for model=Llama:8B prompt_len=31
2025-08-20 16:55:28,710 INFO inference run_from_config inference finished, wrote test_output.csv (rows=3) duration=0.00s
2025-08-20 16:55:28,711 INFO inference run_from_config start for inference, keys=['output_csv']
2025-08-20 16:55:28,711 ERROR inference missing config key: input_csv
2025-08-20 16:55:28,711 INFO inference run_from_config start for inference, keys=['input_csv']
2025-08-20 16:55:28,711 ERROR inference missing config key: output_csv
2025-08-20 16:55:28,712 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv']
2025-08-20 16:55:28,713 ERROR inference input CSV missing required 'prompt' column
2025-08-20 16:55:28,713 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv']
2025-08-20 16:55:28,714 ERROR inference failed to load input_csv: File not found
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 42, in run_from_config
    df = load_csv(input_csv)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1173, in _execute_mock_call
    raise effect
Exception: File not found
2025-08-20 16:55:28,716 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 16:55:28,717 INFO inference generated for model=Llama:8B prompt_len=12
2025-08-20 16:55:28,717 INFO src.io Wrote dataframe to /tmp/tmpu6ru59bp.csv (rows=1)
2025-08-20 16:55:28,718 INFO inference run_from_config inference finished, wrote /tmp/tmpu6ru59bp.csv (rows=1) duration=0.00s
2025-08-20 16:55:29,110 WARNING judge template_dir does not exist: nonexistent_dir
2025-08-20 16:55:29,112 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:55:29,113 INFO judge run_from_config judge finished, wrote ratings.csv (rows=3) duration=0.00s
2025-08-20 16:55:29,115 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:55:29,115 INFO judge run_from_config judge finished, wrote ratings.csv (rows=1) duration=0.00s
2025-08-20 16:55:29,116 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:55:29,116 INFO judge run_from_config judge finished, wrote ratings.csv (rows=1) duration=0.00s
2025-08-20 16:55:29,118 INFO judge run_from_config start for judge, keys=['output_csv', 'template_dir']
2025-08-20 16:55:29,118 ERROR judge missing config key: input_csv
2025-08-20 16:55:29,118 INFO judge run_from_config start for judge, keys=['input_csv', 'template_dir']
2025-08-20 16:55:29,118 ERROR judge missing config key: output_csv
2025-08-20 16:55:29,118 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv']
2025-08-20 16:55:29,118 ERROR judge missing config key: template_dir
2025-08-20 16:55:29,119 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:55:29,119 ERROR judge failed to load input_csv: File not found
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/judge.py", line 83, in run_from_config
    df = load_csv(input_csv)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1173, in _execute_mock_call
    raise effect
Exception: File not found
2025-08-20 16:55:29,122 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:55:29,124 INFO src.io Wrote dataframe to /tmp/tmp967t_9_c.csv (rows=1)
2025-08-20 16:55:29,125 INFO judge run_from_config judge finished, wrote /tmp/tmp967t_9_c.csv (rows=1) duration=0.00s
2025-08-20 16:55:33,514 INFO analytics loading input_csv: ratings.csv
2025-08-20 16:55:33,514 INFO analytics aggregating data by ['model']
2025-08-20 16:55:33,517 WARNING analytics Using fallback aggregation: 'Series' object has no attribute 'columns'
2025-08-20 16:55:33,517 INFO analytics writing output to analysis.csv
2025-08-20 16:55:33,519 INFO analytics loading input_csv: ratings.csv
2025-08-20 16:55:33,519 INFO analytics aggregating data by ['model']
2025-08-20 16:55:33,521 WARNING analytics Using fallback aggregation: 'Series' object has no attribute 'columns'
2025-08-20 16:55:33,521 INFO analytics writing output to analysis.csv
2025-08-20 16:55:33,522 ERROR analytics missing config key: input_csv
2025-08-20 16:55:33,522 ERROR analytics missing config key: output_csv
2025-08-20 16:55:33,523 ERROR analytics missing config key: group_by
2025-08-20 16:55:33,524 INFO analytics loading input_csv: input.csv
2025-08-20 16:55:33,524 ERROR analytics input data is empty
2025-08-20 16:55:33,525 INFO analytics loading input_csv: nonexistent.csv
2025-08-20 16:55:33,525 ERROR analytics failed to load input_csv: File not found
2025-08-20 16:55:33,527 INFO analytics loading input_csv: /tmp/tmpqq8ug_2p.csv
2025-08-20 16:55:33,528 INFO analytics aggregating data by ['model']
2025-08-20 16:55:33,529 WARNING analytics Using fallback aggregation: 'Series' object has no attribute 'columns'
2025-08-20 16:55:33,530 INFO analytics writing output to /tmp/tmptp1e7pdm.csv
2025-08-20 16:55:33,530 INFO src.io Wrote dataframe to /tmp/tmptp1e7pdm.csv (rows=2)
2025-08-20 16:55:33,923 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model']
2025-08-20 16:55:33,924 INFO inference generated for model=Llama:8B prompt_len=12
2025-08-20 16:55:33,924 INFO inference generated for model=Llama:8B prompt_len=12
2025-08-20 16:55:33,925 INFO inference generated for model=gpt-3.5-turbo prompt_len=12
2025-08-20 16:55:33,925 INFO inference generated for model=gpt-3.5-turbo prompt_len=12
2025-08-20 16:55:33,925 INFO src.io Wrote dataframe to /tmp/tmpew7p2en3/completions.csv (rows=4)
2025-08-20 16:55:33,925 INFO inference run_from_config inference finished, wrote /tmp/tmpew7p2en3/completions.csv (rows=4) duration=0.00s
2025-08-20 16:55:33,926 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:55:33,927 INFO src.io Wrote dataframe to /tmp/tmpew7p2en3/ratings.csv (rows=4)
2025-08-20 16:55:33,927 INFO judge run_from_config judge finished, wrote /tmp/tmpew7p2en3/ratings.csv (rows=4) duration=0.00s
2025-08-20 16:55:33,928 INFO analytics loading input_csv: /tmp/tmpew7p2en3/ratings.csv
2025-08-20 16:55:33,929 INFO analytics aggregating data by ['model']
2025-08-20 16:55:33,931 WARNING analytics Using fallback aggregation: 'Series' object has no attribute 'columns'
2025-08-20 16:55:33,931 INFO analytics writing output to /tmp/tmpew7p2en3/analysis.csv
2025-08-20 16:55:33,932 INFO src.io Wrote dataframe to /tmp/tmpew7p2en3/analysis.csv (rows=2)
2025-08-20 16:55:34,462 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 16:55:34,463 INFO inference generated for model=Llama:8B prompt_len=26
2025-08-20 16:55:34,463 INFO inference generated for model=Llama:8B prompt_len=25
2025-08-20 16:55:34,464 INFO inference generated for model=Llama:8B prompt_len=31
2025-08-20 16:55:34,464 INFO inference run_from_config inference finished, wrote test_output.csv (rows=3) duration=0.00s
2025-08-20 16:55:34,467 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 16:55:34,467 INFO inference generated for model=Llama:8B prompt_len=26
2025-08-20 16:55:34,468 ERROR inference generation failed for model=Llama:8B: Ollama API error: context length exceeded
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 58, in run_from_config
    resp: ModelResponse = model.generate(prompt)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1177, in _execute_mock_call
    raise result
Exception: Ollama API error: context length exceeded
2025-08-20 16:55:34,469 INFO inference generated for model=Llama:8B prompt_len=31
2025-08-20 16:55:34,470 INFO inference run_from_config inference finished, wrote test_output.csv (rows=3) duration=0.00s
2025-08-20 16:55:34,471 INFO inference run_from_config start for inference, keys=['output_csv']
2025-08-20 16:55:34,471 ERROR inference missing config key: input_csv
2025-08-20 16:55:34,472 INFO inference run_from_config start for inference, keys=['input_csv']
2025-08-20 16:55:34,472 ERROR inference missing config key: output_csv
2025-08-20 16:55:34,473 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv']
2025-08-20 16:55:34,473 ERROR inference input CSV missing required 'prompt' column
2025-08-20 16:55:34,474 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv']
2025-08-20 16:55:34,474 ERROR inference failed to load input_csv: File not found
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 42, in run_from_config
    df = load_csv(input_csv)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1173, in _execute_mock_call
    raise effect
Exception: File not found
2025-08-20 16:55:34,479 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 16:55:34,481 INFO inference generated for model=Llama:8B prompt_len=12
2025-08-20 16:55:34,482 INFO src.io Wrote dataframe to /tmp/tmpz7z14v1k.csv (rows=1)
2025-08-20 16:55:34,482 INFO inference run_from_config inference finished, wrote /tmp/tmpz7z14v1k.csv (rows=1) duration=0.00s
2025-08-20 16:55:35,001 WARNING judge template_dir does not exist: nonexistent_dir
2025-08-20 16:55:35,004 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:55:35,005 INFO judge run_from_config judge finished, wrote ratings.csv (rows=3) duration=0.00s
2025-08-20 16:55:35,008 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:55:35,009 INFO judge run_from_config judge finished, wrote ratings.csv (rows=1) duration=0.00s
2025-08-20 16:55:35,009 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:55:35,010 INFO judge run_from_config judge finished, wrote ratings.csv (rows=1) duration=0.00s
2025-08-20 16:55:35,012 INFO judge run_from_config start for judge, keys=['output_csv', 'template_dir']
2025-08-20 16:55:35,012 ERROR judge missing config key: input_csv
2025-08-20 16:55:35,013 INFO judge run_from_config start for judge, keys=['input_csv', 'template_dir']
2025-08-20 16:55:35,013 ERROR judge missing config key: output_csv
2025-08-20 16:55:35,013 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv']
2025-08-20 16:55:35,013 ERROR judge missing config key: template_dir
2025-08-20 16:55:35,014 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:55:35,015 ERROR judge failed to load input_csv: File not found
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/judge.py", line 83, in run_from_config
    df = load_csv(input_csv)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1173, in _execute_mock_call
    raise effect
Exception: File not found
2025-08-20 16:55:35,018 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:55:35,020 INFO src.io Wrote dataframe to /tmp/tmpv5v4geur.csv (rows=1)
2025-08-20 16:55:35,021 INFO judge run_from_config judge finished, wrote /tmp/tmpv5v4geur.csv (rows=1) duration=0.00s
2025-08-20 16:58:28,818 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 16:58:28,818 INFO inference generated for model=Llama:8B prompt_len=26
2025-08-20 16:58:28,819 INFO inference generated for model=Llama:8B prompt_len=25
2025-08-20 16:58:28,819 INFO inference generated for model=Llama:8B prompt_len=31
2025-08-20 16:58:28,819 INFO inference run_from_config inference finished, wrote test_output.csv (rows=3) duration=0.00s
2025-08-20 16:58:28,821 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 16:58:28,822 INFO inference generated for model=Llama:8B prompt_len=26
2025-08-20 16:58:28,822 ERROR inference generation failed for model=Llama:8B: Ollama API error: context length exceeded
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 58, in run_from_config
    resp: ModelResponse = model.generate(prompt)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1177, in _execute_mock_call
    raise result
Exception: Ollama API error: context length exceeded
2025-08-20 16:58:28,823 INFO inference generated for model=Llama:8B prompt_len=31
2025-08-20 16:58:28,823 INFO inference run_from_config inference finished, wrote test_output.csv (rows=3) duration=0.00s
2025-08-20 16:58:28,824 INFO inference run_from_config start for inference, keys=['output_csv']
2025-08-20 16:58:28,824 ERROR inference missing config key: input_csv
2025-08-20 16:58:28,825 INFO inference run_from_config start for inference, keys=['input_csv']
2025-08-20 16:58:28,825 ERROR inference missing config key: output_csv
2025-08-20 16:58:28,826 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv']
2025-08-20 16:58:28,826 ERROR inference input CSV missing required 'prompt' column
2025-08-20 16:58:28,827 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv']
2025-08-20 16:58:28,827 ERROR inference failed to load input_csv: File not found
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 42, in run_from_config
    df = load_csv(input_csv)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1173, in _execute_mock_call
    raise effect
Exception: File not found
2025-08-20 16:58:28,830 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 16:58:28,831 INFO inference generated for model=Llama:8B prompt_len=12
2025-08-20 16:58:28,831 INFO src.io Wrote dataframe to /tmp/tmp6kio12oc.csv (rows=1)
2025-08-20 16:58:28,831 INFO inference run_from_config inference finished, wrote /tmp/tmp6kio12oc.csv (rows=1) duration=0.00s
2025-08-20 16:58:29,233 WARNING judge template_dir does not exist: nonexistent_dir
2025-08-20 16:58:29,236 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:58:29,237 INFO judge run_from_config judge finished, wrote ratings.csv (rows=3) duration=0.00s
2025-08-20 16:58:29,238 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:58:29,239 INFO judge run_from_config judge finished, wrote ratings.csv (rows=1) duration=0.00s
2025-08-20 16:58:29,239 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:58:29,240 INFO judge run_from_config judge finished, wrote ratings.csv (rows=1) duration=0.00s
2025-08-20 16:58:29,241 INFO judge run_from_config start for judge, keys=['output_csv', 'template_dir']
2025-08-20 16:58:29,241 ERROR judge missing config key: input_csv
2025-08-20 16:58:29,241 INFO judge run_from_config start for judge, keys=['input_csv', 'template_dir']
2025-08-20 16:58:29,241 ERROR judge missing config key: output_csv
2025-08-20 16:58:29,241 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv']
2025-08-20 16:58:29,241 ERROR judge missing config key: template_dir
2025-08-20 16:58:29,242 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:58:29,242 ERROR judge failed to load input_csv: File not found
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/judge.py", line 83, in run_from_config
    df = load_csv(input_csv)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1173, in _execute_mock_call
    raise effect
Exception: File not found
2025-08-20 16:58:29,245 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:58:29,247 INFO src.io Wrote dataframe to /tmp/tmps3dr5j6l.csv (rows=1)
2025-08-20 16:58:29,247 INFO judge run_from_config judge finished, wrote /tmp/tmps3dr5j6l.csv (rows=1) duration=0.00s
2025-08-20 16:58:33,640 INFO analytics loading input_csv: ratings.csv
2025-08-20 16:58:33,640 INFO analytics aggregating data by ['model']
2025-08-20 16:58:33,642 WARNING analytics Using fallback aggregation: 'Series' object has no attribute 'columns'
2025-08-20 16:58:33,643 INFO analytics writing output to analysis.csv
2025-08-20 16:58:33,644 INFO analytics loading input_csv: ratings.csv
2025-08-20 16:58:33,644 INFO analytics aggregating data by ['model']
2025-08-20 16:58:33,646 WARNING analytics Using fallback aggregation: 'Series' object has no attribute 'columns'
2025-08-20 16:58:33,646 INFO analytics writing output to analysis.csv
2025-08-20 16:58:33,648 ERROR analytics missing config key: input_csv
2025-08-20 16:58:33,648 ERROR analytics missing config key: output_csv
2025-08-20 16:58:33,648 ERROR analytics missing config key: group_by
2025-08-20 16:58:33,649 INFO analytics loading input_csv: input.csv
2025-08-20 16:58:33,649 ERROR analytics input data is empty
2025-08-20 16:58:33,650 INFO analytics loading input_csv: nonexistent.csv
2025-08-20 16:58:33,650 ERROR analytics failed to load input_csv: File not found
2025-08-20 16:58:33,653 INFO analytics loading input_csv: /tmp/tmpj0a6eoqq.csv
2025-08-20 16:58:33,653 INFO analytics aggregating data by ['model']
2025-08-20 16:58:33,655 WARNING analytics Using fallback aggregation: 'Series' object has no attribute 'columns'
2025-08-20 16:58:33,655 INFO analytics writing output to /tmp/tmpwlx50ka6.csv
2025-08-20 16:58:33,656 INFO src.io Wrote dataframe to /tmp/tmpwlx50ka6.csv (rows=2)
2025-08-20 16:58:34,548 ERROR visualization missing config key: input_csv
2025-08-20 16:58:34,548 ERROR visualization missing config key: output_dir
2025-08-20 16:58:34,548 ERROR visualization missing config key: visualization_type
2025-08-20 16:58:34,549 INFO visualization loading input_csv: nonexistent.csv
2025-08-20 16:58:34,549 ERROR visualization failed to load input_csv: File not found
2025-08-20 16:58:34,551 INFO visualization loading input_csv: input.csv
2025-08-20 16:58:34,551 ERROR visualization input data is empty
2025-08-20 16:58:34,553 INFO visualization loading input_csv: input.csv
2025-08-20 16:58:34,553 INFO visualization creating comparative visualization for score
2025-08-20 16:58:34,556 INFO visualization loading input_csv: input.csv
2025-08-20 16:58:34,556 INFO visualization creating statistical visualization for score
2025-08-20 16:58:34,559 INFO visualization loading input_csv: input.csv
2025-08-20 16:58:34,560 INFO visualization creating radar chart
2025-08-20 16:58:34,562 INFO visualization loading input_csv: input.csv
2025-08-20 16:58:34,562 ERROR visualization unknown visualization_type: unknown
2025-08-20 16:58:34,564 INFO visualization loading input_csv: input.csv
2025-08-20 16:58:34,564 ERROR visualization metric_name is required for comparative visualization
2025-08-20 16:58:34,564 INFO visualization loading input_csv: input.csv
2025-08-20 16:58:34,565 ERROR visualization metric_name is required for statistical visualization
2025-08-20 16:58:34,566 INFO visualization loading input_csv: input.csv
2025-08-20 16:58:34,566 ERROR visualization no numeric columns found for radar chart
2025-08-20 16:58:35,742 INFO visualization loading input_csv: /tmp/tmpe95sj2bf.csv
2025-08-20 16:58:35,744 INFO visualization creating comparative visualization for score
2025-08-20 16:58:35,959 INFO visualization loading input_csv: /tmp/tmpe95sj2bf.csv
2025-08-20 16:58:35,960 INFO visualization creating radar chart
2025-08-20 16:58:37,746 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model']
2025-08-20 16:58:37,747 INFO inference generated for model=Llama:8B prompt_len=12
2025-08-20 16:58:37,747 INFO inference generated for model=Llama:8B prompt_len=12
2025-08-20 16:58:37,747 INFO inference generated for model=gpt-3.5-turbo prompt_len=12
2025-08-20 16:58:37,748 INFO inference generated for model=gpt-3.5-turbo prompt_len=12
2025-08-20 16:58:37,748 INFO src.io Wrote dataframe to /tmp/tmpeo9o6a3_/completions.csv (rows=4)
2025-08-20 16:58:37,748 INFO inference run_from_config inference finished, wrote /tmp/tmpeo9o6a3_/completions.csv (rows=4) duration=0.00s
2025-08-20 16:58:37,748 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:58:37,750 INFO src.io Wrote dataframe to /tmp/tmpeo9o6a3_/ratings.csv (rows=4)
2025-08-20 16:58:37,750 INFO judge run_from_config judge finished, wrote /tmp/tmpeo9o6a3_/ratings.csv (rows=4) duration=0.00s
2025-08-20 16:58:37,751 INFO analytics loading input_csv: /tmp/tmpeo9o6a3_/ratings.csv
2025-08-20 16:58:37,752 INFO analytics aggregating data by ['model']
2025-08-20 16:58:37,753 WARNING analytics Using fallback aggregation: 'Series' object has no attribute 'columns'
2025-08-20 16:58:37,754 INFO analytics writing output to /tmp/tmpeo9o6a3_/analysis.csv
2025-08-20 16:58:37,755 INFO src.io Wrote dataframe to /tmp/tmpeo9o6a3_/analysis.csv (rows=2)
2025-08-20 16:58:38,264 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 16:58:38,265 INFO inference generated for model=Llama:8B prompt_len=26
2025-08-20 16:58:38,265 INFO inference generated for model=Llama:8B prompt_len=25
2025-08-20 16:58:38,266 INFO inference generated for model=Llama:8B prompt_len=31
2025-08-20 16:58:38,266 INFO inference run_from_config inference finished, wrote test_output.csv (rows=3) duration=0.00s
2025-08-20 16:58:38,268 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 16:58:38,269 INFO inference generated for model=Llama:8B prompt_len=26
2025-08-20 16:58:38,269 ERROR inference generation failed for model=Llama:8B: Ollama API error: context length exceeded
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 58, in run_from_config
    resp: ModelResponse = model.generate(prompt)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1177, in _execute_mock_call
    raise result
Exception: Ollama API error: context length exceeded
2025-08-20 16:58:38,270 INFO inference generated for model=Llama:8B prompt_len=31
2025-08-20 16:58:38,271 INFO inference run_from_config inference finished, wrote test_output.csv (rows=3) duration=0.00s
2025-08-20 16:58:38,273 INFO inference run_from_config start for inference, keys=['output_csv']
2025-08-20 16:58:38,273 ERROR inference missing config key: input_csv
2025-08-20 16:58:38,273 INFO inference run_from_config start for inference, keys=['input_csv']
2025-08-20 16:58:38,273 ERROR inference missing config key: output_csv
2025-08-20 16:58:38,275 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv']
2025-08-20 16:58:38,275 ERROR inference input CSV missing required 'prompt' column
2025-08-20 16:58:38,276 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv']
2025-08-20 16:58:38,276 ERROR inference failed to load input_csv: File not found
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 42, in run_from_config
    df = load_csv(input_csv)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1173, in _execute_mock_call
    raise effect
Exception: File not found
2025-08-20 16:58:38,280 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 16:58:38,281 INFO inference generated for model=Llama:8B prompt_len=12
2025-08-20 16:58:38,282 INFO src.io Wrote dataframe to /tmp/tmpwacz6c7k.csv (rows=1)
2025-08-20 16:58:38,282 INFO inference run_from_config inference finished, wrote /tmp/tmpwacz6c7k.csv (rows=1) duration=0.00s
2025-08-20 16:58:38,797 WARNING judge template_dir does not exist: nonexistent_dir
2025-08-20 16:58:38,799 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:58:38,801 INFO judge run_from_config judge finished, wrote ratings.csv (rows=3) duration=0.00s
2025-08-20 16:58:38,803 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:58:38,804 INFO judge run_from_config judge finished, wrote ratings.csv (rows=1) duration=0.00s
2025-08-20 16:58:38,804 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:58:38,804 INFO judge run_from_config judge finished, wrote ratings.csv (rows=1) duration=0.00s
2025-08-20 16:58:38,806 INFO judge run_from_config start for judge, keys=['output_csv', 'template_dir']
2025-08-20 16:58:38,806 ERROR judge missing config key: input_csv
2025-08-20 16:58:38,806 INFO judge run_from_config start for judge, keys=['input_csv', 'template_dir']
2025-08-20 16:58:38,806 ERROR judge missing config key: output_csv
2025-08-20 16:58:38,806 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv']
2025-08-20 16:58:38,806 ERROR judge missing config key: template_dir
2025-08-20 16:58:38,808 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:58:38,808 ERROR judge failed to load input_csv: File not found
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/judge.py", line 83, in run_from_config
    df = load_csv(input_csv)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1173, in _execute_mock_call
    raise effect
Exception: File not found
2025-08-20 16:58:38,811 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 16:58:38,813 INFO src.io Wrote dataframe to /tmp/tmpfjhxrjjt.csv (rows=1)
2025-08-20 16:58:38,813 INFO judge run_from_config judge finished, wrote /tmp/tmpfjhxrjjt.csv (rows=1) duration=0.00s
2025-08-20 17:01:35,186 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 17:01:35,187 INFO inference generated for model=Llama:8B prompt_len=26
2025-08-20 17:01:35,187 INFO inference generated for model=Llama:8B prompt_len=25
2025-08-20 17:01:35,187 INFO inference generated for model=Llama:8B prompt_len=31
2025-08-20 17:01:35,188 INFO inference run_from_config inference finished, wrote test_output.csv (rows=3) duration=0.00s
2025-08-20 17:01:35,190 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 17:01:35,190 INFO inference generated for model=Llama:8B prompt_len=26
2025-08-20 17:01:35,190 ERROR inference generation failed for model=Llama:8B: Ollama API error: context length exceeded
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 58, in run_from_config
    resp: ModelResponse = model.generate(prompt)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1177, in _execute_mock_call
    raise result
Exception: Ollama API error: context length exceeded
2025-08-20 17:01:35,191 INFO inference generated for model=Llama:8B prompt_len=31
2025-08-20 17:01:35,192 INFO inference run_from_config inference finished, wrote test_output.csv (rows=3) duration=0.00s
2025-08-20 17:01:35,194 INFO inference run_from_config start for inference, keys=['output_csv']
2025-08-20 17:01:35,194 ERROR inference missing config key: input_csv
2025-08-20 17:01:35,194 INFO inference run_from_config start for inference, keys=['input_csv']
2025-08-20 17:01:35,194 ERROR inference missing config key: output_csv
2025-08-20 17:01:35,196 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv']
2025-08-20 17:01:35,196 ERROR inference input CSV missing required 'prompt' column
2025-08-20 17:01:35,197 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv']
2025-08-20 17:01:35,197 ERROR inference failed to load input_csv: File not found
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 42, in run_from_config
    df = load_csv(input_csv)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1173, in _execute_mock_call
    raise effect
Exception: File not found
2025-08-20 17:01:35,199 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 17:01:35,200 INFO inference generated for model=Llama:8B prompt_len=12
2025-08-20 17:01:35,201 INFO src.io Wrote dataframe to /tmp/tmpbsnkphcr.csv (rows=1)
2025-08-20 17:01:35,201 INFO inference run_from_config inference finished, wrote /tmp/tmpbsnkphcr.csv (rows=1) duration=0.00s
2025-08-20 17:01:35,595 WARNING judge template_dir does not exist: nonexistent_dir
2025-08-20 17:01:35,597 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 17:01:35,598 INFO judge run_from_config judge finished, wrote ratings.csv (rows=3) duration=0.00s
2025-08-20 17:01:35,600 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 17:01:35,601 INFO judge run_from_config judge finished, wrote ratings.csv (rows=1) duration=0.00s
2025-08-20 17:01:35,601 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 17:01:35,601 INFO judge run_from_config judge finished, wrote ratings.csv (rows=1) duration=0.00s
2025-08-20 17:01:35,602 INFO judge run_from_config start for judge, keys=['output_csv', 'template_dir']
2025-08-20 17:01:35,602 ERROR judge missing config key: input_csv
2025-08-20 17:01:35,602 INFO judge run_from_config start for judge, keys=['input_csv', 'template_dir']
2025-08-20 17:01:35,602 ERROR judge missing config key: output_csv
2025-08-20 17:01:35,603 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv']
2025-08-20 17:01:35,603 ERROR judge missing config key: template_dir
2025-08-20 17:01:35,604 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 17:01:35,604 ERROR judge failed to load input_csv: File not found
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/judge.py", line 83, in run_from_config
    df = load_csv(input_csv)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1173, in _execute_mock_call
    raise effect
Exception: File not found
2025-08-20 17:01:35,606 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 17:01:35,608 INFO src.io Wrote dataframe to /tmp/tmpnwd5h8wc.csv (rows=1)
2025-08-20 17:01:35,609 INFO judge run_from_config judge finished, wrote /tmp/tmpnwd5h8wc.csv (rows=1) duration=0.00s
2025-08-20 17:01:46,152 ERROR visualization missing config key: input_csv
2025-08-20 17:01:46,153 ERROR visualization missing config key: output_dir
2025-08-20 17:01:46,153 ERROR visualization missing config key: visualization_type
2025-08-20 17:01:46,154 INFO visualization loading input_csv: nonexistent.csv
2025-08-20 17:01:46,154 ERROR visualization failed to load input_csv: File not found
2025-08-20 17:01:46,155 INFO visualization loading input_csv: input.csv
2025-08-20 17:01:46,155 ERROR visualization input data is empty
2025-08-20 17:01:46,158 INFO visualization loading input_csv: input.csv
2025-08-20 17:01:46,158 INFO visualization creating comparative visualization for score
2025-08-20 17:01:46,162 INFO visualization loading input_csv: input.csv
2025-08-20 17:01:46,162 INFO visualization creating statistical visualization for score
2025-08-20 17:01:46,165 INFO visualization loading input_csv: input.csv
2025-08-20 17:01:46,166 INFO visualization creating radar chart
2025-08-20 17:01:46,167 INFO visualization loading input_csv: input.csv
2025-08-20 17:01:46,168 ERROR visualization unknown visualization_type: unknown
2025-08-20 17:01:46,170 INFO visualization loading input_csv: input.csv
2025-08-20 17:01:46,170 ERROR visualization metric_name is required for comparative visualization
2025-08-20 17:01:46,170 INFO visualization loading input_csv: input.csv
2025-08-20 17:01:46,170 ERROR visualization metric_name is required for statistical visualization
2025-08-20 17:01:46,172 INFO visualization loading input_csv: input.csv
2025-08-20 17:01:46,172 ERROR visualization no numeric columns found for radar chart
2025-08-20 17:01:49,390 INFO analytics loading input_csv: ratings.csv
2025-08-20 17:01:49,391 INFO analytics aggregating data by ['model']
2025-08-20 17:01:49,393 WARNING analytics Using fallback aggregation: 'Series' object has no attribute 'columns'
2025-08-20 17:01:49,394 INFO analytics writing output to analysis.csv
2025-08-20 17:01:49,396 INFO analytics loading input_csv: ratings.csv
2025-08-20 17:01:49,396 INFO analytics aggregating data by ['model']
2025-08-20 17:01:49,397 WARNING analytics Using fallback aggregation: 'Series' object has no attribute 'columns'
2025-08-20 17:01:49,398 INFO analytics writing output to analysis.csv
2025-08-20 17:01:49,399 ERROR analytics missing config key: input_csv
2025-08-20 17:01:49,399 ERROR analytics missing config key: output_csv
2025-08-20 17:01:49,399 ERROR analytics missing config key: group_by
2025-08-20 17:01:49,402 INFO analytics loading input_csv: input.csv
2025-08-20 17:01:49,402 ERROR analytics input data is empty
2025-08-20 17:01:49,403 INFO analytics loading input_csv: nonexistent.csv
2025-08-20 17:01:49,403 ERROR analytics failed to load input_csv: File not found
2025-08-20 17:01:49,406 INFO analytics loading input_csv: /tmp/tmp78jopd32.csv
2025-08-20 17:01:49,407 INFO analytics aggregating data by ['model']
2025-08-20 17:01:49,408 WARNING analytics Using fallback aggregation: 'Series' object has no attribute 'columns'
2025-08-20 17:01:49,409 INFO analytics writing output to /tmp/tmpfp9pnadc.csv
2025-08-20 17:01:49,409 INFO src.io Wrote dataframe to /tmp/tmpfp9pnadc.csv (rows=2)
2025-08-20 17:01:50,724 ERROR visualization missing config key: input_csv
2025-08-20 17:01:50,725 ERROR visualization missing config key: output_dir
2025-08-20 17:01:50,725 ERROR visualization missing config key: visualization_type
2025-08-20 17:01:50,726 INFO visualization loading input_csv: nonexistent.csv
2025-08-20 17:01:50,726 ERROR visualization failed to load input_csv: File not found
2025-08-20 17:01:50,727 INFO visualization loading input_csv: input.csv
2025-08-20 17:01:50,727 ERROR visualization input data is empty
2025-08-20 17:01:50,729 INFO visualization loading input_csv: input.csv
2025-08-20 17:01:50,729 INFO visualization creating comparative visualization for score
2025-08-20 17:01:50,733 INFO visualization loading input_csv: input.csv
2025-08-20 17:01:50,734 INFO visualization creating statistical visualization for score
2025-08-20 17:01:50,736 INFO visualization loading input_csv: input.csv
2025-08-20 17:01:50,737 INFO visualization creating radar chart
2025-08-20 17:01:50,738 INFO visualization loading input_csv: input.csv
2025-08-20 17:01:50,739 ERROR visualization unknown visualization_type: unknown
2025-08-20 17:01:50,740 INFO visualization loading input_csv: input.csv
2025-08-20 17:01:50,740 ERROR visualization metric_name is required for comparative visualization
2025-08-20 17:01:50,741 INFO visualization loading input_csv: input.csv
2025-08-20 17:01:50,741 ERROR visualization metric_name is required for statistical visualization
2025-08-20 17:01:50,742 INFO visualization loading input_csv: input.csv
2025-08-20 17:01:50,743 ERROR visualization no numeric columns found for radar chart
2025-08-20 17:01:51,938 INFO visualization loading input_csv: /tmp/tmp7t2bsjnk.csv
2025-08-20 17:01:51,939 INFO visualization creating comparative visualization for score
2025-08-20 17:01:52,135 INFO visualization loading input_csv: /tmp/tmp7t2bsjnk.csv
2025-08-20 17:01:52,137 INFO visualization creating radar chart
2025-08-20 17:01:53,917 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model']
2025-08-20 17:01:53,918 INFO inference generated for model=Llama:8B prompt_len=12
2025-08-20 17:01:53,919 INFO inference generated for model=Llama:8B prompt_len=12
2025-08-20 17:01:53,919 INFO inference generated for model=gpt-3.5-turbo prompt_len=12
2025-08-20 17:01:53,919 INFO inference generated for model=gpt-3.5-turbo prompt_len=12
2025-08-20 17:01:53,920 INFO src.io Wrote dataframe to /tmp/tmp6z74pxw1/completions.csv (rows=4)
2025-08-20 17:01:53,920 INFO inference run_from_config inference finished, wrote /tmp/tmp6z74pxw1/completions.csv (rows=4) duration=0.00s
2025-08-20 17:01:53,920 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 17:01:53,922 INFO src.io Wrote dataframe to /tmp/tmp6z74pxw1/ratings.csv (rows=4)
2025-08-20 17:01:53,923 INFO judge run_from_config judge finished, wrote /tmp/tmp6z74pxw1/ratings.csv (rows=4) duration=0.00s
2025-08-20 17:01:53,923 INFO analytics loading input_csv: /tmp/tmp6z74pxw1/ratings.csv
2025-08-20 17:01:53,924 INFO analytics aggregating data by ['model']
2025-08-20 17:01:53,926 WARNING analytics Using fallback aggregation: 'Series' object has no attribute 'columns'
2025-08-20 17:01:53,927 INFO analytics writing output to /tmp/tmp6z74pxw1/analysis.csv
2025-08-20 17:01:53,928 INFO src.io Wrote dataframe to /tmp/tmp6z74pxw1/analysis.csv (rows=2)
2025-08-20 17:01:54,421 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 17:01:54,422 INFO inference generated for model=Llama:8B prompt_len=26
2025-08-20 17:01:54,422 INFO inference generated for model=Llama:8B prompt_len=25
2025-08-20 17:01:54,422 INFO inference generated for model=Llama:8B prompt_len=31
2025-08-20 17:01:54,423 INFO inference run_from_config inference finished, wrote test_output.csv (rows=3) duration=0.00s
2025-08-20 17:01:54,425 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 17:01:54,425 INFO inference generated for model=Llama:8B prompt_len=26
2025-08-20 17:01:54,426 ERROR inference generation failed for model=Llama:8B: Ollama API error: context length exceeded
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 58, in run_from_config
    resp: ModelResponse = model.generate(prompt)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1177, in _execute_mock_call
    raise result
Exception: Ollama API error: context length exceeded
2025-08-20 17:01:54,429 INFO inference generated for model=Llama:8B prompt_len=31
2025-08-20 17:01:54,430 INFO inference run_from_config inference finished, wrote test_output.csv (rows=3) duration=0.00s
2025-08-20 17:01:54,432 INFO inference run_from_config start for inference, keys=['output_csv']
2025-08-20 17:01:54,432 ERROR inference missing config key: input_csv
2025-08-20 17:01:54,432 INFO inference run_from_config start for inference, keys=['input_csv']
2025-08-20 17:01:54,432 ERROR inference missing config key: output_csv
2025-08-20 17:01:54,433 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv']
2025-08-20 17:01:54,434 ERROR inference input CSV missing required 'prompt' column
2025-08-20 17:01:54,435 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv']
2025-08-20 17:01:54,435 ERROR inference failed to load input_csv: File not found
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 42, in run_from_config
    df = load_csv(input_csv)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1173, in _execute_mock_call
    raise effect
Exception: File not found
2025-08-20 17:01:54,438 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 17:01:54,439 INFO inference generated for model=Llama:8B prompt_len=12
2025-08-20 17:01:54,440 INFO src.io Wrote dataframe to /tmp/tmpu1e80r4o.csv (rows=1)
2025-08-20 17:01:54,440 INFO inference run_from_config inference finished, wrote /tmp/tmpu1e80r4o.csv (rows=1) duration=0.00s
2025-08-20 17:01:54,973 WARNING judge template_dir does not exist: nonexistent_dir
2025-08-20 17:01:54,977 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 17:01:54,979 INFO judge run_from_config judge finished, wrote ratings.csv (rows=3) duration=0.00s
2025-08-20 17:01:54,981 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 17:01:54,982 INFO judge run_from_config judge finished, wrote ratings.csv (rows=1) duration=0.00s
2025-08-20 17:01:54,982 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 17:01:54,983 INFO judge run_from_config judge finished, wrote ratings.csv (rows=1) duration=0.00s
2025-08-20 17:01:54,984 INFO judge run_from_config start for judge, keys=['output_csv', 'template_dir']
2025-08-20 17:01:54,984 ERROR judge missing config key: input_csv
2025-08-20 17:01:54,984 INFO judge run_from_config start for judge, keys=['input_csv', 'template_dir']
2025-08-20 17:01:54,984 ERROR judge missing config key: output_csv
2025-08-20 17:01:54,985 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv']
2025-08-20 17:01:54,985 ERROR judge missing config key: template_dir
2025-08-20 17:01:54,986 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 17:01:54,986 ERROR judge failed to load input_csv: File not found
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/judge.py", line 83, in run_from_config
    df = load_csv(input_csv)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1173, in _execute_mock_call
    raise effect
Exception: File not found
2025-08-20 17:01:54,993 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 17:01:54,996 INFO src.io Wrote dataframe to /tmp/tmp6tqdxhbc.csv (rows=1)
2025-08-20 17:01:54,996 INFO judge run_from_config judge finished, wrote /tmp/tmp6tqdxhbc.csv (rows=1) duration=0.00s
2025-08-20 17:02:03,643 ERROR visualization missing config key: input_csv
2025-08-20 17:02:03,643 ERROR visualization missing config key: output_dir
2025-08-20 17:02:03,643 ERROR visualization missing config key: visualization_type
2025-08-20 17:02:03,645 INFO visualization loading input_csv: nonexistent.csv
2025-08-20 17:02:03,645 ERROR visualization failed to load input_csv: File not found
2025-08-20 17:02:03,646 INFO visualization loading input_csv: input.csv
2025-08-20 17:02:03,646 ERROR visualization input data is empty
2025-08-20 17:02:03,648 INFO visualization loading input_csv: input.csv
2025-08-20 17:02:03,648 INFO visualization creating comparative visualization for score
2025-08-20 17:02:03,652 INFO visualization loading input_csv: input.csv
2025-08-20 17:02:03,652 INFO visualization creating statistical visualization for score
2025-08-20 17:02:03,655 INFO visualization loading input_csv: input.csv
2025-08-20 17:02:03,656 INFO visualization creating radar chart
2025-08-20 17:02:03,657 INFO visualization loading input_csv: input.csv
2025-08-20 17:02:03,658 ERROR visualization unknown visualization_type: unknown
2025-08-20 17:02:03,659 INFO visualization loading input_csv: input.csv
2025-08-20 17:02:03,660 ERROR visualization metric_name is required for comparative visualization
2025-08-20 17:02:03,660 INFO visualization loading input_csv: input.csv
2025-08-20 17:02:03,661 ERROR visualization metric_name is required for statistical visualization
2025-08-20 17:02:03,662 INFO visualization loading input_csv: input.csv
2025-08-20 17:02:03,663 ERROR visualization no numeric columns found for radar chart
2025-08-20 17:02:03,917 INFO analytics loading input_csv: ratings.csv
2025-08-20 17:02:03,917 INFO analytics aggregating data by ['model']
2025-08-20 17:02:03,919 WARNING analytics Using fallback aggregation: 'Series' object has no attribute 'columns'
2025-08-20 17:02:03,920 INFO analytics writing output to analysis.csv
2025-08-20 17:02:03,922 INFO analytics loading input_csv: ratings.csv
2025-08-20 17:02:03,922 INFO analytics aggregating data by ['model']
2025-08-20 17:02:03,924 WARNING analytics Using fallback aggregation: 'Series' object has no attribute 'columns'
2025-08-20 17:02:03,925 INFO analytics writing output to analysis.csv
2025-08-20 17:02:03,926 ERROR analytics missing config key: input_csv
2025-08-20 17:02:03,926 ERROR analytics missing config key: output_csv
2025-08-20 17:02:03,927 ERROR analytics missing config key: group_by
2025-08-20 17:02:03,928 INFO analytics loading input_csv: input.csv
2025-08-20 17:02:03,928 ERROR analytics input data is empty
2025-08-20 17:02:03,930 INFO analytics loading input_csv: nonexistent.csv
2025-08-20 17:02:03,930 ERROR analytics failed to load input_csv: File not found
2025-08-20 17:04:22,617 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 17:04:22,618 INFO inference generated for model=Llama:8B prompt_len=26
2025-08-20 17:04:22,618 INFO inference generated for model=Llama:8B prompt_len=25
2025-08-20 17:04:22,618 INFO inference generated for model=Llama:8B prompt_len=31
2025-08-20 17:04:22,619 INFO inference run_from_config inference finished, wrote test_output.csv (rows=3) duration=0.00s
2025-08-20 17:04:22,621 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 17:04:22,621 INFO inference generated for model=Llama:8B prompt_len=26
2025-08-20 17:04:22,621 ERROR inference generation failed for model=Llama:8B: Ollama API error: context length exceeded
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 58, in run_from_config
    resp: ModelResponse = model.generate(prompt)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1177, in _execute_mock_call
    raise result
Exception: Ollama API error: context length exceeded
2025-08-20 17:04:22,623 INFO inference generated for model=Llama:8B prompt_len=31
2025-08-20 17:04:22,624 INFO inference run_from_config inference finished, wrote test_output.csv (rows=3) duration=0.00s
2025-08-20 17:04:22,625 INFO inference run_from_config start for inference, keys=['output_csv']
2025-08-20 17:04:22,625 ERROR inference missing config key: input_csv
2025-08-20 17:04:22,626 INFO inference run_from_config start for inference, keys=['input_csv']
2025-08-20 17:04:22,626 ERROR inference missing config key: output_csv
2025-08-20 17:04:22,627 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv']
2025-08-20 17:04:22,628 ERROR inference input CSV missing required 'prompt' column
2025-08-20 17:04:22,628 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv']
2025-08-20 17:04:22,629 ERROR inference failed to load input_csv: File not found
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 42, in run_from_config
    df = load_csv(input_csv)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1173, in _execute_mock_call
    raise effect
Exception: File not found
2025-08-20 17:04:22,632 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 17:04:22,634 INFO inference generated for model=Llama:8B prompt_len=12
2025-08-20 17:04:22,635 INFO src.io Wrote dataframe to /tmp/tmpy6dqpkub.csv (rows=1)
2025-08-20 17:04:22,635 INFO inference run_from_config inference finished, wrote /tmp/tmpy6dqpkub.csv (rows=1) duration=0.00s
2025-08-20 17:04:23,052 WARNING judge template_dir does not exist: nonexistent_dir
2025-08-20 17:04:23,054 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 17:04:23,055 INFO judge run_from_config judge finished, wrote ratings.csv (rows=3) duration=0.00s
2025-08-20 17:04:23,057 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 17:04:23,058 INFO judge run_from_config judge finished, wrote ratings.csv (rows=1) duration=0.00s
2025-08-20 17:04:23,058 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 17:04:23,058 INFO judge run_from_config judge finished, wrote ratings.csv (rows=1) duration=0.00s
2025-08-20 17:04:23,059 INFO judge run_from_config start for judge, keys=['output_csv', 'template_dir']
2025-08-20 17:04:23,059 ERROR judge missing config key: input_csv
2025-08-20 17:04:23,059 INFO judge run_from_config start for judge, keys=['input_csv', 'template_dir']
2025-08-20 17:04:23,060 ERROR judge missing config key: output_csv
2025-08-20 17:04:23,060 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv']
2025-08-20 17:04:23,060 ERROR judge missing config key: template_dir
2025-08-20 17:04:23,061 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 17:04:23,061 ERROR judge failed to load input_csv: File not found
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/judge.py", line 83, in run_from_config
    df = load_csv(input_csv)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1173, in _execute_mock_call
    raise effect
Exception: File not found
2025-08-20 17:04:23,064 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 17:04:23,066 INFO src.io Wrote dataframe to /tmp/tmpdwbjogbs.csv (rows=1)
2025-08-20 17:04:23,066 INFO judge run_from_config judge finished, wrote /tmp/tmpdwbjogbs.csv (rows=1) duration=0.00s
2025-08-20 17:04:30,161 ERROR visualization missing config key: input_csv
2025-08-20 17:04:30,161 ERROR visualization missing config key: output_dir
2025-08-20 17:04:30,161 ERROR visualization missing config key: visualization_type
2025-08-20 17:04:30,162 INFO visualization loading input_csv: nonexistent.csv
2025-08-20 17:04:30,162 ERROR visualization failed to load input_csv: File not found
2025-08-20 17:04:30,164 INFO visualization loading input_csv: input.csv
2025-08-20 17:04:30,164 ERROR visualization input data is empty
2025-08-20 17:04:30,167 INFO visualization loading input_csv: input.csv
2025-08-20 17:04:30,167 INFO visualization creating comparative visualization for score
2025-08-20 17:04:30,172 INFO visualization loading input_csv: input.csv
2025-08-20 17:04:30,172 INFO visualization creating statistical visualization for score
2025-08-20 17:04:30,176 INFO visualization loading input_csv: input.csv
2025-08-20 17:04:30,176 INFO visualization creating radar chart
2025-08-20 17:04:30,178 INFO visualization loading input_csv: input.csv
2025-08-20 17:04:30,178 ERROR visualization unknown visualization_type: unknown
2025-08-20 17:04:30,179 INFO visualization loading input_csv: input.csv
2025-08-20 17:04:30,180 ERROR visualization metric_name is required for comparative visualization
2025-08-20 17:04:30,180 INFO visualization loading input_csv: input.csv
2025-08-20 17:04:30,180 ERROR visualization metric_name is required for statistical visualization
2025-08-20 17:04:30,181 INFO visualization loading input_csv: input.csv
2025-08-20 17:04:30,182 ERROR visualization no numeric columns found for radar chart
2025-08-20 17:04:32,802 INFO analytics loading input_csv: ratings.csv
2025-08-20 17:04:32,802 INFO analytics aggregating data by ['model']
2025-08-20 17:04:32,804 WARNING analytics Using fallback aggregation: 'Series' object has no attribute 'columns'
2025-08-20 17:04:32,805 INFO analytics writing output to analysis.csv
2025-08-20 17:04:32,806 INFO analytics loading input_csv: ratings.csv
2025-08-20 17:04:32,806 INFO analytics aggregating data by ['model']
2025-08-20 17:04:32,808 WARNING analytics Using fallback aggregation: 'Series' object has no attribute 'columns'
2025-08-20 17:04:32,809 INFO analytics writing output to analysis.csv
2025-08-20 17:04:32,810 ERROR analytics missing config key: input_csv
2025-08-20 17:04:32,810 ERROR analytics missing config key: output_csv
2025-08-20 17:04:32,810 ERROR analytics missing config key: group_by
2025-08-20 17:04:32,813 INFO analytics loading input_csv: input.csv
2025-08-20 17:04:32,813 ERROR analytics input data is empty
2025-08-20 17:04:32,814 INFO analytics loading input_csv: nonexistent.csv
2025-08-20 17:04:32,814 ERROR analytics failed to load input_csv: File not found
2025-08-20 17:04:32,817 INFO analytics loading input_csv: /tmp/tmp69yo4ox4.csv
2025-08-20 17:04:32,818 INFO analytics aggregating data by ['model']
2025-08-20 17:04:32,820 WARNING analytics Using fallback aggregation: 'Series' object has no attribute 'columns'
2025-08-20 17:04:32,821 INFO analytics writing output to /tmp/tmpwj2pxs68.csv
2025-08-20 17:04:32,821 INFO src.io Wrote dataframe to /tmp/tmpwj2pxs68.csv (rows=2)
2025-08-20 17:04:34,164 ERROR visualization missing config key: input_csv
2025-08-20 17:04:34,164 ERROR visualization missing config key: output_dir
2025-08-20 17:04:34,164 ERROR visualization missing config key: visualization_type
2025-08-20 17:04:34,165 INFO visualization loading input_csv: nonexistent.csv
2025-08-20 17:04:34,165 ERROR visualization failed to load input_csv: File not found
2025-08-20 17:04:34,166 INFO visualization loading input_csv: input.csv
2025-08-20 17:04:34,166 ERROR visualization input data is empty
2025-08-20 17:04:34,169 INFO visualization loading input_csv: input.csv
2025-08-20 17:04:34,169 INFO visualization creating comparative visualization for score
2025-08-20 17:04:34,172 INFO visualization loading input_csv: input.csv
2025-08-20 17:04:34,172 INFO visualization creating statistical visualization for score
2025-08-20 17:04:34,176 INFO visualization loading input_csv: input.csv
2025-08-20 17:04:34,176 INFO visualization creating radar chart
2025-08-20 17:04:34,178 INFO visualization loading input_csv: input.csv
2025-08-20 17:04:34,179 ERROR visualization unknown visualization_type: unknown
2025-08-20 17:04:34,180 INFO visualization loading input_csv: input.csv
2025-08-20 17:04:34,180 ERROR visualization metric_name is required for comparative visualization
2025-08-20 17:04:34,180 INFO visualization loading input_csv: input.csv
2025-08-20 17:04:34,181 ERROR visualization metric_name is required for statistical visualization
2025-08-20 17:04:34,182 INFO visualization loading input_csv: input.csv
2025-08-20 17:04:34,182 ERROR visualization no numeric columns found for radar chart
2025-08-20 17:04:35,442 INFO visualization loading input_csv: /tmp/tmpd5g4fss9.csv
2025-08-20 17:04:35,444 INFO visualization creating comparative visualization for score
2025-08-20 17:04:35,655 INFO visualization loading input_csv: /tmp/tmpd5g4fss9.csv
2025-08-20 17:04:35,657 INFO visualization creating radar chart
2025-08-20 17:04:37,521 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model']
2025-08-20 17:04:37,523 INFO inference generated for model=Llama:8B prompt_len=12
2025-08-20 17:04:37,523 INFO inference generated for model=Llama:8B prompt_len=12
2025-08-20 17:04:37,523 INFO inference generated for model=gpt-3.5-turbo prompt_len=12
2025-08-20 17:04:37,524 INFO inference generated for model=gpt-3.5-turbo prompt_len=12
2025-08-20 17:04:37,524 INFO src.io Wrote dataframe to /tmp/tmplshtnwkh/completions.csv (rows=4)
2025-08-20 17:04:37,524 INFO inference run_from_config inference finished, wrote /tmp/tmplshtnwkh/completions.csv (rows=4) duration=0.00s
2025-08-20 17:04:37,525 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 17:04:37,527 INFO src.io Wrote dataframe to /tmp/tmplshtnwkh/ratings.csv (rows=4)
2025-08-20 17:04:37,527 INFO judge run_from_config judge finished, wrote /tmp/tmplshtnwkh/ratings.csv (rows=4) duration=0.00s
2025-08-20 17:04:37,528 INFO analytics loading input_csv: /tmp/tmplshtnwkh/ratings.csv
2025-08-20 17:04:37,528 INFO analytics aggregating data by ['model']
2025-08-20 17:04:37,530 WARNING analytics Using fallback aggregation: 'Series' object has no attribute 'columns'
2025-08-20 17:04:37,531 INFO analytics writing output to /tmp/tmplshtnwkh/analysis.csv
2025-08-20 17:04:37,531 INFO src.io Wrote dataframe to /tmp/tmplshtnwkh/analysis.csv (rows=2)
2025-08-20 17:04:38,056 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 17:04:38,057 INFO inference generated for model=Llama:8B prompt_len=26
2025-08-20 17:04:38,057 INFO inference generated for model=Llama:8B prompt_len=25
2025-08-20 17:04:38,057 INFO inference generated for model=Llama:8B prompt_len=31
2025-08-20 17:04:38,058 INFO inference run_from_config inference finished, wrote test_output.csv (rows=3) duration=0.00s
2025-08-20 17:04:38,060 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 17:04:38,061 INFO inference generated for model=Llama:8B prompt_len=26
2025-08-20 17:04:38,061 ERROR inference generation failed for model=Llama:8B: Ollama API error: context length exceeded
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 58, in run_from_config
    resp: ModelResponse = model.generate(prompt)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1177, in _execute_mock_call
    raise result
Exception: Ollama API error: context length exceeded
2025-08-20 17:04:38,063 INFO inference generated for model=Llama:8B prompt_len=31
2025-08-20 17:04:38,064 INFO inference run_from_config inference finished, wrote test_output.csv (rows=3) duration=0.00s
2025-08-20 17:04:38,065 INFO inference run_from_config start for inference, keys=['output_csv']
2025-08-20 17:04:38,065 ERROR inference missing config key: input_csv
2025-08-20 17:04:38,065 INFO inference run_from_config start for inference, keys=['input_csv']
2025-08-20 17:04:38,066 ERROR inference missing config key: output_csv
2025-08-20 17:04:38,067 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv']
2025-08-20 17:04:38,067 ERROR inference input CSV missing required 'prompt' column
2025-08-20 17:04:38,068 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv']
2025-08-20 17:04:38,068 ERROR inference failed to load input_csv: File not found
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 42, in run_from_config
    df = load_csv(input_csv)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1173, in _execute_mock_call
    raise effect
Exception: File not found
2025-08-20 17:04:38,071 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 17:04:38,073 INFO inference generated for model=Llama:8B prompt_len=12
2025-08-20 17:04:38,073 INFO src.io Wrote dataframe to /tmp/tmpy3j2t3c5.csv (rows=1)
2025-08-20 17:04:38,073 INFO inference run_from_config inference finished, wrote /tmp/tmpy3j2t3c5.csv (rows=1) duration=0.00s
2025-08-20 17:04:38,614 WARNING judge template_dir does not exist: nonexistent_dir
2025-08-20 17:04:38,617 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 17:04:38,619 INFO judge run_from_config judge finished, wrote ratings.csv (rows=3) duration=0.00s
2025-08-20 17:04:38,621 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 17:04:38,622 INFO judge run_from_config judge finished, wrote ratings.csv (rows=1) duration=0.00s
2025-08-20 17:04:38,622 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 17:04:38,623 INFO judge run_from_config judge finished, wrote ratings.csv (rows=1) duration=0.00s
2025-08-20 17:04:38,625 INFO judge run_from_config start for judge, keys=['output_csv', 'template_dir']
2025-08-20 17:04:38,625 ERROR judge missing config key: input_csv
2025-08-20 17:04:38,625 INFO judge run_from_config start for judge, keys=['input_csv', 'template_dir']
2025-08-20 17:04:38,625 ERROR judge missing config key: output_csv
2025-08-20 17:04:38,625 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv']
2025-08-20 17:04:38,625 ERROR judge missing config key: template_dir
2025-08-20 17:04:38,627 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 17:04:38,627 ERROR judge failed to load input_csv: File not found
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/judge.py", line 83, in run_from_config
    df = load_csv(input_csv)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1173, in _execute_mock_call
    raise effect
Exception: File not found
2025-08-20 17:04:38,633 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 17:04:38,635 INFO src.io Wrote dataframe to /tmp/tmp3xhujdlv.csv (rows=1)
2025-08-20 17:04:38,635 INFO judge run_from_config judge finished, wrote /tmp/tmp3xhujdlv.csv (rows=1) duration=0.00s
2025-08-20 17:04:46,821 ERROR visualization missing config key: input_csv
2025-08-20 17:04:46,822 ERROR visualization missing config key: output_dir
2025-08-20 17:04:46,822 ERROR visualization missing config key: visualization_type
2025-08-20 17:04:46,823 INFO visualization loading input_csv: nonexistent.csv
2025-08-20 17:04:46,823 ERROR visualization failed to load input_csv: File not found
2025-08-20 17:04:46,825 INFO visualization loading input_csv: input.csv
2025-08-20 17:04:46,825 ERROR visualization input data is empty
2025-08-20 17:04:46,827 INFO visualization loading input_csv: input.csv
2025-08-20 17:04:46,827 INFO visualization creating comparative visualization for score
2025-08-20 17:04:46,830 INFO visualization loading input_csv: input.csv
2025-08-20 17:04:46,831 INFO visualization creating statistical visualization for score
2025-08-20 17:04:46,834 INFO visualization loading input_csv: input.csv
2025-08-20 17:04:46,834 INFO visualization creating radar chart
2025-08-20 17:04:46,836 INFO visualization loading input_csv: input.csv
2025-08-20 17:04:46,837 ERROR visualization unknown visualization_type: unknown
2025-08-20 17:04:46,839 INFO visualization loading input_csv: input.csv
2025-08-20 17:04:46,840 ERROR visualization metric_name is required for comparative visualization
2025-08-20 17:04:46,840 INFO visualization loading input_csv: input.csv
2025-08-20 17:04:46,841 ERROR visualization metric_name is required for statistical visualization
2025-08-20 17:04:46,842 INFO visualization loading input_csv: input.csv
2025-08-20 17:04:46,843 ERROR visualization no numeric columns found for radar chart
2025-08-20 17:04:47,094 INFO analytics loading input_csv: ratings.csv
2025-08-20 17:04:47,094 INFO analytics aggregating data by ['model']
2025-08-20 17:04:47,096 WARNING analytics Using fallback aggregation: 'Series' object has no attribute 'columns'
2025-08-20 17:04:47,097 INFO analytics writing output to analysis.csv
2025-08-20 17:04:47,099 INFO analytics loading input_csv: ratings.csv
2025-08-20 17:04:47,099 INFO analytics aggregating data by ['model']
2025-08-20 17:04:47,101 WARNING analytics Using fallback aggregation: 'Series' object has no attribute 'columns'
2025-08-20 17:04:47,102 INFO analytics writing output to analysis.csv
2025-08-20 17:04:47,103 ERROR analytics missing config key: input_csv
2025-08-20 17:04:47,103 ERROR analytics missing config key: output_csv
2025-08-20 17:04:47,104 ERROR analytics missing config key: group_by
2025-08-20 17:04:47,105 INFO analytics loading input_csv: input.csv
2025-08-20 17:04:47,105 ERROR analytics input data is empty
2025-08-20 17:04:47,106 INFO analytics loading input_csv: nonexistent.csv
2025-08-20 17:04:47,106 ERROR analytics failed to load input_csv: File not found
2025-08-20 17:08:26,456 ERROR visualization missing config key: input_csv
2025-08-20 17:08:26,456 ERROR visualization missing config key: output_dir
2025-08-20 17:08:26,456 ERROR visualization missing config key: visualization_type
2025-08-20 17:08:26,457 INFO visualization loading input_csv: nonexistent.csv
2025-08-20 17:08:26,457 ERROR visualization failed to load input_csv: File not found
2025-08-20 17:08:26,458 INFO visualization loading input_csv: input.csv
2025-08-20 17:08:26,458 ERROR visualization input data is empty
2025-08-20 17:08:26,460 INFO visualization loading input_csv: input.csv
2025-08-20 17:08:26,460 INFO visualization creating comparative visualization for score
2025-08-20 17:08:26,466 INFO visualization loading input_csv: input.csv
2025-08-20 17:08:26,466 INFO visualization creating statistical visualization for score
2025-08-20 17:08:26,468 INFO visualization loading input_csv: input.csv
2025-08-20 17:08:26,469 INFO visualization creating radar chart
2025-08-20 17:08:26,471 INFO visualization loading input_csv: input.csv
2025-08-20 17:08:26,471 ERROR visualization unknown visualization_type: unknown
2025-08-20 17:08:26,473 INFO visualization loading input_csv: input.csv
2025-08-20 17:08:26,473 ERROR visualization metric_name is required for comparative visualization
2025-08-20 17:08:26,473 INFO visualization loading input_csv: input.csv
2025-08-20 17:08:26,474 ERROR visualization metric_name is required for statistical visualization
2025-08-20 17:08:26,475 INFO visualization loading input_csv: input.csv
2025-08-20 17:08:26,475 ERROR visualization no numeric columns found for radar chart
2025-08-20 17:08:45,891 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 17:08:45,891 INFO inference generated for model=Llama:8B prompt_len=26
2025-08-20 17:08:45,892 INFO inference generated for model=Llama:8B prompt_len=25
2025-08-20 17:08:45,892 INFO inference generated for model=Llama:8B prompt_len=31
2025-08-20 17:08:45,892 INFO inference run_from_config inference finished, wrote test_output.csv (rows=3) duration=0.00s
2025-08-20 17:08:45,894 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 17:08:45,894 INFO inference generated for model=Llama:8B prompt_len=26
2025-08-20 17:08:45,894 ERROR inference generation failed for model=Llama:8B: Ollama API error: context length exceeded
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 58, in run_from_config
    resp: ModelResponse = model.generate(prompt)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1177, in _execute_mock_call
    raise result
Exception: Ollama API error: context length exceeded
2025-08-20 17:08:45,896 INFO inference generated for model=Llama:8B prompt_len=31
2025-08-20 17:08:45,896 INFO inference run_from_config inference finished, wrote test_output.csv (rows=3) duration=0.00s
2025-08-20 17:08:45,897 INFO inference run_from_config start for inference, keys=['output_csv']
2025-08-20 17:08:45,897 ERROR inference missing config key: input_csv
2025-08-20 17:08:45,898 INFO inference run_from_config start for inference, keys=['input_csv']
2025-08-20 17:08:45,898 ERROR inference missing config key: output_csv
2025-08-20 17:08:45,899 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv']
2025-08-20 17:08:45,899 ERROR inference input CSV missing required 'prompt' column
2025-08-20 17:08:45,900 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv']
2025-08-20 17:08:45,900 ERROR inference failed to load input_csv: File not found
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 42, in run_from_config
    df = load_csv(input_csv)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1173, in _execute_mock_call
    raise effect
Exception: File not found
2025-08-20 17:08:45,906 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 17:08:45,908 INFO inference generated for model=Llama:8B prompt_len=12
2025-08-20 17:08:45,908 INFO src.io Wrote dataframe to /tmp/tmpiqqw1tnf.csv (rows=1)
2025-08-20 17:08:45,908 INFO inference run_from_config inference finished, wrote /tmp/tmpiqqw1tnf.csv (rows=1) duration=0.00s
2025-08-20 17:08:46,330 WARNING judge template_dir does not exist: nonexistent_dir
2025-08-20 17:08:46,332 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 17:08:46,333 INFO judge run_from_config judge finished, wrote ratings.csv (rows=3) duration=0.00s
2025-08-20 17:08:46,334 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 17:08:46,336 INFO judge run_from_config judge finished, wrote ratings.csv (rows=1) duration=0.00s
2025-08-20 17:08:46,336 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 17:08:46,336 INFO judge run_from_config judge finished, wrote ratings.csv (rows=1) duration=0.00s
2025-08-20 17:08:46,337 INFO judge run_from_config start for judge, keys=['output_csv', 'template_dir']
2025-08-20 17:08:46,337 ERROR judge missing config key: input_csv
2025-08-20 17:08:46,337 INFO judge run_from_config start for judge, keys=['input_csv', 'template_dir']
2025-08-20 17:08:46,337 ERROR judge missing config key: output_csv
2025-08-20 17:08:46,337 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv']
2025-08-20 17:08:46,337 ERROR judge missing config key: template_dir
2025-08-20 17:08:46,338 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 17:08:46,338 ERROR judge failed to load input_csv: File not found
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/judge.py", line 83, in run_from_config
    df = load_csv(input_csv)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1173, in _execute_mock_call
    raise effect
Exception: File not found
2025-08-20 17:08:46,341 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 17:08:46,343 INFO src.io Wrote dataframe to /tmp/tmps8yddh2x.csv (rows=1)
2025-08-20 17:08:46,343 INFO judge run_from_config judge finished, wrote /tmp/tmps8yddh2x.csv (rows=1) duration=0.00s
2025-08-20 17:08:56,549 ERROR visualization missing config key: input_csv
2025-08-20 17:08:56,550 ERROR visualization missing config key: output_dir
2025-08-20 17:08:56,550 ERROR visualization missing config key: visualization_type
2025-08-20 17:08:56,551 INFO visualization loading input_csv: nonexistent.csv
2025-08-20 17:08:56,551 ERROR visualization failed to load input_csv: File not found
2025-08-20 17:08:56,552 INFO visualization loading input_csv: input.csv
2025-08-20 17:08:56,552 ERROR visualization input data is empty
2025-08-20 17:08:56,555 INFO visualization loading input_csv: input.csv
2025-08-20 17:08:56,555 INFO visualization creating comparative visualization for score
2025-08-20 17:08:56,558 INFO visualization loading input_csv: input.csv
2025-08-20 17:08:56,558 INFO visualization creating statistical visualization for score
2025-08-20 17:08:56,560 INFO visualization loading input_csv: input.csv
2025-08-20 17:08:56,561 INFO visualization creating radar chart
2025-08-20 17:08:56,562 INFO visualization loading input_csv: input.csv
2025-08-20 17:08:56,562 ERROR visualization unknown visualization_type: unknown
2025-08-20 17:08:56,564 INFO visualization loading input_csv: input.csv
2025-08-20 17:08:56,564 ERROR visualization metric_name is required for comparative visualization
2025-08-20 17:08:56,564 INFO visualization loading input_csv: input.csv
2025-08-20 17:08:56,564 ERROR visualization metric_name is required for statistical visualization
2025-08-20 17:08:56,565 INFO visualization loading input_csv: input.csv
2025-08-20 17:08:56,566 ERROR visualization no numeric columns found for radar chart
2025-08-20 17:08:59,301 INFO analytics loading input_csv: ratings.csv
2025-08-20 17:08:59,301 INFO analytics aggregating data by ['model']
2025-08-20 17:08:59,303 WARNING analytics Using fallback aggregation: 'Series' object has no attribute 'columns'
2025-08-20 17:08:59,304 INFO analytics writing output to analysis.csv
2025-08-20 17:08:59,305 INFO analytics loading input_csv: ratings.csv
2025-08-20 17:08:59,305 INFO analytics aggregating data by ['model']
2025-08-20 17:08:59,307 WARNING analytics Using fallback aggregation: 'Series' object has no attribute 'columns'
2025-08-20 17:08:59,307 INFO analytics writing output to analysis.csv
2025-08-20 17:08:59,308 ERROR analytics missing config key: input_csv
2025-08-20 17:08:59,309 ERROR analytics missing config key: output_csv
2025-08-20 17:08:59,309 ERROR analytics missing config key: group_by
2025-08-20 17:08:59,310 INFO analytics loading input_csv: input.csv
2025-08-20 17:08:59,310 ERROR analytics input data is empty
2025-08-20 17:08:59,311 INFO analytics loading input_csv: nonexistent.csv
2025-08-20 17:08:59,311 ERROR analytics failed to load input_csv: File not found
2025-08-20 17:08:59,314 INFO analytics loading input_csv: /tmp/tmp18tujobw.csv
2025-08-20 17:08:59,315 INFO analytics aggregating data by ['model']
2025-08-20 17:08:59,317 WARNING analytics Using fallback aggregation: 'Series' object has no attribute 'columns'
2025-08-20 17:08:59,318 INFO analytics writing output to /tmp/tmpdpc5m63p.csv
2025-08-20 17:08:59,319 INFO src.io Wrote dataframe to /tmp/tmpdpc5m63p.csv (rows=2)
2025-08-20 17:09:00,575 ERROR visualization missing config key: input_csv
2025-08-20 17:09:00,575 ERROR visualization missing config key: output_dir
2025-08-20 17:09:00,575 ERROR visualization missing config key: visualization_type
2025-08-20 17:09:00,576 INFO visualization loading input_csv: nonexistent.csv
2025-08-20 17:09:00,576 ERROR visualization failed to load input_csv: File not found
2025-08-20 17:09:00,578 INFO visualization loading input_csv: input.csv
2025-08-20 17:09:00,578 ERROR visualization input data is empty
2025-08-20 17:09:00,580 INFO visualization loading input_csv: input.csv
2025-08-20 17:09:00,580 INFO visualization creating comparative visualization for score
2025-08-20 17:09:00,584 INFO visualization loading input_csv: input.csv
2025-08-20 17:09:00,584 INFO visualization creating statistical visualization for score
2025-08-20 17:09:00,587 INFO visualization loading input_csv: input.csv
2025-08-20 17:09:00,587 INFO visualization creating radar chart
2025-08-20 17:09:00,590 INFO visualization loading input_csv: input.csv
2025-08-20 17:09:00,590 ERROR visualization unknown visualization_type: unknown
2025-08-20 17:09:00,592 INFO visualization loading input_csv: input.csv
2025-08-20 17:09:00,592 ERROR visualization metric_name is required for comparative visualization
2025-08-20 17:09:00,592 INFO visualization loading input_csv: input.csv
2025-08-20 17:09:00,593 ERROR visualization metric_name is required for statistical visualization
2025-08-20 17:09:00,594 INFO visualization loading input_csv: input.csv
2025-08-20 17:09:00,594 ERROR visualization no numeric columns found for radar chart
2025-08-20 17:09:01,818 INFO visualization loading input_csv: /tmp/tmpjvut08x3.csv
2025-08-20 17:09:01,820 INFO visualization creating comparative visualization for score
2025-08-20 17:09:02,033 INFO visualization loading input_csv: /tmp/tmpjvut08x3.csv
2025-08-20 17:09:02,034 INFO visualization creating radar chart
2025-08-20 17:09:03,790 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model']
2025-08-20 17:09:03,792 INFO inference generated for model=Llama:8B prompt_len=12
2025-08-20 17:09:03,793 INFO inference generated for model=Llama:8B prompt_len=12
2025-08-20 17:09:03,793 INFO inference generated for model=gpt-3.5-turbo prompt_len=12
2025-08-20 17:09:03,793 INFO inference generated for model=gpt-3.5-turbo prompt_len=12
2025-08-20 17:09:03,794 INFO src.io Wrote dataframe to /tmp/tmp0u4f43az/completions.csv (rows=4)
2025-08-20 17:09:03,794 INFO inference run_from_config inference finished, wrote /tmp/tmp0u4f43az/completions.csv (rows=4) duration=0.00s
2025-08-20 17:09:03,794 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 17:09:03,797 INFO src.io Wrote dataframe to /tmp/tmp0u4f43az/ratings.csv (rows=4)
2025-08-20 17:09:03,798 INFO judge run_from_config judge finished, wrote /tmp/tmp0u4f43az/ratings.csv (rows=4) duration=0.00s
2025-08-20 17:09:03,799 INFO analytics loading input_csv: /tmp/tmp0u4f43az/ratings.csv
2025-08-20 17:09:03,800 INFO analytics aggregating data by ['model']
2025-08-20 17:09:03,802 WARNING analytics Using fallback aggregation: 'Series' object has no attribute 'columns'
2025-08-20 17:09:03,802 INFO analytics writing output to /tmp/tmp0u4f43az/analysis.csv
2025-08-20 17:09:03,803 INFO src.io Wrote dataframe to /tmp/tmp0u4f43az/analysis.csv (rows=2)
2025-08-20 17:09:04,335 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 17:09:04,336 INFO inference generated for model=Llama:8B prompt_len=26
2025-08-20 17:09:04,336 INFO inference generated for model=Llama:8B prompt_len=25
2025-08-20 17:09:04,336 INFO inference generated for model=Llama:8B prompt_len=31
2025-08-20 17:09:04,337 INFO inference run_from_config inference finished, wrote test_output.csv (rows=3) duration=0.00s
2025-08-20 17:09:04,340 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 17:09:04,340 INFO inference generated for model=Llama:8B prompt_len=26
2025-08-20 17:09:04,340 ERROR inference generation failed for model=Llama:8B: Ollama API error: context length exceeded
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 58, in run_from_config
    resp: ModelResponse = model.generate(prompt)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1177, in _execute_mock_call
    raise result
Exception: Ollama API error: context length exceeded
2025-08-20 17:09:04,342 INFO inference generated for model=Llama:8B prompt_len=31
2025-08-20 17:09:04,343 INFO inference run_from_config inference finished, wrote test_output.csv (rows=3) duration=0.00s
2025-08-20 17:09:04,345 INFO inference run_from_config start for inference, keys=['output_csv']
2025-08-20 17:09:04,345 ERROR inference missing config key: input_csv
2025-08-20 17:09:04,345 INFO inference run_from_config start for inference, keys=['input_csv']
2025-08-20 17:09:04,345 ERROR inference missing config key: output_csv
2025-08-20 17:09:04,346 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv']
2025-08-20 17:09:04,347 ERROR inference input CSV missing required 'prompt' column
2025-08-20 17:09:04,348 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv']
2025-08-20 17:09:04,348 ERROR inference failed to load input_csv: File not found
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/inference.py", line 42, in run_from_config
    df = load_csv(input_csv)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1173, in _execute_mock_call
    raise effect
Exception: File not found
2025-08-20 17:09:04,351 INFO inference run_from_config start for inference, keys=['input_csv', 'output_csv', 'default_model', 'model_config', 'api_params']
2025-08-20 17:09:04,352 INFO inference generated for model=Llama:8B prompt_len=12
2025-08-20 17:09:04,353 INFO src.io Wrote dataframe to /tmp/tmp6ay0jiq_.csv (rows=1)
2025-08-20 17:09:04,353 INFO inference run_from_config inference finished, wrote /tmp/tmp6ay0jiq_.csv (rows=1) duration=0.00s
2025-08-20 17:09:04,918 WARNING judge template_dir does not exist: nonexistent_dir
2025-08-20 17:09:04,921 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 17:09:04,923 INFO judge run_from_config judge finished, wrote ratings.csv (rows=3) duration=0.00s
2025-08-20 17:09:04,925 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 17:09:04,927 INFO judge run_from_config judge finished, wrote ratings.csv (rows=1) duration=0.00s
2025-08-20 17:09:04,927 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 17:09:04,927 INFO judge run_from_config judge finished, wrote ratings.csv (rows=1) duration=0.00s
2025-08-20 17:09:04,929 INFO judge run_from_config start for judge, keys=['output_csv', 'template_dir']
2025-08-20 17:09:04,929 ERROR judge missing config key: input_csv
2025-08-20 17:09:04,929 INFO judge run_from_config start for judge, keys=['input_csv', 'template_dir']
2025-08-20 17:09:04,929 ERROR judge missing config key: output_csv
2025-08-20 17:09:04,929 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv']
2025-08-20 17:09:04,929 ERROR judge missing config key: template_dir
2025-08-20 17:09:04,932 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 17:09:04,932 ERROR judge failed to load input_csv: File not found
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/judge.py", line 83, in run_from_config
    df = load_csv(input_csv)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1114, in __call__
    return self._mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1118, in _mock_call
    return self._execute_mock_call(*args, **kwargs)
  File "/home/johnro/miniconda3/envs/research-env/lib/python3.10/unittest/mock.py", line 1173, in _execute_mock_call
    raise effect
Exception: File not found
2025-08-20 17:09:04,938 INFO judge run_from_config start for judge, keys=['input_csv', 'output_csv', 'template_dir']
2025-08-20 17:09:04,940 INFO src.io Wrote dataframe to /tmp/tmpawmf_hwy.csv (rows=1)
2025-08-20 17:09:04,940 INFO judge run_from_config judge finished, wrote /tmp/tmpawmf_hwy.csv (rows=1) duration=0.00s
2025-08-20 17:09:13,285 ERROR visualization missing config key: input_csv
2025-08-20 17:09:13,285 ERROR visualization missing config key: output_dir
2025-08-20 17:09:13,286 ERROR visualization missing config key: visualization_type
2025-08-20 17:09:13,287 INFO visualization loading input_csv: nonexistent.csv
2025-08-20 17:09:13,287 ERROR visualization failed to load input_csv: File not found
2025-08-20 17:09:13,289 INFO visualization loading input_csv: input.csv
2025-08-20 17:09:13,289 ERROR visualization input data is empty
2025-08-20 17:09:13,292 INFO visualization loading input_csv: input.csv
2025-08-20 17:09:13,292 INFO visualization creating comparative visualization for score
2025-08-20 17:09:13,295 INFO visualization loading input_csv: input.csv
2025-08-20 17:09:13,295 INFO visualization creating statistical visualization for score
2025-08-20 17:09:13,298 INFO visualization loading input_csv: input.csv
2025-08-20 17:09:13,299 INFO visualization creating radar chart
2025-08-20 17:09:13,301 INFO visualization loading input_csv: input.csv
2025-08-20 17:09:13,302 ERROR visualization unknown visualization_type: unknown
2025-08-20 17:09:13,304 INFO visualization loading input_csv: input.csv
2025-08-20 17:09:13,305 ERROR visualization metric_name is required for comparative visualization
2025-08-20 17:09:13,305 INFO visualization loading input_csv: input.csv
2025-08-20 17:09:13,305 ERROR visualization metric_name is required for statistical visualization
2025-08-20 17:09:13,307 INFO visualization loading input_csv: input.csv
2025-08-20 17:09:13,307 ERROR visualization no numeric columns found for radar chart
2025-08-20 17:09:13,599 INFO analytics loading input_csv: ratings.csv
2025-08-20 17:09:13,599 INFO analytics aggregating data by ['model']
2025-08-20 17:09:13,601 WARNING analytics Using fallback aggregation: 'Series' object has no attribute 'columns'
2025-08-20 17:09:13,602 INFO analytics writing output to analysis.csv
2025-08-20 17:09:13,605 INFO analytics loading input_csv: ratings.csv
2025-08-20 17:09:13,605 INFO analytics aggregating data by ['model']
2025-08-20 17:09:13,607 WARNING analytics Using fallback aggregation: 'Series' object has no attribute 'columns'
2025-08-20 17:09:13,608 INFO analytics writing output to analysis.csv
2025-08-20 17:09:13,609 ERROR analytics missing config key: input_csv
2025-08-20 17:09:13,609 ERROR analytics missing config key: output_csv
2025-08-20 17:09:13,609 ERROR analytics missing config key: group_by
2025-08-20 17:09:13,611 INFO analytics loading input_csv: input.csv
2025-08-20 17:09:13,611 ERROR analytics input data is empty
2025-08-20 17:09:13,612 INFO analytics loading input_csv: nonexistent.csv
2025-08-20 17:09:13,612 ERROR analytics failed to load input_csv: File not found
2025-08-20 17:09:17,422 ERROR visualization missing config key: input_csv
2025-08-20 17:09:17,422 ERROR visualization missing config key: output_dir
2025-08-20 17:09:17,422 ERROR visualization missing config key: visualization_type
2025-08-20 17:09:17,423 INFO visualization loading input_csv: nonexistent.csv
2025-08-20 17:09:17,424 ERROR visualization failed to load input_csv: File not found
2025-08-20 17:09:17,425 INFO visualization loading input_csv: input.csv
2025-08-20 17:09:17,425 ERROR visualization input data is empty
2025-08-20 17:09:17,427 INFO visualization loading input_csv: input.csv
2025-08-20 17:09:17,428 INFO visualization creating comparative visualization for score
2025-08-20 17:09:17,431 INFO visualization loading input_csv: input.csv
2025-08-20 17:09:17,431 INFO visualization creating statistical visualization for score
2025-08-20 17:09:17,434 INFO visualization loading input_csv: input.csv
2025-08-20 17:09:17,435 INFO visualization creating radar chart
2025-08-20 17:09:17,437 INFO visualization loading input_csv: input.csv
2025-08-20 17:09:17,437 ERROR visualization unknown visualization_type: unknown
2025-08-20 17:09:17,439 INFO visualization loading input_csv: input.csv
2025-08-20 17:09:17,439 ERROR visualization metric_name is required for comparative visualization
2025-08-20 17:09:17,439 INFO visualization loading input_csv: input.csv
2025-08-20 17:09:17,440 ERROR visualization metric_name is required for statistical visualization
2025-08-20 17:09:17,441 INFO visualization loading input_csv: input.csv
2025-08-20 17:09:17,442 ERROR visualization no numeric columns found for radar chart
2025-08-20 17:09:17,855 INFO visualization loading input_csv: /tmp/tmpmwzvjftr.csv
2025-08-20 17:09:17,858 INFO visualization creating comparative visualization for score
2025-08-20 17:09:18,092 INFO visualization loading input_csv: /tmp/tmpmwzvjftr.csv
2025-08-20 17:09:18,094 INFO visualization creating radar chart
2025-08-20 19:33:54,387 INFO analyst run_from_config start for analyst with cfg keys=['input_csv', 'output_csv', 'group_col', 'rating_col']
2025-08-20 19:33:54,387 ERROR analyst run_from_config failed: CSV not found: gnosis/input/full_log_sop_stable.cleaned.csv
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/analyst.py", line 54, in run_from_config
    df = load_csv(input_csv)
  File "/home/johnro/sop-research/gnosis/src/io.py", line 17, in load_csv
    raise FileNotFoundError(f"CSV not found: {path}")
FileNotFoundError: CSV not found: gnosis/input/full_log_sop_stable.cleaned.csv
2025-08-20 19:35:59,485 INFO analyst run_from_config start for analyst with cfg keys=['input_csv', 'output_csv', 'group_col', 'rating_col']
2025-08-20 19:35:59,485 ERROR analyst run_from_config failed: CSV not found: gnosis/input/full_log_sop_stable.cleaned.csv
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/analyst.py", line 54, in run_from_config
    df = load_csv(input_csv)
  File "/home/johnro/sop-research/gnosis/src/io.py", line 17, in load_csv
    raise FileNotFoundError(f"CSV not found: {path}")
FileNotFoundError: CSV not found: gnosis/input/full_log_sop_stable.cleaned.csv
2025-08-20 19:43:02,344 INFO analyst run_from_config start for analyst with cfg keys=['input_csv', 'output_csv', 'group_col', 'rating_col']
2025-08-20 19:43:02,508 INFO analyst analyze_df start group_col=synthesis_id rating_col=score_faithfulness_rating
2025-08-20 19:43:02,514 INFO analyst analyze_df done (groups=82)
2025-08-20 19:43:02,515 INFO src.io Wrote dataframe to output/analysis_summary_faithfulness.csv (rows=82)
2025-08-20 19:43:02,515 INFO analyst run_from_config finished successfully, wrote output/analysis_summary_faithfulness.csv
2025-08-20 19:43:02,520 INFO src.io Wrote provenance for output/analysis_summary_faithfulness.csv -> output/analysis_summary_faithfulness.csv.meta.json
2025-08-20 19:43:02,520 INFO analyst run_from_config start for analyst with cfg keys=['input_csv', 'output_csv', 'group_col', 'rating_col']
2025-08-20 19:43:02,661 INFO analyst analyze_df start group_col=synthesis_id rating_col=score_correctness_rating
2025-08-20 19:43:02,663 INFO analyst analyze_df done (groups=82)
2025-08-20 19:43:02,663 INFO src.io Wrote dataframe to output/analysis_summary_correctness.csv (rows=82)
2025-08-20 19:43:02,663 INFO analyst run_from_config finished successfully, wrote output/analysis_summary_correctness.csv
2025-08-20 19:43:02,666 INFO src.io Wrote provenance for output/analysis_summary_correctness.csv -> output/analysis_summary_correctness.csv.meta.json
2025-08-20 19:43:02,666 INFO analyst run_from_config start for analyst with cfg keys=['input_csv', 'output_csv', 'group_col', 'rating_col']
2025-08-20 19:43:02,804 INFO analyst analyze_df start group_col=synthesis_id rating_col=score_completeness_rating
2025-08-20 19:43:02,806 INFO analyst analyze_df done (groups=82)
2025-08-20 19:43:02,806 INFO src.io Wrote dataframe to output/analysis_summary_completeness.csv (rows=82)
2025-08-20 19:43:02,807 INFO analyst run_from_config finished successfully, wrote output/analysis_summary_completeness.csv
2025-08-20 19:43:02,809 INFO src.io Wrote provenance for output/analysis_summary_completeness.csv -> output/analysis_summary_completeness.csv.meta.json
2025-08-20 19:43:02,809 INFO analyst run_from_config start for analyst with cfg keys=['input_csv', 'output_csv', 'group_col', 'rating_col']
2025-08-20 19:43:02,952 INFO analyst analyze_df start group_col=synthesis_id rating_col=score_clarity_rating
2025-08-20 19:43:02,954 INFO analyst analyze_df done (groups=82)
2025-08-20 19:43:02,955 INFO src.io Wrote dataframe to output/analysis_summary_clarity.csv (rows=82)
2025-08-20 19:43:02,955 INFO analyst run_from_config finished successfully, wrote output/analysis_summary_clarity.csv
2025-08-20 19:43:02,958 INFO src.io Wrote provenance for output/analysis_summary_clarity.csv -> output/analysis_summary_clarity.csv.meta.json
2025-08-20 19:55:21,829 INFO analyst run_from_config start for analyst with cfg keys=['input_csv', 'output_csv', 'group_col', 'rating_col']
2025-08-20 19:55:21,983 INFO analyst analyze_df start group_col=synthesis_id, ablation_condition rating_col=score_faithfulness_rating
2025-08-20 19:55:21,983 ERROR analyst group_col 'synthesis_id, ablation_condition' not found in DataFrame columns
2025-08-20 19:55:21,983 ERROR analyst run_from_config failed: group_col 'synthesis_id, ablation_condition' not found in DataFrame columns
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/analyst.py", line 55, in run_from_config
    out_df = analyze_df(df, group_col=group_col, rating_col=rating_col)
  File "/home/johnro/sop-research/gnosis/src/analyst.py", line 21, in analyze_df
    raise ValueError(f"group_col '{group_col}' not found in DataFrame columns")
ValueError: group_col 'synthesis_id, ablation_condition' not found in DataFrame columns
2025-08-20 19:56:35,989 INFO analyst run_from_config start for analyst with cfg keys=['input_csv', 'output_csv', 'group_col', 'rating_col']
2025-08-20 19:56:36,131 INFO analyst analyze_df start group_col=['synthesis_id', 'ablation_condition'] rating_col=score_faithfulness_rating
2025-08-20 19:56:36,131 ERROR analyst run_from_config failed: unhashable type: 'list'
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/analyst.py", line 55, in run_from_config
    out_df = analyze_df(df, group_col=group_col, rating_col=rating_col)
  File "/home/johnro/sop-research/gnosis/src/analyst.py", line 19, in analyze_df
    if group_col not in df.columns:
  File "/home/johnro/sop-research/gnosis/.venv/lib/python3.10/site-packages/pandas/core/indexes/base.py", line 5370, in __contains__
    hash(key)
TypeError: unhashable type: 'list'
2025-08-20 19:58:55,367 INFO analyst run_from_config start for analyst with cfg keys=['input_csv', 'output_csv', 'group_col', 'rating_col']
2025-08-20 19:58:55,526 INFO analyst analyze_df start group_col=['synthesis_id', 'ablation_condition'] rating_col=score_faithfulness_rating
2025-08-20 19:58:55,526 ERROR analyst run_from_config failed: unhashable type: 'list'
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/analyst.py", line 55, in run_from_config
    out_df = analyze_df(df, group_col=group_col, rating_col=rating_col)
  File "/home/johnro/sop-research/gnosis/src/analyst.py", line 19, in analyze_df
    if group_col not in df.columns:
  File "/home/johnro/sop-research/gnosis/.venv/lib/python3.10/site-packages/pandas/core/indexes/base.py", line 5370, in __contains__
    hash(key)
TypeError: unhashable type: 'list'
2025-08-20 19:59:06,281 INFO analyst run_from_config start for analyst with cfg keys=['input_csv', 'output_csv', 'group_col', 'rating_col']
2025-08-20 19:59:06,445 INFO analyst analyze_df start group_col=['synthesis_id', 'ablation_condition'] rating_col=score_faithfulness_rating
2025-08-20 19:59:06,445 ERROR analyst run_from_config failed: unhashable type: 'list'
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/analyst.py", line 55, in run_from_config
    out_df = analyze_df(df, group_col=group_col, rating_col=rating_col)
  File "/home/johnro/sop-research/gnosis/src/analyst.py", line 19, in analyze_df
    if group_col not in df.columns:
  File "/home/johnro/sop-research/gnosis/.venv/lib/python3.10/site-packages/pandas/core/indexes/base.py", line 5370, in __contains__
    hash(key)
TypeError: unhashable type: 'list'
2025-08-20 20:01:13,036 INFO analyst run_from_config start for analyst with cfg keys=['input_csv', 'output_csv', 'group_col', 'rating_col']
2025-08-20 20:01:13,181 INFO analyst analyze_df start group_col=['synthesis_id', 'ablation_condition'] rating_col=score_faithfulness_rating
2025-08-20 20:01:13,181 ERROR analyst run_from_config failed: unhashable type: 'list'
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/analyst.py", line 55, in run_from_config
    out_df = analyze_df(df, group_col=group_col, rating_col=rating_col)
  File "/home/johnro/sop-research/gnosis/src/analyst.py", line 19, in analyze_df
    if group_col not in df.columns:
  File "/home/johnro/sop-research/gnosis/.venv/lib/python3.10/site-packages/pandas/core/indexes/base.py", line 5370, in __contains__
    hash(key)
TypeError: unhashable type: 'list'
2025-08-20 20:03:21,021 INFO analyst run_from_config start for analyst with cfg keys=['input_csv', 'output_csv', 'group_col', 'rating_col']
2025-08-20 20:03:21,167 INFO analyst analyze_df start group_col=['synthesis_id', 'ablation_condition'] rating_col=score_faithfulness_rating
2025-08-20 20:03:21,172 INFO analyst analyze_df done (groups=328)
2025-08-20 20:03:21,173 INFO src.io Wrote dataframe to output/analysis_summary_faithfulness.csv (rows=328)
2025-08-20 20:03:21,173 INFO analyst run_from_config finished successfully, wrote output/analysis_summary_faithfulness.csv
2025-08-20 20:03:21,177 INFO src.io Wrote provenance for output/analysis_summary_faithfulness.csv -> output/analysis_summary_faithfulness.csv.meta.json
2025-08-20 20:03:21,177 INFO analyst run_from_config start for analyst with cfg keys=['input_csv', 'output_csv', 'group_col', 'rating_col']
2025-08-20 20:03:21,321 INFO analyst analyze_df start group_col=['synthesis_id', 'ablation_condition'] rating_col=score_correctness_rating
2025-08-20 20:03:21,323 INFO analyst analyze_df done (groups=328)
2025-08-20 20:03:21,324 INFO src.io Wrote dataframe to output/analysis_summary_correctness.csv (rows=328)
2025-08-20 20:03:21,325 INFO analyst run_from_config finished successfully, wrote output/analysis_summary_correctness.csv
2025-08-20 20:03:21,328 INFO src.io Wrote provenance for output/analysis_summary_correctness.csv -> output/analysis_summary_correctness.csv.meta.json
2025-08-20 20:03:21,328 INFO analyst run_from_config start for analyst with cfg keys=['input_csv', 'output_csv', 'group_col', 'rating_col']
2025-08-20 20:03:21,470 INFO analyst analyze_df start group_col=['synthesis_id', 'ablation_condition'] rating_col=score_completeness_rating
2025-08-20 20:03:21,473 INFO analyst analyze_df done (groups=328)
2025-08-20 20:03:21,474 INFO src.io Wrote dataframe to output/analysis_summary_completeness.csv (rows=328)
2025-08-20 20:03:21,474 INFO analyst run_from_config finished successfully, wrote output/analysis_summary_completeness.csv
2025-08-20 20:03:21,477 INFO src.io Wrote provenance for output/analysis_summary_completeness.csv -> output/analysis_summary_completeness.csv.meta.json
2025-08-20 20:03:21,477 INFO analyst run_from_config start for analyst with cfg keys=['input_csv', 'output_csv', 'group_col', 'rating_col']
2025-08-20 20:03:21,615 INFO analyst analyze_df start group_col=['synthesis_id', 'ablation_condition'] rating_col=score_clarity_rating
2025-08-20 20:03:21,617 INFO analyst analyze_df done (groups=328)
2025-08-20 20:03:21,618 INFO src.io Wrote dataframe to output/analysis_summary_clarity.csv (rows=328)
2025-08-20 20:03:21,618 INFO analyst run_from_config finished successfully, wrote output/analysis_summary_clarity.csv
2025-08-20 20:03:21,621 INFO src.io Wrote provenance for output/analysis_summary_clarity.csv -> output/analysis_summary_clarity.csv.meta.json
2025-08-20 20:03:46,461 INFO analyst run_from_config start for analyst with cfg keys=['input_csv', 'output_csv', 'group_col', 'rating_col']
2025-08-20 20:03:46,602 INFO analyst analyze_df start group_col=['synthesis_id', 'ablation_condition'] rating_col=score_faithfulness_rating
2025-08-20 20:03:46,604 INFO analyst analyze_df done (groups=328)
2025-08-20 20:03:46,605 INFO src.io Wrote dataframe to output/analysis_summary_faithfulness.csv (rows=328)
2025-08-20 20:03:46,605 INFO analyst run_from_config finished successfully, wrote output/analysis_summary_faithfulness.csv
2025-08-20 20:03:46,608 INFO src.io Wrote provenance for output/analysis_summary_faithfulness.csv -> output/analysis_summary_faithfulness.csv.meta.json
2025-08-20 20:03:46,608 INFO analyst run_from_config start for analyst with cfg keys=['input_csv', 'output_csv', 'group_col', 'rating_col']
2025-08-20 20:03:46,743 INFO analyst analyze_df start group_col=['synthesis_id', 'ablation_condition'] rating_col=score_correctness_rating
2025-08-20 20:03:46,745 INFO analyst analyze_df done (groups=328)
2025-08-20 20:03:46,746 INFO src.io Wrote dataframe to output/analysis_summary_correctness.csv (rows=328)
2025-08-20 20:03:46,746 INFO analyst run_from_config finished successfully, wrote output/analysis_summary_correctness.csv
2025-08-20 20:03:46,748 INFO src.io Wrote provenance for output/analysis_summary_correctness.csv -> output/analysis_summary_correctness.csv.meta.json
2025-08-20 20:03:46,748 INFO analyst run_from_config start for analyst with cfg keys=['input_csv', 'output_csv', 'group_col', 'rating_col']
2025-08-20 20:03:46,881 INFO analyst analyze_df start group_col=['synthesis_id', 'ablation_condition'] rating_col=score_completeness_rating
2025-08-20 20:03:46,884 INFO analyst analyze_df done (groups=328)
2025-08-20 20:03:46,885 INFO src.io Wrote dataframe to output/analysis_summary_completeness.csv (rows=328)
2025-08-20 20:03:46,885 INFO analyst run_from_config finished successfully, wrote output/analysis_summary_completeness.csv
2025-08-20 20:03:46,888 INFO src.io Wrote provenance for output/analysis_summary_completeness.csv -> output/analysis_summary_completeness.csv.meta.json
2025-08-20 20:03:46,888 INFO analyst run_from_config start for analyst with cfg keys=['input_csv', 'output_csv', 'group_col', 'rating_col']
2025-08-20 20:03:47,034 INFO analyst analyze_df start group_col=['synthesis_id', 'ablation_condition'] rating_col=score_clarity_rating
2025-08-20 20:03:47,037 INFO analyst analyze_df done (groups=328)
2025-08-20 20:03:47,038 INFO src.io Wrote dataframe to output/analysis_summary_clarity.csv (rows=328)
2025-08-20 20:03:47,038 INFO analyst run_from_config finished successfully, wrote output/analysis_summary_clarity.csv
2025-08-20 20:03:47,042 INFO src.io Wrote provenance for output/analysis_summary_clarity.csv -> output/analysis_summary_clarity.csv.meta.json
2025-08-20 20:06:34,295 INFO analyst run_from_config start for analyst with cfg keys=['input_csv', 'output_csv', 'group_col', 'rating_col']
2025-08-20 20:06:34,453 INFO analyst analyze_df start group_col=['synthesis_id', 'ablation_condition'] rating_col=score_faithfulness_rating
2025-08-20 20:06:34,455 INFO analyst analyze_df done (groups=328)
2025-08-20 20:06:34,456 INFO src.io Wrote dataframe to output/analysis_summary_faithfulness.csv (rows=328)
2025-08-20 20:06:34,456 INFO analyst run_from_config finished successfully, wrote output/analysis_summary_faithfulness.csv
2025-08-20 20:06:34,459 INFO src.io Wrote provenance for output/analysis_summary_faithfulness.csv -> output/analysis_summary_faithfulness.csv.meta.json
2025-08-20 20:06:34,459 INFO analyst run_from_config start for analyst with cfg keys=['input_csv', 'output_csv', 'group_col', 'rating_col']
2025-08-20 20:06:34,602 INFO analyst analyze_df start group_col=['synthesis_id', 'ablation_condition'] rating_col=score_correctness_rating
2025-08-20 20:06:34,604 INFO analyst analyze_df done (groups=328)
2025-08-20 20:06:34,605 INFO src.io Wrote dataframe to output/analysis_summary_correctness.csv (rows=328)
2025-08-20 20:06:34,605 INFO analyst run_from_config finished successfully, wrote output/analysis_summary_correctness.csv
2025-08-20 20:06:34,608 INFO src.io Wrote provenance for output/analysis_summary_correctness.csv -> output/analysis_summary_correctness.csv.meta.json
2025-08-20 20:06:34,608 INFO analyst run_from_config start for analyst with cfg keys=['input_csv', 'output_csv', 'group_col', 'rating_col']
2025-08-20 20:06:34,740 INFO analyst analyze_df start group_col=['synthesis_id', 'ablation_condition'] rating_col=score_completeness_rating
2025-08-20 20:06:34,743 INFO analyst analyze_df done (groups=328)
2025-08-20 20:06:34,744 INFO src.io Wrote dataframe to output/analysis_summary_completeness.csv (rows=328)
2025-08-20 20:06:34,744 INFO analyst run_from_config finished successfully, wrote output/analysis_summary_completeness.csv
2025-08-20 20:06:34,747 INFO src.io Wrote provenance for output/analysis_summary_completeness.csv -> output/analysis_summary_completeness.csv.meta.json
2025-08-20 20:06:34,747 INFO analyst run_from_config start for analyst with cfg keys=['input_csv', 'output_csv', 'group_col', 'rating_col']
2025-08-20 20:06:34,880 INFO analyst analyze_df start group_col=['synthesis_id', 'ablation_condition'] rating_col=score_clarity_rating
2025-08-20 20:06:34,883 INFO analyst analyze_df done (groups=328)
2025-08-20 20:06:34,884 INFO src.io Wrote dataframe to output/analysis_summary_clarity.csv (rows=328)
2025-08-20 20:06:34,884 INFO analyst run_from_config finished successfully, wrote output/analysis_summary_clarity.csv
2025-08-20 20:06:34,887 INFO src.io Wrote provenance for output/analysis_summary_clarity.csv -> output/analysis_summary_clarity.csv.meta.json
2025-08-20 20:06:34,887 INFO analyst run_from_config start for analyst with cfg keys=['input_csv', 'output_csv', 'group_col', 'rating_col', 'metrics']
2025-08-20 20:06:35,023 INFO analyst analyze_df start group_col=['synthesis_id', 'ablation_condition'] rating_col=raw_model_output
2025-08-20 20:06:35,026 ERROR analyst run_from_config failed: agg function failed [how->mean,dtype->object]
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/.venv/lib/python3.10/site-packages/pandas/core/groupby/groupby.py", line 1943, in _agg_py_fallback
    res_values = self._grouper.agg_series(ser, alt, preserve_dtype=True)
  File "/home/johnro/sop-research/gnosis/.venv/lib/python3.10/site-packages/pandas/core/groupby/ops.py", line 864, in agg_series
    result = self._aggregate_series_pure_python(obj, func)
  File "/home/johnro/sop-research/gnosis/.venv/lib/python3.10/site-packages/pandas/core/groupby/ops.py", line 885, in _aggregate_series_pure_python
    res = func(group)
  File "/home/johnro/sop-research/gnosis/.venv/lib/python3.10/site-packages/pandas/core/groupby/groupby.py", line 2460, in <lambda>
    alt=lambda x: Series(x, copy=False).mean(numeric_only=numeric_only),
  File "/home/johnro/sop-research/gnosis/.venv/lib/python3.10/site-packages/pandas/core/series.py", line 6560, in mean
    return NDFrame.mean(self, axis, skipna, numeric_only, **kwargs)
  File "/home/johnro/sop-research/gnosis/.venv/lib/python3.10/site-packages/pandas/core/generic.py", line 12439, in mean
    return self._stat_function(
  File "/home/johnro/sop-research/gnosis/.venv/lib/python3.10/site-packages/pandas/core/generic.py", line 12396, in _stat_function
    return self._reduce(
  File "/home/johnro/sop-research/gnosis/.venv/lib/python3.10/site-packages/pandas/core/series.py", line 6468, in _reduce
    return op(delegate, skipna=skipna, **kwds)
  File "/home/johnro/sop-research/gnosis/.venv/lib/python3.10/site-packages/pandas/core/nanops.py", line 147, in f
    result = alt(values, axis=axis, skipna=skipna, **kwds)
  File "/home/johnro/sop-research/gnosis/.venv/lib/python3.10/site-packages/pandas/core/nanops.py", line 404, in new_func
    result = func(values, axis=axis, skipna=skipna, mask=mask, **kwargs)
  File "/home/johnro/sop-research/gnosis/.venv/lib/python3.10/site-packages/pandas/core/nanops.py", line 720, in nanmean
    the_sum = _ensure_numeric(the_sum)
  File "/home/johnro/sop-research/gnosis/.venv/lib/python3.10/site-packages/pandas/core/nanops.py", line 1701, in _ensure_numeric
    raise TypeError(f"Could not convert string '{x}' to numeric")
TypeError: Could not convert string 'The self-attention mechanism, introduced in the "Attention Is All You Need" paper, provides a crucial architectural foundation for the in-context few-shot learning capability demonstrated in the GPT-3 paper.

In the context of transformer models like GPT-3, self-attention allows the model to attend to different parts of the input sequence simultaneously and weigh their importance. This mechanism enables the model to capture long-range dependencies and contextual relationships within the input data.

For few-shot learning, the key insight is that the self-attention mechanism can be used to selectively focus on specific parts of the input data that are relevant for a given task or prompt. In other words, the model can attend to the most informative regions of the input sequence that are related to the target task, and ignore irrelevant information.

In the GPT-3 paper, this is achieved by using the self-attention mechanism in combination with a few-shot learning approach called "in-context learning". The idea is to fine-tune the pre-trained GPT-3 model on a small number of examples (typically 1-5) that are relevant to the target task. During this fine-tuning process, the self-attention mechanism allows the model to selectively focus on the most informative regions of the input sequence that are related to the target task.

By leveraging the self-attention mechanism in this way, GPT-3 can learn to adapt to new tasks and prompts with only a few examples, demonstrating impressive few-shot learning capabilities. In essence, the self-attention mechanism enables the model to selectively attend to relevant information and ignore irrelevant information, which is critical for effective few-shot learning.

In summary, the self-attention mechanism provides the architectural foundation for GPT-3's in-context few-shot learning capability by enabling the model to selectively focus on relevant regions of the input sequence, allowing it to adapt to new tasks and prompts with only a few examples.The self-attention mechanism, introduced in the "Attention Is All You Need" paper, provides a crucial architectural foundation for the in-context few-shot learning capability demonstrated in the GPT-3 paper.

In the context of transformer models like GPT-3, self-attention allows the model to attend to different parts of the input sequence simultaneously and weigh their importance. This mechanism enables the model to capture long-range dependencies and contextual relationships within the input data.

For few-shot learning, the key insight is that the self-attention mechanism can be used to selectively focus on specific parts of the input data that are relevant for a given task or prompt. In other words, the model can attend to the most informative regions of the input sequence that are related to the target task, and ignore irrelevant information.

In the GPT-3 paper, this is achieved by using the self-attention mechanism in combination with a few-shot learning approach called "in-context learning". The idea is to fine-tune the pre-trained GPT-3 model on a small number of examples (typically 1-5) that are relevant to the target task. During this fine-tuning process, the self-attention mechanism allows the model to selectively focus on the most informative regions of the input sequence that are related to the target task.

By leveraging the self-attention mechanism in this way, GPT-3 can learn to adapt to new tasks and prompts with only a few examples, demonstrating impressive few-shot learning capabilities. In essence, the self-attention mechanism enables the model to selectively attend to relevant information and ignore irrelevant information, which is critical for effective few-shot learning.

In summary, the self-attention mechanism provides the architectural foundation for GPT-3's in-context few-shot learning capability by enabling the model to selectively focus on relevant regions of the input sequence, allowing it to adapt to new tasks and prompts with only a few examples.The self-attention mechanism, introduced in the "Attention Is All You Need" paper, provides a crucial architectural foundation for the in-context few-shot learning capability demonstrated in the GPT-3 paper.

In the context of transformer models like GPT-3, self-attention allows the model to attend to different parts of the input sequence simultaneously and weigh their importance. This mechanism enables the model to capture long-range dependencies and contextual relationships within the input data.

For few-shot learning, the key insight is that the self-attention mechanism can be used to selectively focus on specific parts of the input data that are relevant for a given task or prompt. In other words, the model can attend to the most informative regions of the input sequence that are related to the target task, and ignore irrelevant information.

In the GPT-3 paper, this is achieved by using the self-attention mechanism in combination with a few-shot learning approach called "in-context learning". The idea is to fine-tune the pre-trained GPT-3 model on a small number of examples (typically 1-5) that are relevant to the target task. During this fine-tuning process, the self-attention mechanism allows the model to selectively focus on the most informative regions of the input sequence that are related to the target task.

By leveraging the self-attention mechanism in this way, GPT-3 can learn to adapt to new tasks and prompts with only a few examples, demonstrating impressive few-shot learning capabilities. In essence, the self-attention mechanism enables the model to selectively attend to relevant information and ignore irrelevant information, which is critical for effective few-shot learning.

In summary, the self-attention mechanism provides the architectural foundation for GPT-3's in-context few-shot learning capability by enabling the model to selectively focus on relevant regions of the input sequence, allowing it to adapt to new tasks and prompts with only a few examples.The self-attention mechanism, introduced in the "Attention Is All You Need" paper, provides a crucial architectural foundation for the in-context few-shot learning capability demonstrated in the GPT-3 paper.

In the context of transformer models like GPT-3, self-attention allows the model to attend to different parts of the input sequence simultaneously and weigh their importance. This mechanism enables the model to capture long-range dependencies and contextual relationships within the input data.

For few-shot learning, the key insight is that the self-attention mechanism can be used to selectively focus on specific parts of the input data that are relevant for a given task or prompt. In other words, the model can attend to the most informative regions of the input sequence that are related to the target task, and ignore irrelevant information.

In the GPT-3 paper, this is achieved by using the self-attention mechanism in combination with a few-shot learning approach called "in-context learning". The idea is to fine-tune the pre-trained GPT-3 model on a small number of examples (typically 1-5) that are relevant to the target task. During this fine-tuning process, the self-attention mechanism allows the model to selectively focus on the most informative regions of the input sequence that are related to the target task.

By leveraging the self-attention mechanism in this way, GPT-3 can learn to adapt to new tasks and prompts with only a few examples, demonstrating impressive few-shot learning capabilities. In essence, the self-attention mechanism enables the model to selectively attend to relevant information and ignore irrelevant information, which is critical for effective few-shot learning.

In summary, the self-attention mechanism provides the architectural foundation for GPT-3's in-context few-shot learning capability by enabling the model to selectively focus on relevant regions of the input sequence, allowing it to adapt to new tasks and prompts with only a few examples.The self-attention mechanism, introduced in the "Attention Is All You Need" paper, provides a crucial architectural foundation for the in-context few-shot learning capability demonstrated in the GPT-3 paper.

In the context of transformer models like GPT-3, self-attention allows the model to attend to different parts of the input sequence simultaneously and weigh their importance. This mechanism enables the model to capture long-range dependencies and contextual relationships within the input data.

For few-shot learning, the key insight is that the self-attention mechanism can be used to selectively focus on specific parts of the input data that are relevant for a given task or prompt. In other words, the model can attend to the most informative regions of the input sequence that are related to the target task, and ignore irrelevant information.

In the GPT-3 paper, this is achieved by using the self-attention mechanism in combination with a few-shot learning approach called "in-context learning". The idea is to fine-tune the pre-trained GPT-3 model on a small number of examples (typically 1-5) that are relevant to the target task. During this fine-tuning process, the self-attention mechanism allows the model to selectively focus on the most informative regions of the input sequence that are related to the target task.

By leveraging the self-attention mechanism in this way, GPT-3 can learn to adapt to new tasks and prompts with only a few examples, demonstrating impressive few-shot learning capabilities. In essence, the self-attention mechanism enables the model to selectively attend to relevant information and ignore irrelevant information, which is critical for effective few-shot learning.

In summary, the self-attention mechanism provides the architectural foundation for GPT-3's in-context few-shot learning capability by enabling the model to selectively focus on relevant regions of the input sequence, allowing it to adapt to new tasks and prompts with only a few examples.The self-attention mechanism, introduced in the "Attention Is All You Need" paper, provides a crucial architectural foundation for the in-context few-shot learning capability demonstrated in the GPT-3 paper.

In the context of transformer models like GPT-3, self-attention allows the model to attend to different parts of the input sequence simultaneously and weigh their importance. This mechanism enables the model to capture long-range dependencies and contextual relationships within the input data.

For few-shot learning, the key insight is that the self-attention mechanism can be used to selectively focus on specific parts of the input data that are relevant for a given task or prompt. In other words, the model can attend to the most informative regions of the input sequence that are related to the target task, and ignore irrelevant information.

In the GPT-3 paper, this is achieved by using the self-attention mechanism in combination with a few-shot learning approach called "in-context learning". The idea is to fine-tune the pre-trained GPT-3 model on a small number of examples (typically 1-5) that are relevant to the target task. During this fine-tuning process, the self-attention mechanism allows the model to selectively focus on the most informative regions of the input sequence that are related to the target task.

By leveraging the self-attention mechanism in this way, GPT-3 can learn to adapt to new tasks and prompts with only a few examples, demonstrating impressive few-shot learning capabilities. In essence, the self-attention mechanism enables the model to selectively attend to relevant information and ignore irrelevant information, which is critical for effective few-shot learning.

In summary, the self-attention mechanism provides the architectural foundation for GPT-3's in-context few-shot learning capability by enabling the model to selectively focus on relevant regions of the input sequence, allowing it to adapt to new tasks and prompts with only a few examples.The self-attention mechanism, introduced in the "Attention Is All You Need" paper, provides a crucial architectural foundation for the in-context few-shot learning capability demonstrated in the GPT-3 paper.

In the context of transformer models like GPT-3, self-attention allows the model to attend to different parts of the input sequence simultaneously and weigh their importance. This mechanism enables the model to capture long-range dependencies and contextual relationships within the input data.

For few-shot learning, the key insight is that the self-attention mechanism can be used to selectively focus on specific parts of the input data that are relevant for a given task or prompt. In other words, the model can attend to the most informative regions of the input sequence that are related to the target task, and ignore irrelevant information.

In the GPT-3 paper, this is achieved by using the self-attention mechanism in combination with a few-shot learning approach called "in-context learning". The idea is to fine-tune the pre-trained GPT-3 model on a small number of examples (typically 1-5) that are relevant to the target task. During this fine-tuning process, the self-attention mechanism allows the model to selectively focus on the most informative regions of the input sequence that are related to the target task.

By leveraging the self-attention mechanism in this way, GPT-3 can learn to adapt to new tasks and prompts with only a few examples, demonstrating impressive few-shot learning capabilities. In essence, the self-attention mechanism enables the model to selectively attend to relevant information and ignore irrelevant information, which is critical for effective few-shot learning.

In summary, the self-attention mechanism provides the architectural foundation for GPT-3's in-context few-shot learning capability by enabling the model to selectively focus on relevant regions of the input sequence, allowing it to adapt to new tasks and prompts with only a few examples.The self-attention mechanism, introduced in the "Attention Is All You Need" paper, provides a crucial architectural foundation for the in-context few-shot learning capability demonstrated in the GPT-3 paper.

In the context of transformer models like GPT-3, self-attention allows the model to attend to different parts of the input sequence simultaneously and weigh their importance. This mechanism enables the model to capture long-range dependencies and contextual relationships within the input data.

For few-shot learning, the key insight is that the self-attention mechanism can be used to selectively focus on specific parts of the input data that are relevant for a given task or prompt. In other words, the model can attend to the most informative regions of the input sequence that are related to the target task, and ignore irrelevant information.

In the GPT-3 paper, this is achieved by using the self-attention mechanism in combination with a few-shot learning approach called "in-context learning". The idea is to fine-tune the pre-trained GPT-3 model on a small number of examples (typically 1-5) that are relevant to the target task. During this fine-tuning process, the self-attention mechanism allows the model to selectively focus on the most informative regions of the input sequence that are related to the target task.

By leveraging the self-attention mechanism in this way, GPT-3 can learn to adapt to new tasks and prompts with only a few examples, demonstrating impressive few-shot learning capabilities. In essence, the self-attention mechanism enables the model to selectively attend to relevant information and ignore irrelevant information, which is critical for effective few-shot learning.

In summary, the self-attention mechanism provides the architectural foundation for GPT-3's in-context few-shot learning capability by enabling the model to selectively focus on relevant regions of the input sequence, allowing it to adapt to new tasks and prompts with only a few examples.The self-attention mechanism, introduced in the "Attention Is All You Need" paper, provides a crucial architectural foundation for the in-context few-shot learning capability demonstrated in the GPT-3 paper.

In the context of transformer models like GPT-3, self-attention allows the model to attend to different parts of the input sequence simultaneously and weigh their importance. This mechanism enables the model to capture long-range dependencies and contextual relationships within the input data.

For few-shot learning, the key insight is that the self-attention mechanism can be used to selectively focus on specific parts of the input data that are relevant for a given task or prompt. In other words, the model can attend to the most informative regions of the input sequence that are related to the target task, and ignore irrelevant information.

In the GPT-3 paper, this is achieved by using the self-attention mechanism in combination with a few-shot learning approach called "in-context learning". The idea is to fine-tune the pre-trained GPT-3 model on a small number of examples (typically 1-5) that are relevant to the target task. During this fine-tuning process, the self-attention mechanism allows the model to selectively focus on the most informative regions of the input sequence that are related to the target task.

By leveraging the self-attention mechanism in this way, GPT-3 can learn to adapt to new tasks and prompts with only a few examples, demonstrating impressive few-shot learning capabilities. In essence, the self-attention mechanism enables the model to selectively attend to relevant information and ignore irrelevant information, which is critical for effective few-shot learning.

In summary, the self-attention mechanism provides the architectural foundation for GPT-3's in-context few-shot learning capability by enabling the model to selectively focus on relevant regions of the input sequence, allowing it to adapt to new tasks and prompts with only a few examples.The self-attention mechanism, introduced in the "Attention Is All You Need" paper, provides a crucial architectural foundation for the in-context few-shot learning capability demonstrated in the GPT-3 paper.

In the context of transformer models like GPT-3, self-attention allows the model to attend to different parts of the input sequence simultaneously and weigh their importance. This mechanism enables the model to capture long-range dependencies and contextual relationships within the input data.

For few-shot learning, the key insight is that the self-attention mechanism can be used to selectively focus on specific parts of the input data that are relevant for a given task or prompt. In other words, the model can attend to the most informative regions of the input sequence that are related to the target task, and ignore irrelevant information.

In the GPT-3 paper, this is achieved by using the self-attention mechanism in combination with a few-shot learning approach called "in-context learning". The idea is to fine-tune the pre-trained GPT-3 model on a small number of examples (typically 1-5) that are relevant to the target task. During this fine-tuning process, the self-attention mechanism allows the model to selectively focus on the most informative regions of the input sequence that are related to the target task.

By leveraging the self-attention mechanism in this way, GPT-3 can learn to adapt to new tasks and prompts with only a few examples, demonstrating impressive few-shot learning capabilities. In essence, the self-attention mechanism enables the model to selectively attend to relevant information and ignore irrelevant information, which is critical for effective few-shot learning.

In summary, the self-attention mechanism provides the architectural foundation for GPT-3's in-context few-shot learning capability by enabling the model to selectively focus on relevant regions of the input sequence, allowing it to adapt to new tasks and prompts with only a few examples.The self-attention mechanism, introduced in the "Attention Is All You Need" paper, provides a crucial architectural foundation for the in-context few-shot learning capability demonstrated in the GPT-3 paper.

In the context of transformer models like GPT-3, self-attention allows the model to attend to different parts of the input sequence simultaneously and weigh their importance. This mechanism enables the model to capture long-range dependencies and contextual relationships within the input data.

For few-shot learning, the key insight is that the self-attention mechanism can be used to selectively focus on specific parts of the input data that are relevant for a given task or prompt. In other words, the model can attend to the most informative regions of the input sequence that are related to the target task, and ignore irrelevant information.

In the GPT-3 paper, this is achieved by using the self-attention mechanism in combination with a few-shot learning approach called "in-context learning". The idea is to fine-tune the pre-trained GPT-3 model on a small number of examples (typically 1-5) that are relevant to the target task. During this fine-tuning process, the self-attention mechanism allows the model to selectively focus on the most informative regions of the input sequence that are related to the target task.

By leveraging the self-attention mechanism in this way, GPT-3 can learn to adapt to new tasks and prompts with only a few examples, demonstrating impressive few-shot learning capabilities. In essence, the self-attention mechanism enables the model to selectively attend to relevant information and ignore irrelevant information, which is critical for effective few-shot learning.

In summary, the self-attention mechanism provides the architectural foundation for GPT-3's in-context few-shot learning capability by enabling the model to selectively focus on relevant regions of the input sequence, allowing it to adapt to new tasks and prompts with only a few examples.The self-attention mechanism, introduced in the "Attention Is All You Need" paper, provides a crucial architectural foundation for the in-context few-shot learning capability demonstrated in the GPT-3 paper.

In the context of transformer models like GPT-3, self-attention allows the model to attend to different parts of the input sequence simultaneously and weigh their importance. This mechanism enables the model to capture long-range dependencies and contextual relationships within the input data.

For few-shot learning, the key insight is that the self-attention mechanism can be used to selectively focus on specific parts of the input data that are relevant for a given task or prompt. In other words, the model can attend to the most informative regions of the input sequence that are related to the target task, and ignore irrelevant information.

In the GPT-3 paper, this is achieved by using the self-attention mechanism in combination with a few-shot learning approach called "in-context learning". The idea is to fine-tune the pre-trained GPT-3 model on a small number of examples (typically 1-5) that are relevant to the target task. During this fine-tuning process, the self-attention mechanism allows the model to selectively focus on the most informative regions of the input sequence that are related to the target task.

By leveraging the self-attention mechanism in this way, GPT-3 can learn to adapt to new tasks and prompts with only a few examples, demonstrating impressive few-shot learning capabilities. In essence, the self-attention mechanism enables the model to selectively attend to relevant information and ignore irrelevant information, which is critical for effective few-shot learning.

In summary, the self-attention mechanism provides the architectural foundation for GPT-3's in-context few-shot learning capability by enabling the model to selectively focus on relevant regions of the input sequence, allowing it to adapt to new tasks and prompts with only a few examples.The self-attention mechanism, introduced in the "Attention Is All You Need" paper, provides a crucial architectural foundation for the in-context few-shot learning capability demonstrated in the GPT-3 paper.

In the context of transformer models like GPT-3, self-attention allows the model to attend to different parts of the input sequence simultaneously and weigh their importance. This mechanism enables the model to capture long-range dependencies and contextual relationships within the input data.

For few-shot learning, the key insight is that the self-attention mechanism can be used to selectively focus on specific parts of the input data that are relevant for a given task or prompt. In other words, the model can attend to the most informative regions of the input sequence that are related to the target task, and ignore irrelevant information.

In the GPT-3 paper, this is achieved by using the self-attention mechanism in combination with a few-shot learning approach called "in-context learning". The idea is to fine-tune the pre-trained GPT-3 model on a small number of examples (typically 1-5) that are relevant to the target task. During this fine-tuning process, the self-attention mechanism allows the model to selectively focus on the most informative regions of the input sequence that are related to the target task.

By leveraging the self-attention mechanism in this way, GPT-3 can learn to adapt to new tasks and prompts with only a few examples, demonstrating impressive few-shot learning capabilities. In essence, the self-attention mechanism enables the model to selectively attend to relevant information and ignore irrelevant information, which is critical for effective few-shot learning.

In summary, the self-attention mechanism provides the architectural foundation for GPT-3's in-context few-shot learning capability by enabling the model to selectively focus on relevant regions of the input sequence, allowing it to adapt to new tasks and prompts with only a few examples.The self-attention mechanism, introduced in the "Attention Is All You Need" paper, provides a crucial architectural foundation for the in-context few-shot learning capability demonstrated in the GPT-3 paper.

In the context of transformer models like GPT-3, self-attention allows the model to attend to different parts of the input sequence simultaneously and weigh their importance. This mechanism enables the model to capture long-range dependencies and contextual relationships within the input data.

For few-shot learning, the key insight is that the self-attention mechanism can be used to selectively focus on specific parts of the input data that are relevant for a given task or prompt. In other words, the model can attend to the most informative regions of the input sequence that are related to the target task, and ignore irrelevant information.

In the GPT-3 paper, this is achieved by using the self-attention mechanism in combination with a few-shot learning approach called "in-context learning". The idea is to fine-tune the pre-trained GPT-3 model on a small number of examples (typically 1-5) that are relevant to the target task. During this fine-tuning process, the self-attention mechanism allows the model to selectively focus on the most informative regions of the input sequence that are related to the target task.

By leveraging the self-attention mechanism in this way, GPT-3 can learn to adapt to new tasks and prompts with only a few examples, demonstrating impressive few-shot learning capabilities. In essence, the self-attention mechanism enables the model to selectively attend to relevant information and ignore irrelevant information, which is critical for effective few-shot learning.

In summary, the self-attention mechanism provides the architectural foundation for GPT-3's in-context few-shot learning capability by enabling the model to selectively focus on relevant regions of the input sequence, allowing it to adapt to new tasks and prompts with only a few examples.The self-attention mechanism, introduced in the "Attention Is All You Need" paper, provides a crucial architectural foundation for the in-context few-shot learning capability demonstrated in the GPT-3 paper.

In the context of transformer models like GPT-3, self-attention allows the model to attend to different parts of the input sequence simultaneously and weigh their importance. This mechanism enables the model to capture long-range dependencies and contextual relationships within the input data.

For few-shot learning, the key insight is that the self-attention mechanism can be used to selectively focus on specific parts of the input data that are relevant for a given task or prompt. In other words, the model can attend to the most informative regions of the input sequence that are related to the target task, and ignore irrelevant information.

In the GPT-3 paper, this is achieved by using the self-attention mechanism in combination with a few-shot learning approach called "in-context learning". The idea is to fine-tune the pre-trained GPT-3 model on a small number of examples (typically 1-5) that are relevant to the target task. During this fine-tuning process, the self-attention mechanism allows the model to selectively focus on the most informative regions of the input sequence that are related to the target task.

By leveraging the self-attention mechanism in this way, GPT-3 can learn to adapt to new tasks and prompts with only a few examples, demonstrating impressive few-shot learning capabilities. In essence, the self-attention mechanism enables the model to selectively attend to relevant information and ignore irrelevant information, which is critical for effective few-shot learning.

In summary, the self-attention mechanism provides the architectural foundation for GPT-3's in-context few-shot learning capability by enabling the model to selectively focus on relevant regions of the input sequence, allowing it to adapt to new tasks and prompts with only a few examples.The self-attention mechanism, introduced in the "Attention Is All You Need" paper, provides a crucial architectural foundation for the in-context few-shot learning capability demonstrated in the GPT-3 paper.

In the context of transformer models like GPT-3, self-attention allows the model to attend to different parts of the input sequence simultaneously and weigh their importance. This mechanism enables the model to capture long-range dependencies and contextual relationships within the input data.

For few-shot learning, the key insight is that the self-attention mechanism can be used to selectively focus on specific parts of the input data that are relevant for a given task or prompt. In other words, the model can attend to the most informative regions of the input sequence that are related to the target task, and ignore irrelevant information.

In the GPT-3 paper, this is achieved by using the self-attention mechanism in combination with a few-shot learning approach called "in-context learning". The idea is to fine-tune the pre-trained GPT-3 model on a small number of examples (typically 1-5) that are relevant to the target task. During this fine-tuning process, the self-attention mechanism allows the model to selectively focus on the most informative regions of the input sequence that are related to the target task.

By leveraging the self-attention mechanism in this way, GPT-3 can learn to adapt to new tasks and prompts with only a few examples, demonstrating impressive few-shot learning capabilities. In essence, the self-attention mechanism enables the model to selectively attend to relevant information and ignore irrelevant information, which is critical for effective few-shot learning.

In summary, the self-attention mechanism provides the architectural foundation for GPT-3's in-context few-shot learning capability by enabling the model to selectively focus on relevant regions of the input sequence, allowing it to adapt to new tasks and prompts with only a few examples.The self-attention mechanism, introduced in the "Attention Is All You Need" paper, provides a crucial architectural foundation for the in-context few-shot learning capability demonstrated in the GPT-3 paper.

In the context of transformer models like GPT-3, self-attention allows the model to attend to different parts of the input sequence simultaneously and weigh their importance. This mechanism enables the model to capture long-range dependencies and contextual relationships within the input data.

For few-shot learning, the key insight is that the self-attention mechanism can be used to selectively focus on specific parts of the input data that are relevant for a given task or prompt. In other words, the model can attend to the most informative regions of the input sequence that are related to the target task, and ignore irrelevant information.

In the GPT-3 paper, this is achieved by using the self-attention mechanism in combination with a few-shot learning approach called "in-context learning". The idea is to fine-tune the pre-trained GPT-3 model on a small number of examples (typically 1-5) that are relevant to the target task. During this fine-tuning process, the self-attention mechanism allows the model to selectively focus on the most informative regions of the input sequence that are related to the target task.

By leveraging the self-attention mechanism in this way, GPT-3 can learn to adapt to new tasks and prompts with only a few examples, demonstrating impressive few-shot learning capabilities. In essence, the self-attention mechanism enables the model to selectively attend to relevant information and ignore irrelevant information, which is critical for effective few-shot learning.

In summary, the self-attention mechanism provides the architectural foundation for GPT-3's in-context few-shot learning capability by enabling the model to selectively focus on relevant regions of the input sequence, allowing it to adapt to new tasks and prompts with only a few examples.The self-attention mechanism, introduced in the "Attention Is All You Need" paper, provides a crucial architectural foundation for the in-context few-shot learning capability demonstrated in the GPT-3 paper.

In the context of transformer models like GPT-3, self-attention allows the model to attend to different parts of the input sequence simultaneously and weigh their importance. This mechanism enables the model to capture long-range dependencies and contextual relationships within the input data.

For few-shot learning, the key insight is that the self-attention mechanism can be used to selectively focus on specific parts of the input data that are relevant for a given task or prompt. In other words, the model can attend to the most informative regions of the input sequence that are related to the target task, and ignore irrelevant information.

In the GPT-3 paper, this is achieved by using the self-attention mechanism in combination with a few-shot learning approach called "in-context learning". The idea is to fine-tune the pre-trained GPT-3 model on a small number of examples (typically 1-5) that are relevant to the target task. During this fine-tuning process, the self-attention mechanism allows the model to selectively focus on the most informative regions of the input sequence that are related to the target task.

By leveraging the self-attention mechanism in this way, GPT-3 can learn to adapt to new tasks and prompts with only a few examples, demonstrating impressive few-shot learning capabilities. In essence, the self-attention mechanism enables the model to selectively attend to relevant information and ignore irrelevant information, which is critical for effective few-shot learning.

In summary, the self-attention mechanism provides the architectural foundation for GPT-3's in-context few-shot learning capability by enabling the model to selectively focus on relevant regions of the input sequence, allowing it to adapt to new tasks and prompts with only a few examples.The self-attention mechanism, introduced in the "Attention Is All You Need" paper, provides a crucial architectural foundation for the in-context few-shot learning capability demonstrated in the GPT-3 paper.

In the context of transformer models like GPT-3, self-attention allows the model to attend to different parts of the input sequence simultaneously and weigh their importance. This mechanism enables the model to capture long-range dependencies and contextual relationships within the input data.

For few-shot learning, the key insight is that the self-attention mechanism can be used to selectively focus on specific parts of the input data that are relevant for a given task or prompt. In other words, the model can attend to the most informative regions of the input sequence that are related to the target task, and ignore irrelevant information.

In the GPT-3 paper, this is achieved by using the self-attention mechanism in combination with a few-shot learning approach called "in-context learning". The idea is to fine-tune the pre-trained GPT-3 model on a small number of examples (typically 1-5) that are relevant to the target task. During this fine-tuning process, the self-attention mechanism allows the model to selectively focus on the most informative regions of the input sequence that are related to the target task.

By leveraging the self-attention mechanism in this way, GPT-3 can learn to adapt to new tasks and prompts with only a few examples, demonstrating impressive few-shot learning capabilities. In essence, the self-attention mechanism enables the model to selectively attend to relevant information and ignore irrelevant information, which is critical for effective few-shot learning.

In summary, the self-attention mechanism provides the architectural foundation for GPT-3's in-context few-shot learning capability by enabling the model to selectively focus on relevant regions of the input sequence, allowing it to adapt to new tasks and prompts with only a few examples.The self-attention mechanism, introduced in the "Attention Is All You Need" paper, provides a crucial architectural foundation for the in-context few-shot learning capability demonstrated in the GPT-3 paper.

In the context of transformer models like GPT-3, self-attention allows the model to attend to different parts of the input sequence simultaneously and weigh their importance. This mechanism enables the model to capture long-range dependencies and contextual relationships within the input data.

For few-shot learning, the key insight is that the self-attention mechanism can be used to selectively focus on specific parts of the input data that are relevant for a given task or prompt. In other words, the model can attend to the most informative regions of the input sequence that are related to the target task, and ignore irrelevant information.

In the GPT-3 paper, this is achieved by using the self-attention mechanism in combination with a few-shot learning approach called "in-context learning". The idea is to fine-tune the pre-trained GPT-3 model on a small number of examples (typically 1-5) that are relevant to the target task. During this fine-tuning process, the self-attention mechanism allows the model to selectively focus on the most informative regions of the input sequence that are related to the target task.

By leveraging the self-attention mechanism in this way, GPT-3 can learn to adapt to new tasks and prompts with only a few examples, demonstrating impressive few-shot learning capabilities. In essence, the self-attention mechanism enables the model to selectively attend to relevant information and ignore irrelevant information, which is critical for effective few-shot learning.

In summary, the self-attention mechanism provides the architectural foundation for GPT-3's in-context few-shot learning capability by enabling the model to selectively focus on relevant regions of the input sequence, allowing it to adapt to new tasks and prompts with only a few examples.The self-attention mechanism, introduced in the "Attention Is All You Need" paper, provides a crucial architectural foundation for the in-context few-shot learning capability demonstrated in the GPT-3 paper.

In the context of transformer models like GPT-3, self-attention allows the model to attend to different parts of the input sequence simultaneously and weigh their importance. This mechanism enables the model to capture long-range dependencies and contextual relationships within the input data.

For few-shot learning, the key insight is that the self-attention mechanism can be used to selectively focus on specific parts of the input data that are relevant for a given task or prompt. In other words, the model can attend to the most informative regions of the input sequence that are related to the target task, and ignore irrelevant information.

In the GPT-3 paper, this is achieved by using the self-attention mechanism in combination with a few-shot learning approach called "in-context learning". The idea is to fine-tune the pre-trained GPT-3 model on a small number of examples (typically 1-5) that are relevant to the target task. During this fine-tuning process, the self-attention mechanism allows the model to selectively focus on the most informative regions of the input sequence that are related to the target task.

By leveraging the self-attention mechanism in this way, GPT-3 can learn to adapt to new tasks and prompts with only a few examples, demonstrating impressive few-shot learning capabilities. In essence, the self-attention mechanism enables the model to selectively attend to relevant information and ignore irrelevant information, which is critical for effective few-shot learning.

In summary, the self-attention mechanism provides the architectural foundation for GPT-3's in-context few-shot learning capability by enabling the model to selectively focus on relevant regions of the input sequence, allowing it to adapt to new tasks and prompts with only a few examples.The self-attention mechanism, introduced in the "Attention Is All You Need" paper, provides a crucial architectural foundation for the in-context few-shot learning capability demonstrated in the GPT-3 paper.

In the context of transformer models like GPT-3, self-attention allows the model to attend to different parts of the input sequence simultaneously and weigh their importance. This mechanism enables the model to capture long-range dependencies and contextual relationships within the input data.

For few-shot learning, the key insight is that the self-attention mechanism can be used to selectively focus on specific parts of the input data that are relevant for a given task or prompt. In other words, the model can attend to the most informative regions of the input sequence that are related to the target task, and ignore irrelevant information.

In the GPT-3 paper, this is achieved by using the self-attention mechanism in combination with a few-shot learning approach called "in-context learning". The idea is to fine-tune the pre-trained GPT-3 model on a small number of examples (typically 1-5) that are relevant to the target task. During this fine-tuning process, the self-attention mechanism allows the model to selectively focus on the most informative regions of the input sequence that are related to the target task.

By leveraging the self-attention mechanism in this way, GPT-3 can learn to adapt to new tasks and prompts with only a few examples, demonstrating impressive few-shot learning capabilities. In essence, the self-attention mechanism enables the model to selectively attend to relevant information and ignore irrelevant information, which is critical for effective few-shot learning.

In summary, the self-attention mechanism provides the architectural foundation for GPT-3's in-context few-shot learning capability by enabling the model to selectively focus on relevant regions of the input sequence, allowing it to adapt to new tasks and prompts with only a few examples.The self-attention mechanism, introduced in the "Attention Is All You Need" paper, provides a crucial architectural foundation for the in-context few-shot learning capability demonstrated in the GPT-3 paper.

In the context of transformer models like GPT-3, self-attention allows the model to attend to different parts of the input sequence simultaneously and weigh their importance. This mechanism enables the model to capture long-range dependencies and contextual relationships within the input data.

For few-shot learning, the key insight is that the self-attention mechanism can be used to selectively focus on specific parts of the input data that are relevant for a given task or prompt. In other words, the model can attend to the most informative regions of the input sequence that are related to the target task, and ignore irrelevant information.

In the GPT-3 paper, this is achieved by using the self-attention mechanism in combination with a few-shot learning approach called "in-context learning". The idea is to fine-tune the pre-trained GPT-3 model on a small number of examples (typically 1-5) that are relevant to the target task. During this fine-tuning process, the self-attention mechanism allows the model to selectively focus on the most informative regions of the input sequence that are related to the target task.

By leveraging the self-attention mechanism in this way, GPT-3 can learn to adapt to new tasks and prompts with only a few examples, demonstrating impressive few-shot learning capabilities. In essence, the self-attention mechanism enables the model to selectively attend to relevant information and ignore irrelevant information, which is critical for effective few-shot learning.

In summary, the self-attention mechanism provides the architectural foundation for GPT-3's in-context few-shot learning capability by enabling the model to selectively focus on relevant regions of the input sequence, allowing it to adapt to new tasks and prompts with only a few examples.The self-attention mechanism, introduced in the "Attention Is All You Need" paper, provides a crucial architectural foundation for the in-context few-shot learning capability demonstrated in the GPT-3 paper.

In the context of transformer models like GPT-3, self-attention allows the model to attend to different parts of the input sequence simultaneously and weigh their importance. This mechanism enables the model to capture long-range dependencies and contextual relationships within the input data.

For few-shot learning, the key insight is that the self-attention mechanism can be used to selectively focus on specific parts of the input data that are relevant for a given task or prompt. In other words, the model can attend to the most informative regions of the input sequence that are related to the target task, and ignore irrelevant information.

In the GPT-3 paper, this is achieved by using the self-attention mechanism in combination with a few-shot learning approach called "in-context learning". The idea is to fine-tune the pre-trained GPT-3 model on a small number of examples (typically 1-5) that are relevant to the target task. During this fine-tuning process, the self-attention mechanism allows the model to selectively focus on the most informative regions of the input sequence that are related to the target task.

By leveraging the self-attention mechanism in this way, GPT-3 can learn to adapt to new tasks and prompts with only a few examples, demonstrating impressive few-shot learning capabilities. In essence, the self-attention mechanism enables the model to selectively attend to relevant information and ignore irrelevant information, which is critical for effective few-shot learning.

In summary, the self-attention mechanism provides the architectural foundation for GPT-3's in-context few-shot learning capability by enabling the model to selectively focus on relevant regions of the input sequence, allowing it to adapt to new tasks and prompts with only a few examples.The self-attention mechanism, introduced in the "Attention Is All You Need" paper, provides a crucial architectural foundation for the in-context few-shot learning capability demonstrated in the GPT-3 paper.

In the context of transformer models like GPT-3, self-attention allows the model to attend to different parts of the input sequence simultaneously and weigh their importance. This mechanism enables the model to capture long-range dependencies and contextual relationships within the input data.

For few-shot learning, the key insight is that the self-attention mechanism can be used to selectively focus on specific parts of the input data that are relevant for a given task or prompt. In other words, the model can attend to the most informative regions of the input sequence that are related to the target task, and ignore irrelevant information.

In the GPT-3 paper, this is achieved by using the self-attention mechanism in combination with a few-shot learning approach called "in-context learning". The idea is to fine-tune the pre-trained GPT-3 model on a small number of examples (typically 1-5) that are relevant to the target task. During this fine-tuning process, the self-attention mechanism allows the model to selectively focus on the most informative regions of the input sequence that are related to the target task.

By leveraging the self-attention mechanism in this way, GPT-3 can learn to adapt to new tasks and prompts with only a few examples, demonstrating impressive few-shot learning capabilities. In essence, the self-attention mechanism enables the model to selectively attend to relevant information and ignore irrelevant information, which is critical for effective few-shot learning.

In summary, the self-attention mechanism provides the architectural foundation for GPT-3's in-context few-shot learning capability by enabling the model to selectively focus on relevant regions of the input sequence, allowing it to adapt to new tasks and prompts with only a few examples.The self-attention mechanism, introduced in the "Attention Is All You Need" paper, provides a crucial architectural foundation for the in-context few-shot learning capability demonstrated in the GPT-3 paper.

In the context of transformer models like GPT-3, self-attention allows the model to attend to different parts of the input sequence simultaneously and weigh their importance. This mechanism enables the model to capture long-range dependencies and contextual relationships within the input data.

For few-shot learning, the key insight is that the self-attention mechanism can be used to selectively focus on specific parts of the input data that are relevant for a given task or prompt. In other words, the model can attend to the most informative regions of the input sequence that are related to the target task, and ignore irrelevant information.

In the GPT-3 paper, this is achieved by using the self-attention mechanism in combination with a few-shot learning approach called "in-context learning". The idea is to fine-tune the pre-trained GPT-3 model on a small number of examples (typically 1-5) that are relevant to the target task. During this fine-tuning process, the self-attention mechanism allows the model to selectively focus on the most informative regions of the input sequence that are related to the target task.

By leveraging the self-attention mechanism in this way, GPT-3 can learn to adapt to new tasks and prompts with only a few examples, demonstrating impressive few-shot learning capabilities. In essence, the self-attention mechanism enables the model to selectively attend to relevant information and ignore irrelevant information, which is critical for effective few-shot learning.

In summary, the self-attention mechanism provides the architectural foundation for GPT-3's in-context few-shot learning capability by enabling the model to selectively focus on relevant regions of the input sequence, allowing it to adapt to new tasks and prompts with only a few examples.The self-attention mechanism, introduced in the "Attention Is All You Need" paper, provides a crucial architectural foundation for the in-context few-shot learning capability demonstrated in the GPT-3 paper.

In the context of transformer models like GPT-3, self-attention allows the model to attend to different parts of the input sequence simultaneously and weigh their importance. This mechanism enables the model to capture long-range dependencies and contextual relationships within the input data.

For few-shot learning, the key insight is that the self-attention mechanism can be used to selectively focus on specific parts of the input data that are relevant for a given task or prompt. In other words, the model can attend to the most informative regions of the input sequence that are related to the target task, and ignore irrelevant information.

In the GPT-3 paper, this is achieved by using the self-attention mechanism in combination with a few-shot learning approach called "in-context learning". The idea is to fine-tune the pre-trained GPT-3 model on a small number of examples (typically 1-5) that are relevant to the target task. During this fine-tuning process, the self-attention mechanism allows the model to selectively focus on the most informative regions of the input sequence that are related to the target task.

By leveraging the self-attention mechanism in this way, GPT-3 can learn to adapt to new tasks and prompts with only a few examples, demonstrating impressive few-shot learning capabilities. In essence, the self-attention mechanism enables the model to selectively attend to relevant information and ignore irrelevant information, which is critical for effective few-shot learning.

In summary, the self-attention mechanism provides the architectural foundation for GPT-3's in-context few-shot learning capability by enabling the model to selectively focus on relevant regions of the input sequence, allowing it to adapt to new tasks and prompts with only a few examples.The self-attention mechanism, introduced in the "Attention Is All You Need" paper, provides a crucial architectural foundation for the in-context few-shot learning capability demonstrated in the GPT-3 paper.

In the context of transformer models like GPT-3, self-attention allows the model to attend to different parts of the input sequence simultaneously and weigh their importance. This mechanism enables the model to capture long-range dependencies and contextual relationships within the input data.

For few-shot learning, the key insight is that the self-attention mechanism can be used to selectively focus on specific parts of the input data that are relevant for a given task or prompt. In other words, the model can attend to the most informative regions of the input sequence that are related to the target task, and ignore irrelevant information.

In the GPT-3 paper, this is achieved by using the self-attention mechanism in combination with a few-shot learning approach called "in-context learning". The idea is to fine-tune the pre-trained GPT-3 model on a small number of examples (typically 1-5) that are relevant to the target task. During this fine-tuning process, the self-attention mechanism allows the model to selectively focus on the most informative regions of the input sequence that are related to the target task.

By leveraging the self-attention mechanism in this way, GPT-3 can learn to adapt to new tasks and prompts with only a few examples, demonstrating impressive few-shot learning capabilities. In essence, the self-attention mechanism enables the model to selectively attend to relevant information and ignore irrelevant information, which is critical for effective few-shot learning.

In summary, the self-attention mechanism provides the architectural foundation for GPT-3's in-context few-shot learning capability by enabling the model to selectively focus on relevant regions of the input sequence, allowing it to adapt to new tasks and prompts with only a few examples.The self-attention mechanism, introduced in the "Attention Is All You Need" paper, provides a crucial architectural foundation for the in-context few-shot learning capability demonstrated in the GPT-3 paper.

In the context of transformer models like GPT-3, self-attention allows the model to attend to different parts of the input sequence simultaneously and weigh their importance. This mechanism enables the model to capture long-range dependencies and contextual relationships within the input data.

For few-shot learning, the key insight is that the self-attention mechanism can be used to selectively focus on specific parts of the input data that are relevant for a given task or prompt. In other words, the model can attend to the most informative regions of the input sequence that are related to the target task, and ignore irrelevant information.

In the GPT-3 paper, this is achieved by using the self-attention mechanism in combination with a few-shot learning approach called "in-context learning". The idea is to fine-tune the pre-trained GPT-3 model on a small number of examples (typically 1-5) that are relevant to the target task. During this fine-tuning process, the self-attention mechanism allows the model to selectively focus on the most informative regions of the input sequence that are related to the target task.

By leveraging the self-attention mechanism in this way, GPT-3 can learn to adapt to new tasks and prompts with only a few examples, demonstrating impressive few-shot learning capabilities. In essence, the self-attention mechanism enables the model to selectively attend to relevant information and ignore irrelevant information, which is critical for effective few-shot learning.

In summary, the self-attention mechanism provides the architectural foundation for GPT-3's in-context few-shot learning capability by enabling the model to selectively focus on relevant regions of the input sequence, allowing it to adapt to new tasks and prompts with only a few examples.The self-attention mechanism, introduced in the "Attention Is All You Need" paper, provides a crucial architectural foundation for the in-context few-shot learning capability demonstrated in the GPT-3 paper.

In the context of transformer models like GPT-3, self-attention allows the model to attend to different parts of the input sequence simultaneously and weigh their importance. This mechanism enables the model to capture long-range dependencies and contextual relationships within the input data.

For few-shot learning, the key insight is that the self-attention mechanism can be used to selectively focus on specific parts of the input data that are relevant for a given task or prompt. In other words, the model can attend to the most informative regions of the input sequence that are related to the target task, and ignore irrelevant information.

In the GPT-3 paper, this is achieved by using the self-attention mechanism in combination with a few-shot learning approach called "in-context learning". The idea is to fine-tune the pre-trained GPT-3 model on a small number of examples (typically 1-5) that are relevant to the target task. During this fine-tuning process, the self-attention mechanism allows the model to selectively focus on the most informative regions of the input sequence that are related to the target task.

By leveraging the self-attention mechanism in this way, GPT-3 can learn to adapt to new tasks and prompts with only a few examples, demonstrating impressive few-shot learning capabilities. In essence, the self-attention mechanism enables the model to selectively attend to relevant information and ignore irrelevant information, which is critical for effective few-shot learning.

In summary, the self-attention mechanism provides the architectural foundation for GPT-3's in-context few-shot learning capability by enabling the model to selectively focus on relevant regions of the input sequence, allowing it to adapt to new tasks and prompts with only a few examples.' to numeric

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/analyst.py", line 62, in run_from_config
    out_df = analyze_df(df, group_col=group_col, rating_col=rating_col)
  File "/home/johnro/sop-research/gnosis/src/analyst.py", line 32, in analyze_df
    grouped = df.groupby(group_col)[rating_col].mean().reset_index()
  File "/home/johnro/sop-research/gnosis/.venv/lib/python3.10/site-packages/pandas/core/groupby/groupby.py", line 2458, in mean
    result = self._cython_agg_general(
  File "/home/johnro/sop-research/gnosis/.venv/lib/python3.10/site-packages/pandas/core/groupby/groupby.py", line 2004, in _cython_agg_general
    new_mgr = data.grouped_reduce(array_func)
  File "/home/johnro/sop-research/gnosis/.venv/lib/python3.10/site-packages/pandas/core/internals/base.py", line 367, in grouped_reduce
    res = func(arr)
  File "/home/johnro/sop-research/gnosis/.venv/lib/python3.10/site-packages/pandas/core/groupby/groupby.py", line 2001, in array_func
    result = self._agg_py_fallback(how, values, ndim=data.ndim, alt=alt)
  File "/home/johnro/sop-research/gnosis/.venv/lib/python3.10/site-packages/pandas/core/groupby/groupby.py", line 1947, in _agg_py_fallback
    raise type(err)(msg) from err
TypeError: agg function failed [how->mean,dtype->object]
2025-08-20 20:08:12,331 INFO analyst run_from_config start for analyst with cfg keys=['input_csv', 'output_csv', 'group_col', 'rating_col']
2025-08-20 20:08:12,485 INFO analyst analyze_df start group_col=['synthesis_id', 'ablation_condition'] rating_col=score_faithfulness_rating
2025-08-20 20:08:12,488 INFO analyst analyze_df done (groups=328)
2025-08-20 20:08:12,489 INFO src.io Wrote dataframe to output/analysis_summary_faithfulness.csv (rows=328)
2025-08-20 20:08:12,489 INFO analyst run_from_config finished successfully, wrote output/analysis_summary_faithfulness.csv
2025-08-20 20:08:12,492 INFO src.io Wrote provenance for output/analysis_summary_faithfulness.csv -> output/analysis_summary_faithfulness.csv.meta.json
2025-08-20 20:08:12,492 INFO analyst run_from_config start for analyst with cfg keys=['input_csv', 'output_csv', 'group_col', 'rating_col']
2025-08-20 20:08:12,625 INFO analyst analyze_df start group_col=['synthesis_id', 'ablation_condition'] rating_col=score_correctness_rating
2025-08-20 20:08:12,627 INFO analyst analyze_df done (groups=328)
2025-08-20 20:08:12,628 INFO src.io Wrote dataframe to output/analysis_summary_correctness.csv (rows=328)
2025-08-20 20:08:12,628 INFO analyst run_from_config finished successfully, wrote output/analysis_summary_correctness.csv
2025-08-20 20:08:12,631 INFO src.io Wrote provenance for output/analysis_summary_correctness.csv -> output/analysis_summary_correctness.csv.meta.json
2025-08-20 20:08:12,631 INFO analyst run_from_config start for analyst with cfg keys=['input_csv', 'output_csv', 'group_col', 'rating_col']
2025-08-20 20:08:12,766 INFO analyst analyze_df start group_col=['synthesis_id', 'ablation_condition'] rating_col=score_completeness_rating
2025-08-20 20:08:12,770 INFO analyst analyze_df done (groups=328)
2025-08-20 20:08:12,771 INFO src.io Wrote dataframe to output/analysis_summary_completeness.csv (rows=328)
2025-08-20 20:08:12,771 INFO analyst run_from_config finished successfully, wrote output/analysis_summary_completeness.csv
2025-08-20 20:08:12,775 INFO src.io Wrote provenance for output/analysis_summary_completeness.csv -> output/analysis_summary_completeness.csv.meta.json
2025-08-20 20:08:12,775 INFO analyst run_from_config start for analyst with cfg keys=['input_csv', 'output_csv', 'group_col', 'rating_col']
2025-08-20 20:08:12,942 INFO analyst analyze_df start group_col=['synthesis_id', 'ablation_condition'] rating_col=score_clarity_rating
2025-08-20 20:08:12,944 INFO analyst analyze_df done (groups=328)
2025-08-20 20:08:12,945 INFO src.io Wrote dataframe to output/analysis_summary_clarity.csv (rows=328)
2025-08-20 20:08:12,945 INFO analyst run_from_config finished successfully, wrote output/analysis_summary_clarity.csv
2025-08-20 20:08:12,948 INFO src.io Wrote provenance for output/analysis_summary_clarity.csv -> output/analysis_summary_clarity.csv.meta.json
2025-08-20 20:08:12,948 INFO analyst run_from_config start for analyst with cfg keys=['input_csv', 'output_csv', 'group_col', 'metrics']
2025-08-20 20:08:13,081 INFO analyst analyze_df start group_col=['synthesis_id', 'ablation_condition'] rating_col=rating
2025-08-20 20:08:13,081 ERROR analyst rating_col 'rating' not found in DataFrame columns
2025-08-20 20:08:13,081 ERROR analyst run_from_config failed: rating_col 'rating' not found in DataFrame columns
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/analyst.py", line 62, in run_from_config
    out_df = analyze_df(df, group_col=group_col, rating_col=rating_col)
  File "/home/johnro/sop-research/gnosis/src/analyst.py", line 31, in analyze_df
    raise ValueError(f"rating_col '{rating_col}' not found in DataFrame columns")
ValueError: rating_col 'rating' not found in DataFrame columns
2025-08-20 20:10:32,329 INFO analyst run_from_config start for analyst with cfg keys=['input_csv', 'output_csv', 'group_col', 'rating_col']
2025-08-20 20:10:32,488 INFO analyst analyze_df start group_col=['synthesis_id', 'ablation_condition'] rating_col=score_faithfulness_rating
2025-08-20 20:10:32,490 INFO analyst analyze_df done (groups=328)
2025-08-20 20:10:32,492 INFO src.io Wrote dataframe to output/analysis_summary_faithfulness.csv (rows=328)
2025-08-20 20:10:32,492 INFO analyst run_from_config finished successfully, wrote output/analysis_summary_faithfulness.csv
2025-08-20 20:10:32,495 INFO src.io Wrote provenance for output/analysis_summary_faithfulness.csv -> output/analysis_summary_faithfulness.csv.meta.json
2025-08-20 20:10:32,495 INFO analyst run_from_config start for analyst with cfg keys=['input_csv', 'output_csv', 'group_col', 'rating_col']
2025-08-20 20:10:32,632 INFO analyst analyze_df start group_col=['synthesis_id', 'ablation_condition'] rating_col=score_correctness_rating
2025-08-20 20:10:32,634 INFO analyst analyze_df done (groups=328)
2025-08-20 20:10:32,635 INFO src.io Wrote dataframe to output/analysis_summary_correctness.csv (rows=328)
2025-08-20 20:10:32,635 INFO analyst run_from_config finished successfully, wrote output/analysis_summary_correctness.csv
2025-08-20 20:10:32,637 INFO src.io Wrote provenance for output/analysis_summary_correctness.csv -> output/analysis_summary_correctness.csv.meta.json
2025-08-20 20:10:32,637 INFO analyst run_from_config start for analyst with cfg keys=['input_csv', 'output_csv', 'group_col', 'rating_col']
2025-08-20 20:10:32,781 INFO analyst analyze_df start group_col=['synthesis_id', 'ablation_condition'] rating_col=score_completeness_rating
2025-08-20 20:10:32,783 INFO analyst analyze_df done (groups=328)
2025-08-20 20:10:32,785 INFO src.io Wrote dataframe to output/analysis_summary_completeness.csv (rows=328)
2025-08-20 20:10:32,785 INFO analyst run_from_config finished successfully, wrote output/analysis_summary_completeness.csv
2025-08-20 20:10:32,788 INFO src.io Wrote provenance for output/analysis_summary_completeness.csv -> output/analysis_summary_completeness.csv.meta.json
2025-08-20 20:10:32,788 INFO analyst run_from_config start for analyst with cfg keys=['input_csv', 'output_csv', 'group_col', 'rating_col']
2025-08-20 20:10:32,932 INFO analyst analyze_df start group_col=['synthesis_id', 'ablation_condition'] rating_col=score_clarity_rating
2025-08-20 20:10:32,935 INFO analyst analyze_df done (groups=328)
2025-08-20 20:10:32,936 INFO src.io Wrote dataframe to output/analysis_summary_clarity.csv (rows=328)
2025-08-20 20:10:32,936 INFO analyst run_from_config finished successfully, wrote output/analysis_summary_clarity.csv
2025-08-20 20:10:32,939 INFO src.io Wrote provenance for output/analysis_summary_clarity.csv -> output/analysis_summary_clarity.csv.meta.json
2025-08-20 20:10:32,939 INFO analyst run_from_config start for analyst with cfg keys=['input_csv', 'output_csv', 'group_col', 'metrics']
2025-08-20 20:10:33,075 ERROR analyst run_from_config failed: Metric 'semantic_difference' is not registered.
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/analyst.py", line 70, in run_from_config
    analyst.register_metric(metric_name)
  File "/home/johnro/sop-research/gnosis/src/analyst.py", line 110, in register_metric
    metric_class = MetricRegistry.get_metric(metric_name)
  File "/home/johnro/sop-research/gnosis/src/analytics/metrics/base.py", line 29, in get_metric
    raise ValueError(f"Metric '{name}' is not registered.")
ValueError: Metric 'semantic_difference' is not registered.
2025-08-20 20:17:24,129 INFO analyst run_from_config start for analyst with cfg keys=['input_csv', 'output_csv', 'group_col', 'rating_col']
2025-08-20 20:17:24,291 INFO analyst analyze_df start group_col=['synthesis_id', 'ablation_condition'] rating_col=score_faithfulness_rating
2025-08-20 20:17:24,293 INFO analyst analyze_df done (groups=328)
2025-08-20 20:17:24,295 INFO src.io Wrote dataframe to output/analysis_summary_faithfulness.csv (rows=328)
2025-08-20 20:17:24,295 INFO analyst run_from_config finished successfully, wrote output/analysis_summary_faithfulness.csv
2025-08-20 20:17:24,298 INFO src.io Wrote provenance for output/analysis_summary_faithfulness.csv -> output/analysis_summary_faithfulness.csv.meta.json
2025-08-20 20:17:24,298 INFO analyst run_from_config start for analyst with cfg keys=['input_csv', 'output_csv', 'group_col', 'rating_col']
2025-08-20 20:17:24,438 INFO analyst analyze_df start group_col=['synthesis_id', 'ablation_condition'] rating_col=score_correctness_rating
2025-08-20 20:17:24,440 INFO analyst analyze_df done (groups=328)
2025-08-20 20:17:24,441 INFO src.io Wrote dataframe to output/analysis_summary_correctness.csv (rows=328)
2025-08-20 20:17:24,441 INFO analyst run_from_config finished successfully, wrote output/analysis_summary_correctness.csv
2025-08-20 20:17:24,443 INFO src.io Wrote provenance for output/analysis_summary_correctness.csv -> output/analysis_summary_correctness.csv.meta.json
2025-08-20 20:17:24,444 INFO analyst run_from_config start for analyst with cfg keys=['input_csv', 'output_csv', 'group_col', 'rating_col']
2025-08-20 20:17:24,577 INFO analyst analyze_df start group_col=['synthesis_id', 'ablation_condition'] rating_col=score_completeness_rating
2025-08-20 20:17:24,580 INFO analyst analyze_df done (groups=328)
2025-08-20 20:17:24,581 INFO src.io Wrote dataframe to output/analysis_summary_completeness.csv (rows=328)
2025-08-20 20:17:24,581 INFO analyst run_from_config finished successfully, wrote output/analysis_summary_completeness.csv
2025-08-20 20:17:24,583 INFO src.io Wrote provenance for output/analysis_summary_completeness.csv -> output/analysis_summary_completeness.csv.meta.json
2025-08-20 20:17:24,584 INFO analyst run_from_config start for analyst with cfg keys=['input_csv', 'output_csv', 'group_col', 'rating_col']
2025-08-20 20:17:24,729 INFO analyst analyze_df start group_col=['synthesis_id', 'ablation_condition'] rating_col=score_clarity_rating
2025-08-20 20:17:24,731 INFO analyst analyze_df done (groups=328)
2025-08-20 20:17:24,732 INFO src.io Wrote dataframe to output/analysis_summary_clarity.csv (rows=328)
2025-08-20 20:17:24,733 INFO analyst run_from_config finished successfully, wrote output/analysis_summary_clarity.csv
2025-08-20 20:17:24,736 INFO src.io Wrote provenance for output/analysis_summary_clarity.csv -> output/analysis_summary_clarity.csv.meta.json
2025-08-20 20:17:24,736 INFO analyst run_from_config start for analyst with cfg keys=['input_csv', 'output_csv', 'group_col', 'metrics']
2025-08-20 20:17:24,876 ERROR analyst run_from_config failed: Metric 'semantic_difference' is not registered.
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/analyst.py", line 70, in run_from_config
    analyst.register_metric(metric_name)
  File "/home/johnro/sop-research/gnosis/src/analyst.py", line 110, in register_metric
    metric_class = MetricRegistry.get_metric(metric_name)
  File "/home/johnro/sop-research/gnosis/src/analytics/metrics/base.py", line 29, in get_metric
    raise ValueError(f"Metric '{name}' is not registered.")
ValueError: Metric 'semantic_difference' is not registered.
2025-08-20 20:32:52,443 INFO analyst run_from_config start for analyst with cfg keys=['input_csv', 'output_csv', 'group_col', 'rating_col']
2025-08-20 20:32:52,600 INFO analyst analyze_df start group_col=['synthesis_id', 'ablation_condition'] rating_col=score_faithfulness_rating
2025-08-20 20:32:52,603 INFO analyst analyze_df done (groups=328)
2025-08-20 20:32:52,604 INFO src.io Wrote dataframe to output/analysis_summary_faithfulness.csv (rows=328)
2025-08-20 20:32:52,604 INFO analyst run_from_config finished successfully, wrote output/analysis_summary_faithfulness.csv
2025-08-20 20:32:52,607 INFO src.io Wrote provenance for output/analysis_summary_faithfulness.csv -> output/analysis_summary_faithfulness.csv.meta.json
2025-08-20 20:32:52,607 INFO analyst run_from_config start for analyst with cfg keys=['input_csv', 'output_csv', 'group_col', 'rating_col']
2025-08-20 20:32:52,739 INFO analyst analyze_df start group_col=['synthesis_id', 'ablation_condition'] rating_col=score_correctness_rating
2025-08-20 20:32:52,742 INFO analyst analyze_df done (groups=328)
2025-08-20 20:32:52,743 INFO src.io Wrote dataframe to output/analysis_summary_correctness.csv (rows=328)
2025-08-20 20:32:52,743 INFO analyst run_from_config finished successfully, wrote output/analysis_summary_correctness.csv
2025-08-20 20:32:52,747 INFO src.io Wrote provenance for output/analysis_summary_correctness.csv -> output/analysis_summary_correctness.csv.meta.json
2025-08-20 20:32:52,747 INFO analyst run_from_config start for analyst with cfg keys=['input_csv', 'output_csv', 'group_col', 'rating_col']
2025-08-20 20:32:52,882 INFO analyst analyze_df start group_col=['synthesis_id', 'ablation_condition'] rating_col=score_completeness_rating
2025-08-20 20:32:52,884 INFO analyst analyze_df done (groups=328)
2025-08-20 20:32:52,885 INFO src.io Wrote dataframe to output/analysis_summary_completeness.csv (rows=328)
2025-08-20 20:32:52,885 INFO analyst run_from_config finished successfully, wrote output/analysis_summary_completeness.csv
2025-08-20 20:32:52,888 INFO src.io Wrote provenance for output/analysis_summary_completeness.csv -> output/analysis_summary_completeness.csv.meta.json
2025-08-20 20:32:52,888 INFO analyst run_from_config start for analyst with cfg keys=['input_csv', 'output_csv', 'group_col', 'rating_col']
2025-08-20 20:32:53,029 INFO analyst analyze_df start group_col=['synthesis_id', 'ablation_condition'] rating_col=score_clarity_rating
2025-08-20 20:32:53,031 INFO analyst analyze_df done (groups=328)
2025-08-20 20:32:53,032 INFO src.io Wrote dataframe to output/analysis_summary_clarity.csv (rows=328)
2025-08-20 20:32:53,033 INFO analyst run_from_config finished successfully, wrote output/analysis_summary_clarity.csv
2025-08-20 20:32:53,035 INFO src.io Wrote provenance for output/analysis_summary_clarity.csv -> output/analysis_summary_clarity.csv.meta.json
2025-08-20 20:32:53,036 INFO analyst run_from_config start for analyst with cfg keys=['input_csv', 'output_csv', 'group_col', 'metrics']
2025-08-20 20:32:53,176 ERROR analyst run_from_config failed: Metric 'semantic_difference' is not registered.
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/analyst.py", line 70, in run_from_config
    analyst.register_metric(metric_name)
  File "/home/johnro/sop-research/gnosis/src/analyst.py", line 110, in register_metric
    metric_class = MetricRegistry.get_metric(metric_name)
  File "/home/johnro/sop-research/gnosis/src/analytics/metrics/base.py", line 29, in get_metric
    raise ValueError(f"Metric '{name}' is not registered.")
ValueError: Metric 'semantic_difference' is not registered.
2025-08-20 20:40:48,724 INFO analyst run_from_config start for analyst with cfg keys=['input_csv', 'output_csv', 'group_col', 'rating_col']
2025-08-20 20:40:48,882 INFO analyst analyze_df start group_col=['synthesis_id', 'ablation_condition'] rating_col=score_faithfulness_rating
2025-08-20 20:40:48,885 INFO analyst analyze_df done (groups=328)
2025-08-20 20:40:48,886 INFO src.io Wrote dataframe to output/analysis_summary_faithfulness.csv (rows=328)
2025-08-20 20:40:48,886 INFO analyst run_from_config finished successfully, wrote output/analysis_summary_faithfulness.csv
2025-08-20 20:40:48,889 INFO src.io Wrote provenance for output/analysis_summary_faithfulness.csv -> output/analysis_summary_faithfulness.csv.meta.json
2025-08-20 20:40:48,889 INFO analyst run_from_config start for analyst with cfg keys=['input_csv', 'output_csv', 'group_col', 'rating_col']
2025-08-20 20:40:49,027 INFO analyst analyze_df start group_col=['synthesis_id', 'ablation_condition'] rating_col=score_correctness_rating
2025-08-20 20:40:49,029 INFO analyst analyze_df done (groups=328)
2025-08-20 20:40:49,030 INFO src.io Wrote dataframe to output/analysis_summary_correctness.csv (rows=328)
2025-08-20 20:40:49,030 INFO analyst run_from_config finished successfully, wrote output/analysis_summary_correctness.csv
2025-08-20 20:40:49,032 INFO src.io Wrote provenance for output/analysis_summary_correctness.csv -> output/analysis_summary_correctness.csv.meta.json
2025-08-20 20:40:49,033 INFO analyst run_from_config start for analyst with cfg keys=['input_csv', 'output_csv', 'group_col', 'rating_col']
2025-08-20 20:40:49,168 INFO analyst analyze_df start group_col=['synthesis_id', 'ablation_condition'] rating_col=score_completeness_rating
2025-08-20 20:40:49,170 INFO analyst analyze_df done (groups=328)
2025-08-20 20:40:49,171 INFO src.io Wrote dataframe to output/analysis_summary_completeness.csv (rows=328)
2025-08-20 20:40:49,171 INFO analyst run_from_config finished successfully, wrote output/analysis_summary_completeness.csv
2025-08-20 20:40:49,173 INFO src.io Wrote provenance for output/analysis_summary_completeness.csv -> output/analysis_summary_completeness.csv.meta.json
2025-08-20 20:40:49,173 INFO analyst run_from_config start for analyst with cfg keys=['input_csv', 'output_csv', 'group_col', 'rating_col']
2025-08-20 20:40:49,314 INFO analyst analyze_df start group_col=['synthesis_id', 'ablation_condition'] rating_col=score_clarity_rating
2025-08-20 20:40:49,316 INFO analyst analyze_df done (groups=328)
2025-08-20 20:40:49,317 INFO src.io Wrote dataframe to output/analysis_summary_clarity.csv (rows=328)
2025-08-20 20:40:49,317 INFO analyst run_from_config finished successfully, wrote output/analysis_summary_clarity.csv
2025-08-20 20:40:49,321 INFO src.io Wrote provenance for output/analysis_summary_clarity.csv -> output/analysis_summary_clarity.csv.meta.json
2025-08-20 20:40:49,321 INFO analyst run_from_config start for analyst with cfg keys=['input_csv', 'output_csv', 'group_col', 'metrics']
2025-08-20 20:40:49,459 ERROR analyst run_from_config failed: Metric 'semantic_difference' is not registered.
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/analyst.py", line 70, in run_from_config
    analyst.register_metric(metric_name)
  File "/home/johnro/sop-research/gnosis/src/analyst.py", line 110, in register_metric
    metric_class = MetricRegistry.get_metric(metric_name)
  File "/home/johnro/sop-research/gnosis/src/analytics/metrics/base.py", line 30, in get_metric
    raise ValueError(f"Metric '{name}' is not registered.")
ValueError: Metric 'semantic_difference' is not registered.
2025-08-20 20:48:50,875 INFO analyst run_from_config start for analyst with cfg keys=['input_csv', 'output_csv', 'group_col', 'rating_col']
2025-08-20 20:48:51,036 INFO analyst analyze_df start group_col=['synthesis_id', 'ablation_condition'] rating_col=score_faithfulness_rating
2025-08-20 20:48:51,038 INFO analyst analyze_df done (groups=328)
2025-08-20 20:48:51,040 INFO src.io Wrote dataframe to output/analysis_summary_faithfulness.csv (rows=328)
2025-08-20 20:48:51,040 INFO analyst run_from_config finished successfully, wrote output/analysis_summary_faithfulness.csv
2025-08-20 20:48:51,044 INFO src.io Wrote provenance for output/analysis_summary_faithfulness.csv -> output/analysis_summary_faithfulness.csv.meta.json
2025-08-20 20:48:51,044 INFO analyst run_from_config start for analyst with cfg keys=['input_csv', 'output_csv', 'group_col', 'rating_col']
2025-08-20 20:48:51,192 INFO analyst analyze_df start group_col=['synthesis_id', 'ablation_condition'] rating_col=score_correctness_rating
2025-08-20 20:48:51,194 INFO analyst analyze_df done (groups=328)
2025-08-20 20:48:51,196 INFO src.io Wrote dataframe to output/analysis_summary_correctness.csv (rows=328)
2025-08-20 20:48:51,196 INFO analyst run_from_config finished successfully, wrote output/analysis_summary_correctness.csv
2025-08-20 20:48:51,199 INFO src.io Wrote provenance for output/analysis_summary_correctness.csv -> output/analysis_summary_correctness.csv.meta.json
2025-08-20 20:48:51,199 INFO analyst run_from_config start for analyst with cfg keys=['input_csv', 'output_csv', 'group_col', 'rating_col']
2025-08-20 20:48:51,352 INFO analyst analyze_df start group_col=['synthesis_id', 'ablation_condition'] rating_col=score_completeness_rating
2025-08-20 20:48:51,354 INFO analyst analyze_df done (groups=328)
2025-08-20 20:48:51,355 INFO src.io Wrote dataframe to output/analysis_summary_completeness.csv (rows=328)
2025-08-20 20:48:51,355 INFO analyst run_from_config finished successfully, wrote output/analysis_summary_completeness.csv
2025-08-20 20:48:51,358 INFO src.io Wrote provenance for output/analysis_summary_completeness.csv -> output/analysis_summary_completeness.csv.meta.json
2025-08-20 20:48:51,359 INFO analyst run_from_config start for analyst with cfg keys=['input_csv', 'output_csv', 'group_col', 'rating_col']
2025-08-20 20:48:51,513 INFO analyst analyze_df start group_col=['synthesis_id', 'ablation_condition'] rating_col=score_clarity_rating
2025-08-20 20:48:51,516 INFO analyst analyze_df done (groups=328)
2025-08-20 20:48:51,517 INFO src.io Wrote dataframe to output/analysis_summary_clarity.csv (rows=328)
2025-08-20 20:48:51,517 INFO analyst run_from_config finished successfully, wrote output/analysis_summary_clarity.csv
2025-08-20 20:48:51,520 INFO src.io Wrote provenance for output/analysis_summary_clarity.csv -> output/analysis_summary_clarity.csv.meta.json
2025-08-20 20:48:51,521 INFO analyst run_from_config start for analyst with cfg keys=['input_csv', 'output_csv', 'group_col', 'metrics']
2025-08-20 20:49:07,633 ERROR analyst run_from_config failed: Metric 'net_semantic_drift' is not registered.
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/analyst.py", line 70, in run_from_config
    analyst.register_metric(metric_name)
  File "/home/johnro/sop-research/gnosis/src/analyst.py", line 110, in register_metric
    metric_class = MetricRegistry.get_metric(metric_name)
  File "/home/johnro/sop-research/gnosis/src/analytics/metrics/base.py", line 30, in get_metric
    raise ValueError(f"Metric '{name}' is not registered.")
ValueError: Metric 'net_semantic_drift' is not registered.
2025-08-20 20:51:14,809 INFO analyst run_from_config start for analyst with cfg keys=['input_csv', 'output_csv', 'group_col', 'rating_col']
2025-08-20 20:51:14,960 INFO analyst analyze_df start group_col=['synthesis_id', 'ablation_condition'] rating_col=score_faithfulness_rating
2025-08-20 20:51:14,963 INFO analyst analyze_df done (groups=328)
2025-08-20 20:51:14,964 INFO src.io Wrote dataframe to output/analysis_summary_faithfulness.csv (rows=328)
2025-08-20 20:51:14,964 INFO analyst run_from_config finished successfully, wrote output/analysis_summary_faithfulness.csv
2025-08-20 20:51:14,967 INFO src.io Wrote provenance for output/analysis_summary_faithfulness.csv -> output/analysis_summary_faithfulness.csv.meta.json
2025-08-20 20:51:14,967 INFO analyst run_from_config start for analyst with cfg keys=['input_csv', 'output_csv', 'group_col', 'rating_col']
2025-08-20 20:51:15,108 INFO analyst analyze_df start group_col=['synthesis_id', 'ablation_condition'] rating_col=score_correctness_rating
2025-08-20 20:51:15,110 INFO analyst analyze_df done (groups=328)
2025-08-20 20:51:15,111 INFO src.io Wrote dataframe to output/analysis_summary_correctness.csv (rows=328)
2025-08-20 20:51:15,111 INFO analyst run_from_config finished successfully, wrote output/analysis_summary_correctness.csv
2025-08-20 20:51:15,113 INFO src.io Wrote provenance for output/analysis_summary_correctness.csv -> output/analysis_summary_correctness.csv.meta.json
2025-08-20 20:51:15,114 INFO analyst run_from_config start for analyst with cfg keys=['input_csv', 'output_csv', 'group_col', 'rating_col']
2025-08-20 20:51:15,253 INFO analyst analyze_df start group_col=['synthesis_id', 'ablation_condition'] rating_col=score_completeness_rating
2025-08-20 20:51:15,255 INFO analyst analyze_df done (groups=328)
2025-08-20 20:51:15,256 INFO src.io Wrote dataframe to output/analysis_summary_completeness.csv (rows=328)
2025-08-20 20:51:15,256 INFO analyst run_from_config finished successfully, wrote output/analysis_summary_completeness.csv
2025-08-20 20:51:15,259 INFO src.io Wrote provenance for output/analysis_summary_completeness.csv -> output/analysis_summary_completeness.csv.meta.json
2025-08-20 20:51:15,259 INFO analyst run_from_config start for analyst with cfg keys=['input_csv', 'output_csv', 'group_col', 'rating_col']
2025-08-20 20:51:15,396 INFO analyst analyze_df start group_col=['synthesis_id', 'ablation_condition'] rating_col=score_clarity_rating
2025-08-20 20:51:15,398 INFO analyst analyze_df done (groups=328)
2025-08-20 20:51:15,399 INFO src.io Wrote dataframe to output/analysis_summary_clarity.csv (rows=328)
2025-08-20 20:51:15,399 INFO analyst run_from_config finished successfully, wrote output/analysis_summary_clarity.csv
2025-08-20 20:51:15,402 INFO src.io Wrote provenance for output/analysis_summary_clarity.csv -> output/analysis_summary_clarity.csv.meta.json
2025-08-20 20:51:15,402 INFO analyst run_from_config start for analyst with cfg keys=['input_csv', 'output_csv', 'group_col', 'metrics']
2025-08-20 20:51:17,563 ERROR analyst run_from_config failed: "Input data must have a 'response' column or key."
Traceback (most recent call last):
  File "/home/johnro/sop-research/gnosis/src/analyst.py", line 79, in run_from_config
    metric_results = analyst.run_analysis(group_df)
  File "/home/johnro/sop-research/gnosis/src/analyst.py", line 119, in run_analysis
    results[metric.name()] = metric.calculate(data)
  File "/home/johnro/sop-research/gnosis/src/analytics/metrics/semantic_difference.py", line 13, in calculate
    raise KeyError("Input data must have a 'response' column or key.")
KeyError: "Input data must have a 'response' column or key."
2025-08-20 21:01:11,124 INFO analyst run_from_config start for analyst with cfg keys=['input_csv', 'output_csv', 'group_col', 'rating_col']
2025-08-20 21:01:11,424 INFO analyst analyze_df start group_col=['synthesis_id', 'ablation_condition'] rating_col=score_faithfulness_rating
2025-08-20 21:01:11,434 INFO analyst analyze_df done (groups=328)
2025-08-20 21:01:11,436 INFO src.io Wrote dataframe to output/analysis_summary_faithfulness.csv (rows=328)
2025-08-20 21:01:11,436 INFO analyst run_from_config finished successfully, wrote output/analysis_summary_faithfulness.csv
2025-08-20 21:01:11,442 INFO src.io Wrote provenance for output/analysis_summary_faithfulness.csv -> output/analysis_summary_faithfulness.csv.meta.json
2025-08-20 21:01:11,442 INFO analyst run_from_config start for analyst with cfg keys=['input_csv', 'output_csv', 'group_col', 'rating_col']
2025-08-20 21:01:11,598 INFO analyst analyze_df start group_col=['synthesis_id', 'ablation_condition'] rating_col=score_correctness_rating
2025-08-20 21:01:11,600 INFO analyst analyze_df done (groups=328)
2025-08-20 21:01:11,601 INFO src.io Wrote dataframe to output/analysis_summary_correctness.csv (rows=328)
2025-08-20 21:01:11,601 INFO analyst run_from_config finished successfully, wrote output/analysis_summary_correctness.csv
2025-08-20 21:01:11,605 INFO src.io Wrote provenance for output/analysis_summary_correctness.csv -> output/analysis_summary_correctness.csv.meta.json
2025-08-20 21:01:11,605 INFO analyst run_from_config start for analyst with cfg keys=['input_csv', 'output_csv', 'group_col', 'rating_col']
2025-08-20 21:01:11,753 INFO analyst analyze_df start group_col=['synthesis_id', 'ablation_condition'] rating_col=score_completeness_rating
2025-08-20 21:01:11,756 INFO analyst analyze_df done (groups=328)
2025-08-20 21:01:11,757 INFO src.io Wrote dataframe to output/analysis_summary_completeness.csv (rows=328)
2025-08-20 21:01:11,757 INFO analyst run_from_config finished successfully, wrote output/analysis_summary_completeness.csv
2025-08-20 21:01:11,760 INFO src.io Wrote provenance for output/analysis_summary_completeness.csv -> output/analysis_summary_completeness.csv.meta.json
2025-08-20 21:01:11,760 INFO analyst run_from_config start for analyst with cfg keys=['input_csv', 'output_csv', 'group_col', 'rating_col']
2025-08-20 21:01:11,906 INFO analyst analyze_df start group_col=['synthesis_id', 'ablation_condition'] rating_col=score_clarity_rating
2025-08-20 21:01:11,908 INFO analyst analyze_df done (groups=328)
2025-08-20 21:01:11,909 INFO src.io Wrote dataframe to output/analysis_summary_clarity.csv (rows=328)
2025-08-20 21:01:11,909 INFO analyst run_from_config finished successfully, wrote output/analysis_summary_clarity.csv
2025-08-20 21:01:11,912 INFO src.io Wrote provenance for output/analysis_summary_clarity.csv -> output/analysis_summary_clarity.csv.meta.json
2025-08-20 21:01:11,912 INFO analyst run_from_config start for analyst with cfg keys=['input_csv', 'output_csv', 'group_col', 'metrics']
2025-08-20 21:02:09,181 INFO src.io Wrote dataframe to output/analysis_summary_semantic_drift.csv (rows=328)
2025-08-20 21:02:09,182 INFO analyst run_from_config finished successfully, wrote output/analysis_summary_semantic_drift.csv
2025-08-20 21:02:09,210 INFO src.io Wrote provenance for output/analysis_summary_semantic_drift.csv -> output/analysis_summary_semantic_drift.csv.meta.json
